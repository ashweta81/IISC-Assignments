{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment : IoT and Edge Devices - Quantization and Pruning of Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu26Vq9jDTpj"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "*  understand about quantization\n",
        "*  batchnorm folding\n",
        "*  quantization aware training\n",
        "*  understand role of pruning in minimization of the resource(power, memory, number of computations) requirements at test time\n",
        "*  implement iterative pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NiLPMhPgYFa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9CxCComgihL"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0k4a9Qbe6p1"
      },
      "source": [
        "In this experiment, we will use the CIFAR-10 dataset from keras API. It consists of 60,000 colour images(32x32) in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.\n",
        "\n",
        "\n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "\n",
        "\n",
        "<img src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Images/CIFAR10.png\" alt=\"Drawing\" height=\"350\" width=\"440\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Deep learning has a growing history of successes, but heavy algorithms running on large graphical processing units are far from ideal. A relatively new family of deep learning methods called quantized neural networks have appeared in answer to this discrepancy. Quantization methods helps enabling efficient high-performance deep learning computation on small devices.\n",
        "\n",
        "Moreover, Deep learning for classification tasks involves training the parameters of a neural network such that the algorithm learns to discern between object classes. This is achieved by feeding many images of labelled data to the neural network, while updating the parameters to increase performance on a smooth objective function. A drawback is that a large number of parameters are used, compared to more traditional algorithms.\n",
        "\n",
        "Thus enters quantization as a method to bring the neural network to a reasonable size, while also achieving high performance accuracy. This is especially important for on-device applications, where the memory size and number of computations are necessarily limited. Quantization for deep learning is the process of approximating a neural network that uses floating-point numbers by a neural network of low bit width numbers. This dramatically reduces both the memory requirement and computational cost of using neural networks.\n",
        "\n",
        "\n",
        "**Note:** Refer to the following to understand more about [Quantization](https://pytorch.org/docs/stable/quantization.html)"
      ],
      "metadata": {
        "id": "TFNIzE-djhjR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2239822\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9167668365\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rWzUR5IMqb7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8fe4edd3-2173-4ea7-ee60-39614edc774d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2239822&recordId=2922\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_32_Quantization_&_Pruning_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required packages"
      ],
      "metadata": {
        "id": "pNYuAjidjlGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LuznkOLEjwAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining helper functions to download and load the data"
      ],
      "metadata": {
        "id": "11swMCS8j5V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "\n",
        "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
        "\n",
        "    train_transform = transforms.Compose([   # Define transformations for train and test sets\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    # Loading train and test dataset\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform)\n",
        "\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    # A Sampler that randomly shuffled indices\n",
        "    # A RandomSampler with a size and dtype for the stored indices.\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "\n",
        "    # A Sampler that returns indices sequentially\n",
        "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "    # Loading the train and test dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_set, batch_size=train_batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_set, batch_size=eval_batch_size,\n",
        "        sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "zdcj6vRVvS8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization Code for Model Training"
      ],
      "metadata": {
        "id": "qdJu24TpjqFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source code for torchvision.models.resnet"
      ],
      "metadata": {
        "id": "E7dVMz_YyOLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}"
      ],
      "metadata": {
        "id": "A7kMsVo8x00k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8sXiKScEmcZ"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        # Rename relu to relu1\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "        # Remember to use two independent ReLU for layer fusion.\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # Use FloatFunctional for addition for quantization compatibility\n",
        "        # out += identity\n",
        "        out = self.skip_add.add(identity, out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # out += identity\n",
        "        out = self.skip_add.add(identity, out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # Each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
        "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    arch: str,\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> ResNet:\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Training Function"
      ],
      "metadata": {
        "id": "48mKTQq3ilZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, device):\n",
        "\n",
        "    # Learning rate\n",
        "    learning_rate = 1e-2\n",
        "\n",
        "    # No. of epochs\n",
        "    num_epochs = 10\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Model Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate through the dataloader\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Model Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        print(\"Epoch: {:02d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QxBD7txEighQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Test Function"
      ],
      "metadata": {
        "id": "o0kT-zi4jEo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "    # Setting model mode to eval()\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterating through the test dataloader\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy"
      ],
      "metadata": {
        "id": "p0tbCRMPGK1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-training quantization (PTQ) can reduce the memory footprint and latency of deep model inference, while still\n",
        "# preserving the accuracy of the model, with only a small unlabeled calibration set and without the retraining on full training set.\n",
        "# To calibrate a quantized model, current PTQ methods usually randomly select some unlabeled data from training set as calibration data.\n",
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)\n",
        "\n",
        "# how to measure inference time correctly\n",
        "# measure the time for model inference\n",
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 3, 32, 32),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    # WAIT FOR GPU SYNC\n",
        "    # torch.cuda.synchronize(). This line of code performs synchronization between the host and device (i.e., GPU and CPU),\n",
        "    # so the time recording takes place only after the process running on the GPU is finished.\n",
        "    # Waits for everything to finish running\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad(): # No gradients calculation\n",
        "        # Start time\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        # Inference time took for the model to run the samples on GPU\n",
        "        end_time = time.time()\n",
        "    # Difference between start and end time\n",
        "    elapsed_time = end_time - start_time\n",
        "    # Average elapsed time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave"
      ],
      "metadata": {
        "id": "w7tkSSlGjWkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and load the models"
      ],
      "metadata": {
        "id": "ltA8B4_YqAJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the the model\n",
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n",
        "\n",
        "# Load the model using pytorch state dict\n",
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Save an offline version of this module for use in a separate process.\n",
        "# The saved module serializes all of the methods, submodules, parameters, and attributes of this module.\n",
        "# https://pytorch.org/docs/stable/generated/torch.jit.save.html\n",
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "\n",
        "# All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from\n",
        "# https://pytorch.org/docs/stable/generated/torch.jit.load.html\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ozz4IsLFjaxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the model"
      ],
      "metadata": {
        "id": "N5u_C0Q6qtgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model(without pretraining) with no of clasess\n",
        "def create_model(num_classes=10):\n",
        "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
        "    return model\n",
        "\n",
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "3ZFM9Vx1jgDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization\n",
        "\n",
        "* Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually **int8** compared to floating point implementations.\n",
        "    \n",
        "* Quantization leverages 8bit integer (int8) instructions to reduce the model size and run the inference faster (reduced latency).\n",
        "    \n",
        "* This enables providing quick inference from a trained model and even fitting it into the resources available on a mobile device.\n",
        "\n",
        "* Quantization allows for siginificant performance gains!\n",
        "    * Up to 4x reduction in model size.\n",
        "    * Up to 2-4x reduction in memory bandwidth.\n",
        "    * Up to 2-4x faster inference due to savings in memory bandwidth and faster compute with int8 arithmetic (the exact speed up varies depending on the hardware, the runtime, and the model).\n",
        "    \n",
        "* Quantization doesn't come without additional cost, as it means introducing approximations and the resulting networks have slightly less accuracy.\n",
        "    \n",
        "* These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/d94bdf2f6668d55de89d3c4b602b971e1fb1699c//assets/tut_compress_quant.png\" width=750px/>\n",
        "</center>\n",
        "\n",
        "\n",
        "* Quantization is available in PyTorch in various flavors starting in version 1.3 and there are published quantized models for ResNet, ResNext, MobileNetV2, GoogleNet, InceptionV3 and ShuffleNetV2 in the PyTorch torchvision 0.5 library.\n",
        "\n",
        "* Quantization is compatible with the rest of PyTorch: quantized models are traceable and scriptable. Quantized and floating point operations can be mixed in a model.\n",
        "\n",
        "* Mapping of floating point tensors to quantized tensors is customizable with user defined observer/fake-quantization blocks. PyTorch provides default implementations that should work for most use cases.\n",
        "\n",
        "* Currently the quantized models can only be run on CPU. However, it is possible to send the non-quantized parts of the model to a GPU.\n",
        "    * GPU quantization is a work-in-progress, see PTQ (Post Training Quantization)\n",
        "\n"
      ],
      "metadata": {
        "id": "QFZN0QvZzmOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define a quantized version of our network.\n",
        "\n",
        "For this, apply `torch.quantization.QuantStub()` and `torch.quantization.DequantStub()` to the inputs and outputs, respectively.\n",
        "\n",
        "**Note**:  This step is to ask PyTorch to specifically collect quantization statistics for the inputs and outputs, respectively. Otherwise, since PyTorch collects quantization statistics for weights and activations by default, there will be problems for the input quantization and output dequantization, since there are no quantization statistics collected for them."
      ],
      "metadata": {
        "id": "Fqruslkzb7tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a floating point model where some layers could be statically quantized\n",
        "class QuantizedResNet18(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "        # FP32 model\n",
        "        # FP32 is a FP32 Floating point data format for Deep Learning where data is represented as a 32-bit floating point number.\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "T172-SvRcCK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post training static quantization\n",
        "\n",
        "Static quantization first feeds batches of data through the network and computes the resulting distributions of the different activations.\n",
        "Static quantization quantizes the weights and activations of the model. It fuses activations into preceding layers wherever possible. It requires calibration with a representative dataset to determine optimal quantization parameters for activations. Post Training Quantization is typically used when both memory bandwidth and compute savings are important with CNNs being a typical use case. Static quantization is also known as **Post Training Quantization** or **PTQ**.\n",
        "\n",
        "Steps involved in post training static quantization are :\n",
        "1.  Load pretrained model or train a model.\n",
        "2.  Fuse modules - combine operations/modules into a single module to obtain higher accuracy and performance. This is done using the fuse_modules() API, which takes in lists of modules to be fused. We currently support the following fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]\n",
        "3.  Evaluate model in calibration dataset.\n",
        "4.  Calibration data is used to calibrate the model. It is usually a subset of training data.\n",
        "5.  Calculate dynamic ranges of weights and activations in the network to calculate quantization parameters(scale and zero point).\n",
        "6.  Quantize the network using quantization parameters and run the inference.\n",
        "\n",
        "\n",
        "Apply quantization after training, quantization parameters are calculated based on sample calibration data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXC8E1GccL6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 0\n",
        "num_classes = 10\n",
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "model_dir = \"saved_models\"\n",
        "model_filename = \"resnet18_cifar10.pt\"\n",
        "quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
        "model_filepath = os.path.join(model_dir, model_filename)\n",
        "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "# Create an untrained model.\n",
        "model = create_model(num_classes=num_classes)\n",
        "\n",
        "train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "\n",
        "# Train model.\n",
        "model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device)\n",
        "\n",
        "# Save model.\n",
        "save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
        "\n",
        "# Load a pretrained model.\n",
        "model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model.to(cpu_device)\n",
        "\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# The model has to be switched to evaluation mode before any layer fusion.\n",
        "# Otherwise the quantization will not work correctly.\n",
        "fused_model.eval()"
      ],
      "metadata": {
        "id": "VxW2caO-Fe_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470761d2-76b3-4545-f267-081e84d844af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 170498071/170498071 [00:04<00:00, 41595616.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Epoch: 00 Train Loss: 1.688 Train Acc: 0.390 Eval Loss: 1.393 Eval Acc: 0.505\n",
            "Epoch: 01 Train Loss: 1.328 Train Acc: 0.522 Eval Loss: 1.139 Eval Acc: 0.588\n",
            "Epoch: 02 Train Loss: 1.151 Train Acc: 0.590 Eval Loss: 1.067 Eval Acc: 0.627\n",
            "Epoch: 03 Train Loss: 1.031 Train Acc: 0.634 Eval Loss: 0.948 Eval Acc: 0.669\n",
            "Epoch: 04 Train Loss: 0.953 Train Acc: 0.662 Eval Loss: 0.961 Eval Acc: 0.666\n",
            "Epoch: 05 Train Loss: 0.882 Train Acc: 0.689 Eval Loss: 0.851 Eval Acc: 0.707\n",
            "Epoch: 06 Train Loss: 0.843 Train Acc: 0.703 Eval Loss: 0.977 Eval Acc: 0.690\n",
            "Epoch: 07 Train Loss: 0.796 Train Acc: 0.719 Eval Loss: 0.793 Eval Acc: 0.725\n",
            "Epoch: 08 Train Loss: 0.761 Train Acc: 0.732 Eval Loss: 0.801 Eval Acc: 0.726\n",
            "Epoch: 09 Train Loss: 0.723 Train Acc: 0.745 Eval Loss: 0.821 Eval Acc: 0.725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusing batchnorm with convolution saves extra computation at time of inference. It is also known as `Batchnorm folding`**.\n",
        "\n",
        "We update the weight and bias  of convolution layer before batchnorm to simulate the effect of batchnorm.\n",
        "\n",
        "Equations (5) & (6) denote the same  \n",
        "\n",
        "<br>\n",
        "\n",
        "![bn_folding.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "ygkiYBoQ8sVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuse the model in place rather manually.\n",
        "# Fuse the activations to preceding layers, where applicable.\n",
        "# This needs to be done manually depending on the model architecture.\n",
        "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
        "# https://pytorch.org/docs/stable/generated/torch.quantization.fuse_modules.html\n",
        "# It returns model with fused modules. A new copy is created if inplace=True.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "for module_name, module in fused_model.named_children():\n",
        "    if \"layer\" in module_name:\n",
        "        for basic_block_name, basic_block in module.named_children():\n",
        "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "            for sub_block_name, sub_block in basic_block.named_children():\n",
        "                if sub_block_name == \"downsample\":\n",
        "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "# Print FP32 model.\n",
        "print(model)\n",
        "\n",
        "# Print fused model.\n",
        "print(fused_model)\n",
        "\n",
        "# Model and fused model should be equivalent.\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\""
      ],
      "metadata": {
        "id": "EgzFQyjY-Hwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5916de3b-2c56-4bd9-e3ac-746b026469e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "ResNet(\n",
            "  (conv1): ConvReLU2d(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (bn1): Identity()\n",
            "  (relu): Identity()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Converting networks to use both integer arithmetic and int8 memory accesses can improve the latency performance.\n",
        "    \n",
        "* Static quantization first feeds batches of data through the network and computes the resulting distributions of the different activations.\n",
        "    * This is done by inserting observer modules at different points that record these distributions.\n",
        "    \n",
        "* This information is used to determine how specifically the different activations should be quantized at inference time.\n",
        "        \n",
        "* A simple technique would be to simply divide the entire range of activations into 256 levels, but PyTorch supports more sophisticated methods as well.\n",
        "    \n",
        "* This step allows to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.\n",
        "    \n",
        "* **Optimizing static quantization includes:**\n",
        "    * **Observers:** observer modules specify how statistics are collected prior to quantization to try out more advanced methods to quantize the data.\n",
        "\n",
        "    * **Operator fusion:** fuse multiple operations into a single operation, saving on memory access while also improving the operations numerical accuracy.\n",
        "    \n",
        "    * **Per-channel quantization:** we can independently quantize weights for each output channel in a convolution/linear layer, which can lead to higher accuracy with almost the same speed.\n"
      ],
      "metadata": {
        "id": "53Hwkm3J2WDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will perform the following:\n",
        "\n",
        "*  Specify quantization configurations, such as symmetric quantization or asymmetric quantization, etc.\n",
        "*  Prepare quantization model for post-training calibration.\n",
        "*  Run post-training calibration.\n",
        "*  Convert the calibrated floating point model to quantized integer model.\n",
        "*  Verify accuracies and inference performance gain.\n",
        "*  Save the quantized integer model."
      ],
      "metadata": {
        "id": "QstO3rQocSXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for static quantization. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "\n",
        "# Using un-fused model will fail.\n",
        "# Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "# quantized_model = QuantizedResNet18(model_fp32=model) # This will not work\n",
        "# Select quantization schemes from\n",
        "# https://pytorch.org/docs/stable/quantization-support.html\n",
        "# When preparing a quantized model, it is necessary to ensure that qconfig and the engine used for\n",
        "# quantized computations match the backend on which the model will be executed. The qconfig controls the type of\n",
        "# observers used during the quantization passes. The qengine controls whether fbgemm or qnnpack specific packing\n",
        "# function is used when packing weights for linear and convolution functions and modules.\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "\n",
        "# Custom quantization configurations\n",
        "# quantization_config = torch.quantization.default_qconfig\n",
        "# quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "quantized_model.qconfig = quantization_config\n",
        "\n",
        "# Print quantization configurations\n",
        "print(quantized_model.qconfig)\n",
        "\n",
        "torch.quantization.prepare(quantized_model, inplace=True)\n",
        "\n",
        "# Use training data for calibration.\n",
        "calibrate_model(model=quantized_model, loader=train_loader, device=cpu_device)\n",
        "\n",
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "\n",
        "# Using high-level static quantization wrapper\n",
        "# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "# quantized_model = torch.quantization.quantize(model=quantized_model, run_fn=calibrate_model, run_args=[train_loader], mapping=None, inplace=False)\n",
        "\n",
        "quantized_model.eval()\n",
        "\n",
        "# Print the quantized model.\n",
        "print(quantized_model)\n",
        "\n",
        "# Save the quantized model.\n",
        "save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "# Load the quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "id": "_LrHNHHuHXZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393109ce-2ed1-435b-cafb-a4c93fb39689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
            "QuantizedResNet18(\n",
            "  (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): ResNet(\n",
            "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.06232645735144615, zero_point=0, padding=(3, 3))\n",
            "    (bn1): Identity()\n",
            "    (relu): Identity()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03696011006832123, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07649149000644684, zero_point=66, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.13620637357234955, zero_point=35\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03670111671090126, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07971568405628204, zero_point=62, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.15992173552513123, zero_point=33\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.040709469467401505, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.08125809580087662, zero_point=63, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.08426189422607422, zero_point=66)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.12221593409776688, zero_point=64\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.037721261382102966, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.08074159920215607, zero_point=60, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.12389332801103592, zero_point=42\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.04625144228339195, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.09039395302534103, zero_point=56, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.08637163043022156, zero_point=63)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.12688307464122772, zero_point=56\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.04983539134263992, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.1292308121919632, zero_point=53, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.17612580955028534, zero_point=41\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.06688787788152695, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.16788645088672638, zero_point=62, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.12215565890073776, zero_point=63)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.2366320639848709, zero_point=65\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.10676904767751694, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.2163422703742981, zero_point=68, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.22423847019672394, zero_point=65\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.2744315266609192, zero_point=36, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n",
            "FP32 evaluation accuracy: 0.725\n",
            "INT8 evaluation accuracy: 0.723\n",
            "FP32 CPU Inference Latency: 11.96 ms / sample\n",
            "FP32 CUDA Inference Latency: 3.28 ms / sample\n",
            "INT8 CPU Inference Latency: 5.16 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 2.07 ms / sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization aware training\n",
        "Static quantization allows the user to generate quantized integer model that is highly efficient during inference. However, sometimes, even with careful post-training calibration, the model accuracies might be sacrificed to some extent that is not acceptable. If this is the case, post-training calibration is not sufficient to generate a quantized integer model. We would have train the model in a way so that the quantization effect has been taken into account. Quantization aware training is capable of modeling the quantization effect during training.\n",
        "\n",
        "The mechanism of quantization aware training is simple, it places fake quantization modules, i.e., all weights and activations are fake quantized during both the forward and backward passes of training: quantization and dequantization modules, at the places where quantization happens during floating-point model to quantized integer model conversion, to simulate the effects of clamping and rounding brought by integer quantization. The fake quantization modules will also monitor scales and zero points of the weights and activations. Once the quantization aware training is finished, the floating point model could be converted to quantized integer model immediately using the information stored in the fake quantization modules."
      ],
      "metadata": {
        "id": "B9LMCuRRNIZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_qat(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=5):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
        "\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    # Evaluation\n",
        "    # model.eval()\n",
        "    # eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "    # print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        if epoch > 3:\n",
        "        # Freeze quantizer parameters\n",
        "          model.apply(torch.quantization.disable_observer)\n",
        "\n",
        "        if epoch > 2:\n",
        "        # Freeze batch norm mean and variance estimates\n",
        "          model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "OpOUfWFT7npm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model_filename = \"resnet18_QAT_quantized_cifar10.pt\"\n",
        "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "# Create an untrained model.\n",
        "model = create_model(num_classes=num_classes)\n",
        "\n",
        "train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "\n",
        "# # Load a pretrained model.\n",
        "model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model.to(cpu_device)\n",
        "\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "# Model and fused model should be equivalent.\n",
        "model.eval()\n",
        "fused_model.eval()\n",
        "\n",
        "\n",
        "# Fuse the model in place rather manually.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "for module_name, module in fused_model.named_children():\n",
        "    if \"layer\" in module_name:\n",
        "        for basic_block_name, basic_block in module.named_children():\n",
        "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "            for sub_block_name, sub_block in basic_block.named_children():\n",
        "                if sub_block_name == \"downsample\":\n",
        "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "# Print FP32 model.\n",
        "print(model)\n",
        "\n",
        "# Print fused model.\n",
        "print(fused_model)\n",
        "\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\""
      ],
      "metadata": {
        "id": "0U8gTtVnHYlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4041c491-53d9-46f7-cee7-e061343d5836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "ResNet(\n",
            "  (conv1): ConvReLU2d(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (bn1): Identity()\n",
            "  (relu): Identity()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model for quantization aware training."
      ],
      "metadata": {
        "id": "Gsd4vE8m3IT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for quantization aware training. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "\n",
        "# Using un-fused model will fail.\n",
        "# Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "# quantized_model = QuantizedResNet18(model_fp32=model)\n",
        "# Select quantization schemes from\n",
        "# https://pytorch.org/docs/stable/quantization-support.html\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "\n",
        "# Custom quantization configurations\n",
        "# quantization_config = torch.quantization.default_qconfig\n",
        "# quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "quantized_model.qconfig = quantization_config\n",
        "\n",
        "# Print quantization configurations\n",
        "print(quantized_model.qconfig)\n",
        "\n",
        "# https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
        "torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "# # Use training data for calibration.\n",
        "print(\"Training QAT Model...\")\n",
        "quantized_model.train()\n",
        "\n",
        "train_model_qat(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=5)\n",
        "quantized_model.to(cpu_device)\n",
        "\n",
        "# Using high-level static quantization wrapper\n",
        "# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "# quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
        "\n",
        "# Convert the observed model to a quantized model. This does several things:\n",
        "# quantizes the weights, computes and stores the scale and bias value to be\n",
        "# used with each activation tensor, fuses modules where appropriate,\n",
        "# and replaces key operators with quantized implementations.\n",
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "quantized_model.eval()\n",
        "\n",
        "# Print quantized model.\n",
        "print(quantized_model)\n",
        "\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"QAT FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"QAT INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
        "\n",
        "print(\"QAT FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"QAT FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"QAT INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"QAT INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "id": "7HwZae8GJ8KY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a295ad00-2e04-41ba-8de0-a913ca459808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
            "Training QAT Model...\n",
            "Epoch: 000 Train Loss: 0.646 Train Acc: 0.771 Eval Loss: 0.682 Eval Acc: 0.766\n",
            "Epoch: 001 Train Loss: 0.621 Train Acc: 0.782 Eval Loss: 0.658 Eval Acc: 0.773\n",
            "Epoch: 002 Train Loss: 0.609 Train Acc: 0.784 Eval Loss: 0.683 Eval Acc: 0.763\n",
            "Epoch: 003 Train Loss: 0.601 Train Acc: 0.787 Eval Loss: 0.679 Eval Acc: 0.764\n",
            "Epoch: 004 Train Loss: 0.593 Train Acc: 0.792 Eval Loss: 0.678 Eval Acc: 0.770\n",
            "QuantizedResNet18(\n",
            "  (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): ResNet(\n",
            "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.05972612649202347, zero_point=0, padding=(3, 3))\n",
            "    (bn1): Identity()\n",
            "    (relu): Identity()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03667579963803291, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.078159861266613, zero_point=67, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.13703395426273346, zero_point=37\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03771718591451645, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08572276681661606, zero_point=63, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.14586834609508514, zero_point=38\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.04157406836748123, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.08427469432353973, zero_point=63, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.08522665500640869, zero_point=66)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.13176417350769043, zero_point=64\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.03898444399237633, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.08531511574983597, zero_point=61, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.12731006741523743, zero_point=42\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.0545312874019146, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.08463506400585175, zero_point=56, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.09232856333255768, zero_point=63)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.12802031636238098, zero_point=57\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.04465280473232269, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.11776300519704819, zero_point=53, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.16129374504089355, zero_point=41\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.06276826560497284, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.14804625511169434, zero_point=62, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.11128149181604385, zero_point=63)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.21363067626953125, zero_point=63\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.09099344909191132, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.18740850687026978, zero_point=69, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.18290512263774872, zero_point=64\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.21598775684833527, zero_point=45, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n",
            "QAT FP32 evaluation accuracy: 0.725\n",
            "QAT INT8 evaluation accuracy: 0.767\n",
            "QAT FP32 CPU Inference Latency: 8.48 ms / sample\n",
            "QAT FP32 CUDA Inference Latency: 3.41 ms / sample\n",
            "QAT INT8 CPU Inference Latency: 3.02 ms / sample\n",
            "QAT INT8 JIT CPU Inference Latency: 1.35 ms / sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruning using PyTorch"
      ],
      "metadata": {
        "id": "XvWpgt4JjV4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning models these days require a significant amount of computing, memory, and power which becomes a bottleneck in the conditions where we need real-time inference or to run models on edge devices and browsers with limited computational resources. Energy efficiency is a major concern for current deep learning models. One of the methods for tackling this efficiency is enabling inference efficiency.\n",
        "\n",
        "**Larger Model => More Memory References => More Energy**\n",
        "\n",
        "Pruning is one of the methods for inference to efficiently produce models smaller in size, more memory-efficient, more power-efficient and faster at inference with minimal loss in accuracy, other such techniques being weight sharing and quantization. Out of several aspects that deep learning takes as an inspiration from the area of Neuroscience. Moreover, pruning in deep learning is also a biologically inspired concept. Now, let us begin with it's implementation.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/720/1*nicFUkeUpWMW1w_hUVtZiw.png\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "L0Hg1PF-jVRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required libraries"
      ],
      "metadata": {
        "id": "teKufNMwi3Fw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNk2Xgu6UTG3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import copy\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX_iW6N-sLkF"
      },
      "source": [
        "### Effects of Pruning Neural Nets\n",
        "\n",
        "\n",
        "\n",
        "Pruning is deleting connections in a neural net in order to improve generalization and reduce computational resources.\n",
        "Two most common type of pruning are :\n",
        "\n",
        "1.   Weight-pruning\n",
        "2.   Unit-pruning\n",
        "\n",
        "In **weight pruning**  the largest weights by absolute value are set to zero.\n",
        "While in **unit-pruning** , the smallest neurons are set to zero by a vector-wise metric like **L2-norm**.\n",
        "\n",
        "Here, we examine the relationship between pruning and accuracy on a vanilla neural net. Before running any experiments, we hypothesize that accuracy for the pruned neural net will slightly rise (due to the regularization), and then have a negative linear correlation with the amount pruned. we also hypothesize that unit-pruning, in deleting entire neurons instead of individual weights, will have a more dramatic negative effect than weight-pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZmejOzw8MHS"
      },
      "source": [
        "First, let's load, normalize, and visualize the MNIST dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QgQR6UUlwF"
      },
      "source": [
        "def load_MNIST():\n",
        "  \"\"\"Function to load and normalize MNIST data\"\"\"\n",
        "  train = torchvision.datasets.MNIST(root='./data', download=True, train=True, transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                                ]))\n",
        "  test = torchvision.datasets.MNIST(root='./data', download=True, train=False, transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                                ]))\n",
        "  print(\"MNIST datset loaded and normalized.\")\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train, shuffle=True, batch_size=100)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=test, shuffle=False, batch_size=100)\n",
        "  print(\"PyTorch DataLoaders loaded.\")\n",
        "  return train, test, train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X3rzpxCXv4R"
      },
      "source": [
        "def visualize_MNIST(train_loader):\n",
        "  \"\"\"Function to visualize data given a DataLoader object\"\"\"\n",
        "  dataiter = iter(train_loader)\n",
        "  images, labels = next(dataiter)\n",
        "  print(\"image shape:\", images.shape, \"\\n label shape:\", labels.shape)\n",
        "  # visualize data\n",
        "  fig, ax = plt.subplots(2,5)\n",
        "  for i, ax in enumerate(ax.flatten()):\n",
        "      im_idx = np.argwhere(labels == i)[0][0]\n",
        "      plottable_image = images[im_idx].squeeze()\n",
        "      ax.imshow(plottable_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awglncFta6ga",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "4b3fd1ff-454a-4ae1-e24e-a86b4aa055db"
      },
      "source": [
        "# load and visualize MNIST\n",
        "train, test, train_loader, test_loader = load_MNIST()\n",
        "visualize_MNIST(train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST datset loaded and normalized.\n",
            "PyTorch DataLoaders loaded.\n",
            "image shape: torch.Size([100, 1, 28, 28]) \n",
            " label shape: torch.Size([100])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avoio0W48Yzi"
      },
      "source": [
        "### Build Vanilla Neural Network\n",
        "\n",
        "Now let's build a vanilla neural net with four hidden layers without pruning.\n",
        "\n",
        "We'll keep things simple and leave out biases, convolutions, and pooling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnUxUg7iU4yn"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  \"\"\"A non-sparse neural network with four hidden fully-connected layers\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.input_layer = nn.Linear(784, 1000, bias=False)\n",
        "    self.hidden1_layer = nn.Linear(1000, 1000, bias=False)\n",
        "    self.hidden2_layer = nn.Linear(1000, 500, bias=False)\n",
        "    self.hidden3_layer = nn.Linear(500, 200, bias=False)\n",
        "    self.hidden4_layer = nn.Linear(200, 10, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden1_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden2_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden3_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden4_layer(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqA-fHwd8toX"
      },
      "source": [
        "### Model Training\n",
        "\n",
        "Let's train our vanilla neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJcwD5-NeJBS"
      },
      "source": [
        "def train(model, train_loader, epochs=3, learning_rate=0.001):\n",
        "  \"\"\"Function to train a neural net\"\"\"\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  time0 = time()\n",
        "  total_samples = 0\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(\"Starting epoch\", e)\n",
        "    total_loss = 0\n",
        "\n",
        "    for idx, (images,labels) in enumerate(train_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      optimizer.zero_grad() # zero out the gradients\n",
        "      output = model(images) # forward pass\n",
        "      loss = lossFunction(output,labels) # calculate loss\n",
        "      loss.backward() # backpropagate\n",
        "      optimizer.step() # update weights\n",
        "\n",
        "      total_samples += labels.size(0)\n",
        "      # Compute the loss\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if idx % 100 == 0:\n",
        "        print(\"Running loss:\", total_loss)\n",
        "\n",
        "  final_time = (time()-time0)/60\n",
        "  print(\"Model trained in \", final_time, \"minutes on \", total_samples, \"samples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51kd-weohlPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb37c643-b68b-43e3-b6ad-ac397283babc"
      },
      "source": [
        "model = Net()\n",
        "train(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0\n",
            "Running loss: 2.302492380142212\n",
            "Running loss: 74.93614676594734\n",
            "Running loss: 109.5449490994215\n",
            "Running loss: 135.81745725125074\n",
            "Running loss: 159.7384386882186\n",
            "Running loss: 178.22458263859153\n",
            "Starting epoch 1\n",
            "Running loss: 0.19232377409934998\n",
            "Running loss: 16.27060179039836\n",
            "Running loss: 31.60938796401024\n",
            "Running loss: 46.37186377868056\n",
            "Running loss: 61.84353853203356\n",
            "Running loss: 74.9751727906987\n",
            "Starting epoch 2\n",
            "Running loss: 0.16433681547641754\n",
            "Running loss: 11.866175523027778\n",
            "Running loss: 22.473620121367276\n",
            "Running loss: 33.328746491111815\n",
            "Running loss: 44.458093415014446\n",
            "Running loss: 55.22322413139045\n",
            "Model trained in  2.123607134819031 minutes on  180000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArL0CsYL83AB"
      },
      "source": [
        "### Model Testing\n",
        "\n",
        "Now we'll test our vanilla neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u61CAZip1UNx"
      },
      "source": [
        "def test(model, test_loader):\n",
        "  \"\"\"Test neural net\"\"\"\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (images, labels) in enumerate(test_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      output = model(images) # Forward pass\n",
        "      values, indices = torch.max(output.data, 1) # maximum probability of the predictions\n",
        "      total += labels.size(0)\n",
        "      correct += (labels == indices).sum().item()\n",
        "\n",
        "    # Accuracy of model\n",
        "    acc = correct / total * 100\n",
        "    # print(\"Accuracy: \", acc, \"% for \", total, \"training samples\")\n",
        "\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAz77644j08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ee161e-b406-430a-ebff-6db3a1cfcdd0"
      },
      "source": [
        "acc = test(model, test_loader)\n",
        "print(\"The accuracy of our vanilla NN is\", acc, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of our vanilla NN is 96.98 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8QEw_mWGA3D"
      },
      "source": [
        "A ~96% accuracy for our vanilla neural network seems reasonable. Now let's do some weight and unit pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function for weight pruning"
      ],
      "metadata": {
        "id": "mao97ZAki8H2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3EavCDY4mOg"
      },
      "source": [
        "def sparsify_by_weights(model, k):\n",
        "\n",
        "  \"\"\"Function that takes un-sparsified neural net and does weight-pruning\n",
        "  by k sparsity\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, i in enumerate(sparse_m.parameters()):\n",
        "      if idx == 4: # skip last layer of 5-layer neural net\n",
        "        break\n",
        "      # change tensor to numpy format, then set appropriate number of smallest weights to zero\n",
        "      layer_copy = torch.flatten(i)\n",
        "      layer_copy = layer_copy.detach().numpy()\n",
        "      indices = abs(layer_copy).argsort() # get indices of smallest weights by absolute value\n",
        "      indices = indices[:int(len(indices)*k)] # get k fraction of smallest indices\n",
        "      layer_copy[indices] = 0\n",
        "\n",
        "      # change weights of model\n",
        "      i = torch.from_numpy(layer_copy)\n",
        "\n",
        "  return sparse_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function for unit pruning"
      ],
      "metadata": {
        "id": "B1-8zJjojErP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTb5ptg80hqV"
      },
      "source": [
        "def l2(array):\n",
        "  return np.sqrt(np.sum([i**2 for i in array]))\n",
        "\n",
        "def sparsify_by_unit(model, k):\n",
        "  \"\"\"Creates a k-sparsity model with unit-level pruning that sets columns with smallest L2 to zero.\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "\n",
        "  for idx, i in enumerate(sparse_m.parameters()):\n",
        "    if idx == 4: # skip last layer of 5-layer neural net\n",
        "      break\n",
        "    layer_copy = i.detach().numpy()\n",
        "    indices = np.argsort([l2(i) for i in layer_copy])\n",
        "    indices = indices[:int(len(indices)*k)]\n",
        "    layer_copy[indices,:] = 0\n",
        "    i = torch.from_numpy(layer_copy)\n",
        "\n",
        "  return sparse_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX21TSoq3udv"
      },
      "source": [
        "def get_pruning_accuracies(model, prune_type):\n",
        "  \"\"\" Takes a model and prune type (\"weight\" or \"unit\") and returns a DataFrame of pruning accuracies for given sparsities.\"\"\"\n",
        "\n",
        "  df = pd.DataFrame({\"sparsity\": [], \"accuracy\": []})\n",
        "  sparsities = [0.0, 0.25, 0.50, 0.60, 0.70, 0.80]#, 0.90, 0.95, 0.97, 0.99]\n",
        "\n",
        "  for s in sparsities:\n",
        "    if prune_type == \"weight\":\n",
        "      new_model = sparsify_by_weights(model, s)\n",
        "    elif prune_type == \"unit\":\n",
        "      new_model = sparsify_by_unit(model, s)\n",
        "    else:\n",
        "      print(\"Must specify prune-type.\")\n",
        "      return\n",
        "    acc = test(new_model, test_loader)\n",
        "    df = df.append({\"sparsity\": s, \"accuracy\": acc}, ignore_index=True)\n",
        "\n",
        "  return df,new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIWGnpQZHYSi"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WgshrNX5Is2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75c085a-1f6b-42db-c1a9-38f7ee7a9b4c"
      },
      "source": [
        "df_weight,weight_pruned_model = get_pruning_accuracies(model, \"weight\")\n",
        "df_unit,unit_pruned_model = get_pruning_accuracies(model, \"unit\")\n",
        "\n",
        "print(\"Accuracies for Weight Pruning\")\n",
        "print(df_weight)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Accuracies for Unit Pruning\")\n",
        "print(df_unit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies for Weight Pruning\n",
            "   sparsity  accuracy\n",
            "0      0.00     96.98\n",
            "1      0.25     97.03\n",
            "2      0.50     96.94\n",
            "3      0.60     96.90\n",
            "4      0.70     96.92\n",
            "5      0.80     95.10\n",
            "\n",
            "Accuracies for Unit Pruning\n",
            "   sparsity  accuracy\n",
            "0      0.00     96.98\n",
            "1      0.25     96.96\n",
            "2      0.50     96.77\n",
            "3      0.60     96.77\n",
            "4      0.70     94.75\n",
            "5      0.80     76.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's visualize weights and compare it with unpruned model."
      ],
      "metadata": {
        "id": "XHCjIOdNOdio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_weights(weights, n_weights):\n",
        "\n",
        "    rows = int(np.sqrt(n_weights))\n",
        "    cols = int(np.sqrt(n_weights))\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    for i in range(rows*cols):\n",
        "        ax = fig.add_subplot(rows, cols, i+1)\n",
        "        ax.imshow(weights[i].view(28, 28).cpu().numpy(), cmap='bone')\n",
        "        ax.axis('off')\n",
        "N_WEIGHTS = 9\n",
        "\n",
        "weights = model.input_layer.weight.data\n",
        "\n",
        "plot_weights(weights, N_WEIGHTS)"
      ],
      "metadata": {
        "id": "N9I-ciGKLXMu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "1a25dbd0-0e5b-49a9-a780-e3762bf3211f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABTwAAAMWCAYAAADcdEn9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACnQElEQVR4nOz6ZXRU5xu3/Z8hIZ5ACB7c3Utx1yIFSnGXQpFCgeJSKFYolKItlNJixb04hWLFrbhDggaICyHyf/F/+TzrO6z1u9e617M5Pm+PPTPX7JmwrznZbmlpaWkGAAAAAAAAAA6Q7v/2AgAAAAAAAADg/xQGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDI/3PXDi3OWyu6XTs9MH/z2QPVvebLK/jX8ru5lZXFSs7Onc3WXPVTSX7NGvo2UPyBQge8q7FNlfPn4pe2BwoOxevl6yP7n9RHYzM7+MfrJHPH8ju7eft+xJie9kL1yxsOyu3L14V3ZXn1HGLBldvsarJ69k9w30lT1zSLDsUeH6ezZrwgDZAcCpbj59KvsvSzbK/nm3T2RfvWSL7NVaV5fdzGzhN9/L/v2KWbL7e+vr6D8nL8re4ZO6si/5Tb/Hh1cfyj5gXHfZ/z54SvY69SrLbmZ27OgF2TNmzSh7Ylyi7Ec3HpX91asw2aOiwmUvU6GG7OOn6+v4hK9/lN3MrNM37WWfNXiq7P1nfC37jgXbZd+0aY7sAOBUTZt+IXu99g1lb1TvY9krFdS/h7NmzSO7mdnt+//JHpOoZyttmvaQvfRH+lo+beog2TP5+8u+8fRp2V+7mElMHfCV7D2GjJTdzGzdL4tk//PAZtlL5dKzpXcpqbJXKFVV9pYde8ge8SJS9uePQ2Vv2Lmp7GZm109el/2HOcNl33Jc7xl3LNZ7kR07Fshuxh2eAAAAAAAAAByEgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMfweN8DH157JHuh8oVkz5glo+ye3p6y+/h7y25m5umjnyMhNkH2O+duy17s4+Kyx0bGyJ6WmqZ7mu7v3ibJ7h/kL3tQ9iDZzcxiI2Nlz5Qj+H9ag7ef/hwfXdffs5wFc8qeLW822d88fyN7RIruZmbRr6Jl9w30lf35gxeyu/oeAMCHavmvW2XPHKKvUSsXbZb9xSP97/Oze89kNzP7fsUs2eeMXCR73ym9ZN+xbKPsbRrVlL16w49kf3r3iexZAgJk37Z8rexVa5WX3cxsx2/rZf9y5jDZN8xbLXu9tk1kv3bCS/bR88fK/iZG76W27Tkqe8nqJWU3M1s7W5+jijVryJ4ri/5byZY/u8s1AMCHaN5vk/+nx5fNV1D2p6/DZe/ZaZTL19h86ozsHWtUl73P5MGy927UQPbqbfTzXzx4UfYytcvIvnnBn7JvO7ZP9r92/CO7mdmXE8fLXj5vPtlvPtN7xmrFS8vunk6P6np80Ub28zfv6n7gvOwH1u6V3cyseovashfKU1j28uX19+jXdXNcrsEV7vAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACO4fG+B+YvnV/2O+fvyJ6akiK7f5C/7B6e6WU3M3sbHyl7cI5Msnv7ecv+9O4T2UOKhMj++ukb2TOHZJb9wZUHsnv7+7h4/mDZzcx8A31lT4xLlD30xmPZs+TOIntSQpLsKcn6e5QunZvsgcGBskc8j5DdzCwgOEB2V9+jl69fyu6fUf8tAMCH6tm9p7LXblFNdp8AfY3z9PaU/V3SO9nNzNYs2Sp7apq+jvl6esnec2I/2bu07i97VNQr2TNnziV7h2bdZS9bubrso3uMlN3MLDExTvZDqw7J7uWlP+dilYrIXqFaKdlvXL0ne94iuWUvUb6w7Inv8T27cuSK7KcP/y370OFdZa/7eW2XawCAD1HloqVlf/Rc/x4OypRD9texsbIv+WOq7GZmy/7YJntqtaqyu9pv5chRQPay+fPJnlY3VfYlY+fIvv/wBtnXHz4me/4yev1mZkPafC77vtU7ZS9dpYLsXfqMlj1vybyyJ7zTe4Wfx+tzePKUXv/sX9fJbmYWdkfPx56/DJXdz8dP9t7t02T/++/VsptxhycAAAAAAAAAB2HgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMj/c9MDUlVfbMIcGyBwYHyv7y8UvZYyJiZTczK1ShkOz3Lt6TPTCzXmM6D3fZLxy4KHvOQjllj4+Ol909vX79WBfnKM3FZ2hm9vTeM9mDc2aSPaRwiOwZsmaU3T8oQPaYN9GyZw7JIvu7t0myv3n2RnYzs5jXMbIHuXiP2fJmk/3h1Ycu1wAAH6LEuETZ79x4KPsndavI7tWohuxb9hyV3czs/Il/ZA8K0teA5TPXyO7t6yV7y97tZY95o69hye+SZb964rLs+3askv3bXxfLbmY2b8Rk2Q/tXS/74XNHZP/ll02yZ86VWfaIFxGyt21SW/ZqZavKvnzPZtnNzJr0biz7odXpZW9Zp7XskVF6X972yR3ZAcCpFu7aKvt3s5bL3n3wCNlvP38ue+ncuWQ3M/MN9JP96K1bsnv7+8g+efkS2TP6+soeGOgve7Zs+WWf88s62ccP7ib7gCEzZTczy5gxq+y79/4m++tYPZuJiIuT/d+rN2X/onVv2Zdu1d/DDu1Gyv7zikmym5nlza6/i49vPpD9p436b6l6hVIu1+AKd3gCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADH8HjfA+9euCu7e3p32X0CfGX3DwqQPfldiuxmZs/uPZPdN1CvIShbkOzXT16XvdjHxWR/evepfv3s+vVfhYXLnpzRX/brp6/Kbmbm7esnu3t6/ZW5e/Ge7CWrl5A9ITZBdi8fL9lvn78te/7S+WUPKRwiu5lZYHCg7BHP38ienJQse85COV2uAQA+RN3Gdpb9296jZP9x9DjZB0+ZLHtS4lvZzcyadW4ne5V6FWWf4OI9bD/wp3786AWyZ8ufTfZmberKvm/9Jtl7DdPrL5Ivl+xmZmFht2QfOHaa7J1a9JL9+99ny37r7mPZ/V3st37+dbPsQ6dNl31Aq66ym5n1Gz9W9q+/6yP7g/BPZZ/ZX/+tAMCHalSHnrKXLFlddh8f/Vvy0tHzss9aNkl2M7MXD5/LXqNbG9kfPHwi+6uwV7KX6VBc9nVHD8qeq0hu2b/sqdefJTiH7L2+Gi+7mVmTVl1k33Hhgux/fPe77MU+Kin7ib36HHUe8qXsd0L1Z+hqfrdu3xHZzcyyZNGf05CZA2X39fSUPfFdkss1uMIdngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDI/3PbDox0Vlf/jfQ9mTk5Jl9/H3lv3Fo7eym5llz5dddk8fT9nfxuvXyFsij+xht8NkL1OrtOxpaTJbllyZZX9y96nsGYIz6Rcws7ioONkfX3ske1D2INmfP3whe3x0vOwhhXLKnrdEXtldncMHVx/KbmaWGJ8oe8Zs+hy8evJKdlffQwD4UGUOCJA9Z0gh2b19/GXPUyy37Ft+2iq7mVnxqsVlnzpgsuwte3SSvUqZKrIXLfqx7P1HdZb9dUys7CMW6vWH3gqVvWHZ8rKbmfn46M953S+LZP/o4yay791yRPZ2nZvKPqLXeNk3bF8ie+WyNWTPni2f7GZmuYrkkj3+bZLsFw9elL1avcYu1wAAH6Jvl+pr0KE1f8ue5uJH/5Eja2WvX2Gf7GZmhQtXkn1tTT2XmD96qux7/tkm+8je+jd/z77fyl6vUz3ZD1y+InupUjVlv33hhuxmZqmpqbJXK1xY9kn3Lst++vRfsncdMEL2dO763sXk5BTZE+P03OXjCiVlNzMLzqRnMxP7jJV93a7fZH/8+rXLNbjCHZ4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxPN73wBcPnstetHJR2e9fvi97amqq7B4e7rKbmSUlvpU99Gao7AXLF5Q94kWEyzXox0fKHvUqSvb4qDjZX4Q9k7109bKym5mFh4bL/vzxU9m9Yr1kf/1SP3+R8sVljwzX5yhD5gyyx0XHy168in59M9ffI0+v9HoNkfpzzFM8t8s1AMCHqM+n3WUvUuQj2fPk03uVcnnzyu41op3sZmYTegyTvWKVurL7+PvInpSk9zrf/TxR9i/a9JX97t0Lsteu3UH2Sg0qy161yqeym5mlpKbI3m2Mfg8F84XI/ufibbIvX7RRdk9Pb9lX7Two+1/H98i+98hp2c3MRnfsI/uef/fLXrmx/ltZMmaByzUAwIfo+cMXsgdlC5L9xCF9Dcifv4zsPUcPkd3MLGPWjLIHBPrJXqpMddn3ntF7hdAbj2W/c/2K7Ae7rpX91asw2S/evyv7rv0nZDczq1pNz05uPtNzkTx5S8o+cuE02RuW169fOFc+2b9fuUb2/64elX3qUJnNzKzLiC9l79+2mexe6fXcpHyR0rK/eaPnX2bc4QkAAAAAAADAQRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDAaeAAAAAAAAABzD430PzF4gh+wJMfGy5ymeR/ZH1x/J7uHtKbuZWWLcW93jE2W/8PdZ2XPmzy17trzZZL945JzsIQX0OYp4GSl72VoVZL9w+IzsZmZZQ3LK7usfIHt0RLTs+YsXlD17/uyyJyUm6Z6gu18GP9lfhb2S3czs3dt3st//74Hsbm76+VNT0lyuAQA+RGXK1ZL96+/6yH7ltv73eeG8tbJnz6+v82ZmPUZ9JXu1KmVl//f0Fdm/GDlB9rthT2W/d++i7Ev37Zb9wRV9Ds/u03uNRp1ayG5mdvmIPgf/bv9X9rpT+suelqavs5EvImVv0beV7CddrO+TulVk379KfwZmZtuO6mP6th0oe2RUuOwTf5vrcg0A8CFKl07fM7ZiyWTZpy9fLbuXi7nHuX3nZTczS0lOkb1l/+ay9xjdSfY8wcGyD/tpvewlK1SSvWWv9rIP7NJK9i7tvpH98pXDspuZfXVH75cC/fRcZPzcpbJvX7hD9p3pdsq+48xJ2Z++fiP74Knfyh4Q5C+7mdm4Hn1l9w3wlf31Uz17qV27g8s1uMIdngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDE83vfA+5fvy54ldxbZvVPTZM+YNaPsj288lt3MzM3NTb9GFv0a75L8ZI+NiJXdy9dLdh8ff9nTe+qPIyLiueyJcfllj4x8IbuZmbu7XkNcXJTsAQFBsr9NTJI94nmE7i90z1sir+z3Lt2T3dvPW3YzswJlC8ge/vil7G9cvMc7F+64XAMAfIhOHt8h+932l2T/uG592aPC9TVuyLAuspuZXQ0Lk93T3V325dPnyv5xjYay716zXvaQkCKyRzx/I3u2fFllL1KxqOwlKhWT3cysYNmCsk/pNVx2j3QDZI90sZd4Ff5E9tvnMsvul0HvJ++/1PuETmO6yW5mtnXzIdk7j+gr+5O7T2UvnjOnyzUAwIco6pXeK/QfPl3277/+WvbaddrJfv36KdnNzEqWriZ75oAA2T099Exg98GTsj96dE324pVKyd6yaU3ZF/yxWXZzMReq30SfYzOznj0myr72+FHZ71y6K/v9+5dkn716vuypaXq+VjZ/Ptn/PHld9vnj9Ps3Mxs2/XvZV0xbKPuQuWNl37tmu8s1uMIdngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDE83vdAv4x+siclJMkelC2j7N6+3rLHRsTKbuZ6jY9vPJY9KfGt7PlL5dePf/tO9tjYSNmrlasqe1pamuwX/jkpe0z0G9nNXK8xY8assucunFd2b38f2d+91d+j3MVyyx56K1T2XEVy/U+PNzPz9Eovu5u7/n+EZ/eeyp6nhD6HAPChmr91lezDPu8le7p0+t/nc6cOyD5hWIrsZmZXLh2Tfcaq+bIPmztZ9ltnbsleK/Mnsqckp8r+NlFfhzPnyiz78T36HL6N13stMzPfDL6y9xw1VPaG1ZrK3mngQNmPbn8ue8nqJWWfM3y87PHRcbJnzBYku5lZtrzZZD+4+qDs/hn8Zb8WGiZ7ubzsVQB8mKJeRcl+4tBu2Y9c+lf29Rv3y75//x+ym5mNWjhN9mqFC8vetes42c+d2Sf75eunZf+4Qj3Z+w1oJ/uuPzbJXqNpQ9nHjegpu5lZ+9aDZc8VHCz70CldZC9Vqpbsi8Yvk/2nXyfIXjinnnt4eHjKXqGCPodmZnfO35Z90i9TZV/67QrZjx3b6HINrnCHJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAyP9z0wKGuQ7KG3QmUPyBQge2pqquwenq6XeuPMVdnzFMkve1LCW9ljImJlL/pRUdkrN60s+8vHL2X39PGS/b+rR2V/8+aZ7GZmXp4+svsH6O9BSvI72X18A2XPmj2X7NfOXJY9fXp9jiJeRMjun8FPdjOzh9ceyZ41T1bZvf31OY5+He1yDQDwISqdO7fsO47ukr1do/a6f/GF7B/VLS+7mVnUuDeyl8ylr3OTF22TvUGX+rL/NGK67JVr1ZX9bVyi7Kd2npJ95ILxst+/qa+hZmZP7z6Vfcsva2UftWCm7PHR8bInJsTIvmHeatmLFasi+/X/Tsv+y6SlspuZjeytz/MX0wbL7uPlKbvLz6mGzgDgVBHP9HU+a9a8ss/7/g/dZw+X/aPa5WQ3M8uTObPsLZv3l71a81qyH9i3Uvb5v2+SvUw5/fyDuo2Wfe0OfZ1MSk6R/eTt27KbmQ2cMVD2/XtOyO7rm0H2cjUryj5kYEfZ61ZpLHtISBHZM2bMIvvCNXNlNzP7c9Vfss8ZPkf22q0byp6QlCS7j6fey5hxhycAAAAAAAAAB2HgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMj/c9MDE+Ufb8pfLpx8fpxz++ESp7SkqK7GZmgRmDZH9w447sWbJnlz1/mQKye3jq01kkT4jsrs5RunRusvv6ZpA9fXov2f//x3jLnjt3MdnDwm7J7umpnz85OUn2tLQ0Fz1VdovR+d3bd/oAM/ML9JU9ITZB9lxFcsnu7uHucg0A8CEa2G2s7HOXT5Z92Nwpsqcm673G3GFzZDczS01Nln3rnn9kL1OrtOw7F++Uvde4r2Qf2bWT7EWKVJI9KCiH7JdOnJHdwyO97GZma7cskP1571ayTxg0W/ZydcrJXqFmTdlrflZD9hwZMsoe4OMj+8zRi2U3MytVuaLs5fLllX377qOye3q5/pwA4EP04sVj2Scsnij70Hb9ZG/W5JrsfacOlN3MbOVv22V//fqJ7PMmjpJ9w7GDsqek6t/kbT+tL/vPi9bL3vPzIbKv2PiT7OnSub7v7++Nh2X/vHsz2S/9fUn2dcsWyR6YOVB2d3c9exoyXe95B3zeXPbOncfIbmYWHflG9sZdWsh+YO1fsr8KeyX7/JnDZDfjDk8AAAAAAAAADsLAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYHu97YHpPfWhKSur/tJD8ZfLLnhAT7/I5bl24IXu2XCGyV2pcSfan957K7hvgK3uwv7/s/Vs2kf3fu3dl95g1Q/bz+8/Lbmbmn9FP9vDQcNm9vfXjnz+/r1/fP0j2jMHBssdFx8qelpYme4bMGWQ3M8tRMIfsMW9iZI9+FSX7u6Rkl2sAgA9R9Ra1ZT98+pLsf/26Q/a8RfVeJDLyuexmZuUr15I9PiZB9tzFcsteu70+B64s3L5N9p4N6sp+98UL2f86fEr2vq31XsfMbPLs5bJfPn5B9oad9GuULl9U9tu3Hso+b+h02Vv0aC+7h2d62Sf98JXsZmY3nuo96aj+02Sv076+7FEvI12uAQA+RGfP7pF9/ZIysrf9oo/s+9ZtkX1Qi89kNzMbPn227AeP6te4//Kl7CuXbZX9j8Xfy/7nP/tlv3TirOyfdG8t+4adh2WfMWyw7GZmLVr3k71mseKy585dTPaCBcvLPrRnW9kbNqwq+6ql22Rvt3647GFht2Q3M+v4lf4ud2rZQPYDa/+Sfdq3A1yuwRXu8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7h8b4Hht4Mlb14lRKyJ79Llv3Ouduy+wT6ym5mVrRCcdlzFMwp+8vHL2UPyhYke4YsGfTrZ8wo+/OoKNmzZwiUPUvuLLKXq1tWdjOzwqUKyH5s17+yB73MJLuPj7/sWUNyyJ4tX1bZ3yYkuehvZU+MSZDdzCzyRYTsHp7pZY+NjJPdL4Pr7zoAfIhiI2Jkz10st+z//XdU9vTpvWQftWSq7GZmx7eckD3CxTXk6d2nsncf1Fb2H0YtkX3YjH6yP4uMlD0qPl724iX1PmLu8o2ym5l16tZc9prNqsre6qPKsmfJkkf2UXPnyD5w1kjZj6w7Intqcqrs/hn1XsnMrFlN/R4ffFZX9mOb9N9Cx9EdXa4BAD5EVx7ckX3b3mOyV6laRva4KP1bsXm39rKbmV0+ckX2FYF+stevWUn23RvXyl616qeyX714S3YPDz2mcnNzk33SgN6yJybqc2xm9t9FvZ/7vP0I2V+91Pu5giX17ComUc8t3qWkyD5zykDZ8+TU+7Wu/b+R3cysRNnCsv+xaa/srQfq73LZEh/Lfv/+ZdnNuMMTAAAAAAAAgIMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4hltaWlra+xw4ecEfsifGJcoe8yZG9pR3ybJnzpVFdjOz8NBw2eNj4mWv2rKq7L4BPrK/fvZG9nTp9Hy5e8uGsj+NiJA99I1+/TNHL8luZhbxQr9G/tL5XT6HcvfCXdkfXn8ge65CuWS/dfmq7IVKlZDd1ffYzCw+Kk72oByZZM+RP7vsd1yco7WrpssOAE5VvLi+Tn/SvpPs+UrmlT3yZZTsHds2kt3MLDklRfb+HYfJ7uXlK3tAYJDsI78fIPuDcL1XWjNttexhYbdkj4x6KfvwH1xfwzLnDJY9S2CA7FMHzZC9QPFiss+Y8ZXse85flD1TxkDZH94Lk71N/Rqym5mN+Wae7G5ubrK/e5ske7+x3WSvVUyfQwBwqhGTF8r+yed1ZZ83aonsFy7ul/32vf9kNzMLj9Gzl99W7ZB96ogvZM+cWf8m/+OAfv7tK3bLnjGb3uvs37hF9iLFKsieGO/6N/8fa/V+pXVzfY6io1/LXr91C9kDg/Veomf7ZrLnDNIzia3nzsqekpoqu5nZo9uhsj+//1z2ZXOnyD5y1o+yj+nbUXYz7vAEAAAAAAAA4CAMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGN4vO+BD648kD04JFj27Pmyy/766SvZb565KbuZWc6COWUPyBQg+9ndZ2QPDskse+mapWQPyaoff/3JE/36/v6yP32hz2H2fNlkNzNLSkySPUPmQNnfPI+Q/cb5y7K7u6eXPTEuUfZsOXPL/s7F+3P1/GZmyckpsifEJMh+88wt2fMUz+NyDQDwIcqfv4zs7bt+IvuQzl/LPnjOaNmTU/S//2ZmI/vPkL1k5Yqyv3v7TvbzJ/6RvUH5yvr1S1SXvVbzJrK/DH8ke+nStWWf0KeP7GZmPQaNkX3rql9lH7t4juzTBwyX/a82NWRvV6Oq7HFv38r+4uUb2Q9c1HslMzNvP2/Zm3ZrJPuSMYtkv377oey1ihWTHQCcqlD5QrKv/2WH7Klpei9Rp0E72f+6dEl2M7OGpUvLvnPln7K3ajVE9tjYSNmvXbkj+3dTBsp+JfSx7Mt+mCL7vLXzZf9lxirZzcwyZwiS/WWkvpaXLf6R7Nf/vSb7qVP6e3Tqr1Oy9/xikuxjuw6WffyyH2Q3M/t5st7zRka+lH390QOy/z5Lf0+tb0fdjTs8AQAAAAAAADgIA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAY3i874HZ82eTPTA4UPaYiFjZvXy9ZA/KFiS7mVnozVDZffy9ZU9KSJI9Q5YMsp/bd172k4n6+TNm1c9fr0UN2fPm1J9ReEy07GZm5WqVkf3xnTDZ3d3dZQ8ICJY9MvKl7EkuzmGBsgVkT4xLlN3DK73s7/Mcrs5B5lyZ9ePT68cDwIeqVNWysnt66G1NXLy+Dk7uNVR2X98A2c3MvvxupOyBmfV+qUj2HLK3+2uj7L2HTJT9yK4dsp/cd1j206d3yf7Dqk2yN+/XXHYzs3E9+sp+5to52Y9evS57cHBO2VdMWSL7tfrXZK/cuJLsdy/elT0xNkF2M7NuAz+T/a+tR2QvUVnv936ZNFv2/i2byA4ATvX07hPZt67V15DADPq3YGpqiuwrlk+W3cxs6oKVsrfs2Vn2Cwf1XGPhH9NkX7J4g+yxifr3dGziW9nX/7Nf9rPn9HX68IH1spuZJSTGy37g6lXZB0zS+7Hfvp8jezoXM4W8xfPJPnJkD9l7tr8n+6W/L8luZvbnPv05D+sxWvZ/9pySvd/Yri7X4Ap3eAIAAAAAAABwDAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMfweN8D01LTZL9z/q7swSHBsnv5eMke8zpGdjOzLLmzyB5656Hs6dLp+e+lE2dl90yv30OhssVljwqPkv3ovtOyZ8ySQT//q2jZzczS0vTnnLNQTtkvHLgge9Y8WWV3c3OTPdXF9zA+Ol72twlvZffwcJfdzMzbz1u/Rrx+Df+M/rInxCa4XAMAfIhK1Sgp+x/Ltsr+eb/esh9Yv1P2EhUqyG5m9vPEObL3GjtY9myB+lpesGB52cMfv5Td2ztA9k4jesneZWQf2S//c1l2L29P2c3MSpWqKXvcW32dvXnmluyDvh8t+/eDxsheunZp2SsVyC975gD9Gfxz8IzsZmb92ujv8oZ962Tfe0TvKbcM+dPlGgDgQ3Rgo94rPHn+QPaVB4/IvmzSPNkfvXolu5mZWzr9mzo8NFz2ly8fyn7sv+uyF6tcVPaTt+/IPuyzzrKXL99Q9gYdG8k+ZOo02c3MZi5dK/uEAd1lfxSu92PTvx4ke61an8t+bN9fsvsG+Mju46NnEg+vPZTdzOz8tduyr96yWPaxI+bJ7tkmvcs1uMIdngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDE83vdAN3c9G82aJ4vsbxPeyp7eK73saWlpspuZxbyJkb1U1bKyXz52Tvbk5CTZnz69I/ur109kr1yrruwPrtyXPVOOYNljI2JlN3P9Ob0KeyW7q88g8nW4XoCbm8yJifp7khCbIHu6dPp7nDFbRtnNzKJeRsqeJbf+W3BP7y67l4+XyzUAwIfo8J9HZM+WL6vs9y7dk73TiF6yN65aQXYzs8F3H8ke/Tpa9h1/HpD91OmdspcuXUv2S5cOyV7zSiPZi1UpJnuGzBlkP7h5u+xmZp6e3rJfCQ2V/fq/12R39R4+7dpN9oRovdeYOvFn2d1c7EV6DW0vu5nZkS37ZD944rzs3n76HD8I1/u1/Fn03xoAONWtW6dlP3brpuwHVx6UPTY2UvZMfn6ym5nFRcbJ3r1/G9k3/D5f9o61a8j+JCJC9i+7jJR9wuKFshcslFs/vvdo2W/ePCW7mdmaf/Tn1KCB3iscu3lL9jcRL2T/69Il2UMfPpN96sCvZI+K0tf5G4/17MnMrFT+IrJ779gqe48RHWRPTklxuQZXuMMTAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4hsf7Hvju7TvZ06XTs9OsebLKfv3fG7L7ZfST3cz1Gh9cfSB7YIZMsmcNCdE9ax7ZY2MiZL967rzsufIUkv3+1Tuye3r6yG5mVqi8fg03NzfZAzIFyB4cqc+xe3r9lbxx/rLsHh6esrv6Hj69Hyq7mVlgUEbZfQJ8ZXd3d5c97FGYyzUAwIcoQ5ZA2S8d1dfR9On1NSI+Jl72Vg3ay25mNn/dYtn7te4he5Ysei/h5aWvMa6ug0Mn/SB76I3HssdGxcp+5sjfsn81e4zsZmYPrz2SvV/j5rJnyZJb9shJL2Uft3Cc7OsWbZH94O71svf4eoTsC6eskN3MrN+Mr2S/fvK67LGR+nO8eeqm7PVmlpQdAJzq6j3972tMYqLst27ovUqGDFn0458/l93M7Pb5W7IvCXsl+6Owu7J36zxW9qz5ssnu55dR9tIlCsqe3kPPDDw9vWXvOXC87GZmk3p/I/uAaaNkT0lJkX3Y+J9kj42Ikb1+lwayR0WFy96mnd5H1ChfU3Yzs2799Z7OzV3PCK9cvi17mbJFXK7BFe7wBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjuHxvgfGR8XJ/i4pWXbfQF/ZC5YtIPu5g2dkNzMrXaOc7O7u7rKHh4XL7uPvI3u2fNlkT4zLIfu7t+9kfxX2SvaQgnlk9/DQ79/MLD4mXvZM2TPJHhUeJXtIkRDZz++/IHtysj5HMTFvZC8SXFT2rHkqym5m9vTeU72GNzGye3qnl93V9wAAPlR12tSSfcjQzrKnc9P/z3v/5UvZA2aMkd3MLCklRfY8eUrI3rxPa9lX/6Cv08ePb5L99u2zsgcGBss+aNoE2XeuWy773K8ny25mNny+Pua3g3tk79WgqeyVq+i+fsk22WfPHib72vrlZc+UQ5/j5T/MkN3MrFD5grIH59Sv8fjGY9lvX73oYgX6HACAU33WtLvszbq1k93V79Xw8FDZvx3o+hqxYt0Psjer11b2PCFLZc+Ro5Ds7UbMkn1sl4GynzhWRfaAIH/Zm3TSe6mbp2/Ibmbm7q5/szf4qJzs5x88lP33hdNkHz59ruwpLvabf507JfvZ01dlT3Yx3zMz8/T2lL1m8WKy38+s992flP9I9sS3ek9sxh2eAAAAAAAAAByEgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMTze98BcRXPJ/vDqI9m9fDxlj4uKl71klTKym5m9ePBC9oBMAbJ7eOrTkd4rvewvH7/Uz+/hLntamsxWsXFF2f/ZfEj22NgI/QJmVqRkWdlT3qXIns5Dz9DDQ8NlT0pKkD1z5pyy5y9TQPbo19Gyx0XGym5mFhgcKHtCjH4Prr6HrjoAfKiWjF0se5FyJWTPkCWD7Ie37pa9WuP6spuZZQzWr5Elew7Z2zasKXtIfv34QS1vyv7noZ2y+3jq/dqxExdlr1CxsewJCTGym5lFv9LX6gsX7sg+f+tm2WuX0d+Tzxp1ln3I4LeyP7hzXfZ8hYrLvv/0YdnNzH5esF72vt1bye7pYl/u4+/jcg0A8CFa+OdPsucMCpK9YPmCsg9r00X2LTsWyW5mNmDgDNmvXz8pe4/+E2VPfpcs+9efdZX9y/H6+T1dzF36tf5E9vlrtsqeq4iebZmZ7dq2VPaiIXlkn7xohexppoc/RSsVkT1LoJ4ZNC1fSfanr/Xs6usT12Q3MxvzTQ/Z37r4nmxfvV92bx9/l2twhTs8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAY3i874E3Tt2UvWC5grKH3gyVPfp1tOw5CuaU3cwsMHOg7DEuXiM1NU325KRk2dN76tOZJU9W2V8+fCF7zJsY/fw5csie0zOP7GZmiXGJssdG6DXkL1NA9uR3+hzmyJdLdi9fL9njouNk9w3wkf3Vk9eyv4906dxkT03R37P0Xun/5zUAgBN1GtVV9r3L98nu6hrX4PMWsh/eskd2MzMff2/Zs+XLLvvWwydlP7jqoOwlSlST/dz567JnyJxB9smD+sk+ePxM2U/8dUh2M7NypQrLfnTjUf34euVkr1OpruxbjuyS/XpomOzpPT+RfcHw2bLfePpUdjOz4SO6yV63cgPZy5WvJ/vAKb1drgEAPkTbt/4t+/WT+jq7YvU02Ts9vy/7Wxe/p83Mxk/9UvbJMwbJHvf2rext6reR/d6Da7IfuHpV9nbVa8v+6qn+zX756AXZv5ym9zJmZhO/0df6q2G6X7n9QPb23YbJ3rdRM9kDA4Nlz55Dz2Ua1Wkr+7b9a2U3M1u6eqfsi6d+K7u3t5/sPj4BLtfgCnd4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDAaeAAAAAAAAAByDgScAAAAAAAAAx/B43wMDMwfK/ujaI9mzF8guu5ubm+yvn7yW3cwsKfGt7BkyZ5A9OTlFdr9AX9lTklNlDwzW5zA5KVn2pMQk2fOWyCu7l6+X7GZmoTdDZS9QtoDsrtYYGxEre2qK/gwy58osu6vvSfSbGNkzZs0ou5lZWlqa7Ok99Z9V9Oto2V8/eaUX0EdnAHCqS4cvy166dmnZf546VfZeI0bKnpz8TnYzM/f0+hqQs2AO2W+fuy37u3eJsnca2UP2G6duyL7z102y9xo8XvaC5QvKnqd4HtnNzG7cfyx7/jL5Zf/9u2Wy9/5mlOwLZ6yUfdS3fWWPT9J7oY7Desm+evpa2c3MliXo/cyqfRtl7/NpD9lnDdfPX3fPUtkBwKlun7sj+9Gj+jr6+NXXsufPX0b2YV//ILuZ2ZJFY2XPllnvRYZ+O0v2UxcOy16ogH4PE5bMk/3G4/uyH7txU/aXj17IfvvaA9nNzJaM+0X2zdsWyH7hqv6euJqrDJs6R3Z3d3fZK9XQn8Gc4fr5C+YIkd3MLF06vee9GarPc2R8vOzXnzxxuQZXuMMTAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7h8b4Hvo1/K7t7enfZI19Eyh5SOET2OxfuyG5m5hvoJ3tsVJzs+Uvlk/32Ob2GvCXzyH5u7znZcxTMIfu9/27JnjUkp+w5XTy/mZl/kL/siXGJsqf3TC97ULaMssdGxMj+8uEL2d88j5C9TO3Ssj+7/1x2M7Oo8CjZ8xTX3wN3D/3/DLmK5nK5BgD4EDVtV0/2nav3yz5o8reyv3n2WvYsWXLLbuZ6L1Esj/43vv+8pbLHxetr0L1L5WU/9/dx2ScumSB72xoNZF/66XbZK5XLL7uZWaPqn8iempIs+5r9G2Uf2nmY7HVaNZN9/FdzZI+KeiV7WNht2Ru0bCu7mVnfQe1kX//nXtlz5y4me4agTC7XAAAfooLlCsr+JLSy7Jn89e/tX3askf300Uuym5ldCwuTfebKVbLXqKh/M3fvOFr2gZMmyf7sgf7N/VfyGdkfX38k+9QZX8m+cf9R2c3Mdu/+RfZlu/Wec/7oKbKPWzpb9hn9Rsn+9Nk92W8/0nsNX59A2Rdu1fs5M7P+LfR+7fqTJ7JfvajXmL1AdpdrcIU7PAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGN4vO+BibEJsvsE+Mqempoq+/OHz2UvUrGI7GZmV49fdXmMXsML2dPS0mSPjYyTPXex3LK/S0yS3d1df1yZQzLLfuf8XdnNzFJSkmVP7+Upu4+/j+ypKfp7EFIop+wuPgLLnEufg3uX7skemDmDfgEzS+eu/5/g8Y3HsocU1u8xKfGdyzUAwIdoaPt+stdt1lr2ds3ryu7m5ib7xPsLZDcz2/XLX7Kveq33Gm/fxss+aOp42V89eS371t3LZf/xl3Wy581bQvbTe8/K/uvJpbKbmWXPnl/3nPlk/7Ltl7KXr1JT9ivHLsr+65qZskfF68/ws4btZD++f7fsZmZFKhWWfc+6jbL/tnWZ7NNGuP6uA8CH6MJhfZ07c2aX7L+u2Sl7hiz692iuorlkNzNbvmCD7INHdJU9b+Zg2ftP6S37N50Gyt5z1FDZvf28Zd+7QV/jfp41SfbSpWvLbmbm7esle8H8+nN4l5Qo+5TeX8u+57jeTz4MD5e9c9shstf8tJ7sHp7pZTczK1b0Y9nfJruaLenXOLHlhOxda+n9nBl3eAIAAAAAAABwEAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdwS0tLS3ufAzt1HSt76VqlZX/99LXsqSmpsrulc5PdzMzH30f2dC6e4+G1R7LnK5lX9ptnbsmeJXcW2aNeRsoemCWD7K7ev39Gf9nNzCKeR8geFx0nu6v3GB8V73INim8GX9n9Av1kf3jtoezp0rn+P4CcBXPI7uXrJXvozVDZ/Vx8Tj9MGiQ7ADjV9KVrZV86fbrss9cvl33TvC2y37pxVnYzs36Tv5F9409rZF+46nvZV63aJbvL63iUvo4XLF9Q9swhmWUvXUI/fkS34bKbmY37+TvZwx4+k93Lx1P2ogXyyL5ywSbZm/VsIvu5vy/KXueTqrLfuftYdjOzQgVzy37j2n3Zn9x5Ivub529k/3XReNkBwKkWbNwh+74/9sielJQoe1iYnil0GjRAdjOz2WO+ln3J7u2y/7PhH9ld7QWiXkXJfu2svk7mLqD3EvEu9jJTfhome/1KtWQ3M2vZro/sJ//eK/vRU7tlL1agpOweHull/2mr3k+WzaP3Ot1a95f9o9quz1H+Mvllv3bimuwhhXLKXqvRx7LXL6nPoRl3eAIAAAAAAABwEAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMfweN8D/TL4yf7y8UvZU1NSZU/vlV52T29P2c3M3NzcZA+7FSZ7QKYA2ZMSk2QvVaOU7LfP3pI9MDhQv36Cfv3UZH2On99/JruZWcZsQbJ7+Xr9T2vInj+b7NGvo2VPTkqWPS0tTfb8pfLLfvv8bdnNzB5cfSh7zkI5ZXdz1//P4OH53n+WAPBB2bdmu+wnLx6TfeHSjbI/e/JA9shIvdcxMzu375zsjTo1l73P54NkD318Xfb6TTvK7u7hLnvxysVkH9dlgOxVazaTvc/EIbKbmT0NfSF7hTJFZf9h1BLZb5TKJ/ul08dlb9qjsey5i+WWffawObr/Oll2M7MDh0/LfuHgRdmLVtbn8Na5my7XAAAfIp8A3//p8Tnz5pU9ISFW9ucPn7t8jU0nDstevXBh2Y+sPyL79FFfyD5j2Z+y+/j7yP7FF5/JvnnXEdlfx+pzWLRoZdnNzB7duif72KUzZS9dtLzs1aq1ln337p9lP+NivxlfI0H2Q0fWy/4gPFx2M7P1G/bJfu/6DdmvXdDvoXjV4i7X4Ap3eAIAAAAAAABwDAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdwS0tLS/u/vQgAAAAAAAAA+D+BOzwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4hsf7Hrju1L+y3zl/V/ZCFQrJ3qBkSdm7tB0qu5nZ/QdXZF+6faXsI7uPkH37/jX68UN/kL1h94ayJ8a9lb1RlfKy9+2o11+iUjnZzcwqN6sse7p0eka+8JvZskdFhct+5sJB2b8ZN1/204cPyX705HbZJ8/6TXYzsxtnrsoeUjCP7P2/7ij74jn6e7Z0/ljZAcCp9v/3n+xFc+SQ/ctu42T/YdlE2RPfvZPdzOzbr+bI/udGfZ38ef0u2Xu2aSJ7yybdZa/SsK7s5/4+LvvEJRNkr1m0mOyr/v5HdjOzSsUKy54nOFh2Py8v2T9rNUT2xX9Mkb1Vo06ye3v7yV6gqN7zPg8Nld3M7NGja7JPXP6j7MVzhcj+KiZG9kalS8sOAE41Z+Um2X8ar/cSpy6fkH3n8TOyH9t8THYzsy7D2ske4O2tH9+4reyVKjWSfZmL6+j2U/o9fl69quyHrutrYHRcguwrv3P9m/+Lqf1lXzPjT9nL1Ssn+7l9+hy0H6VnBq7eQ/nalWR//fSN7MsW6j2zmVno69eyp6Smyt697SDZ+88cKnvHqvp7YsYdngAAAAAAAAAchIEnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHcEtLS0t7nwMPXbsm+4wh38t+8OBK2TeePi373CHTZDcz6zFuoMtjFP8gf9nvXrwr+7n9+j2Uq1NR9tGDOsu+YtdB2SuWLSb7twOmy25m5uXlK3v11jVlH9WrvezNmvWX/dmz+7K37d9D9jIfFZf93LHLsn/7lX5+M7Ny5erJfuHiIdmX7zkge/WyJWUvERIiOwA4VYsWA2QvXrmU7O/eJslevWU12Y9tOyG7mVnM62jZz578W/bg4Jyy7z+4Sva1/xyX/fMaVWRvVK+T7LnzFpH908Gfyr7vt32ym5l9NqCl7KM6D5K9dmP9+EVzR8menJIi+8TZv8resfMnsl+4fU/2ecOnyG5mVre5fo+3L96U/dc1M2Xv3Umfoz17lsoOAE4VERcn+/c/6bmHb6Cf7D07NpP90uNHspuZlc6VW/YMvvo3f9uW+jf7o4dX9eN7fyF7i7b1ZT934Ybsg9vra+C9Fy9k/7yJ3uuYmc3440fZ798Lk/0jF7OZYycuyv7T+ImyV67cVPa6nfTMIiEmXvaaNcrLbmZ2+vx12ed8M0b2Yd/r+VSlCnq2U72I3pOacYcnAAAAAAAAAAdh4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDLe0tLS09zmwfcdRsucslFP2wOBA2SOeR8hepFJh2c3MPq1bTfZVm/bKvnTGDNkLFiwv+/gFY2SPe/tW9nnf/Ch7024tZY9+EyP7o2uPZDczu3X1kuyt+nbUa3gVLfvTu09lj3mjH//69XPZ79+/JHvevCVlz54zn+xmZhkyZ5A9IFOA7CFFQly+hjK8y2f/0+MB4P+rXkZHyf4qJlb2b4f/JHtCnH78pYuHZDczW/rXBtljExNlv3rymuwVa5eTfcV3q2TPVSSX7K62hWeOHJa9VZ/Osi//fpbsZmYDp0yU/dm9Z7JfOnZG9qdP7sreoksX2T0808vu5eMl++et68s+uLveT5qZ3blzXvZRP82WfeWMJbJv+GuF7LmDg2UHAKdyNRcZ+/1A2bMG6rlI93Zfy/5Jz09lNzPLmDWj7BcOXJC936B2srep30b2f84ckH3wF1Nlnzx3qOwdP9HX6dpNW8g+ZJh+vJnZb79vl33fhs2yBwTo6+RnAzvIHphFzxyqFi0i+7a9x2TPVzyP7PVL6rmJmVmrT3rJXqFmVdmnTxwge2R8vOw5g4JkN+MOTwAAAAAAAAAOwsATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BhuaWlpae9z4NWwUNmPX7gm+2d1qskenZgo+7QJS2Q3Mxv17ReyL1u6WfZLx87IXrpqRdljI2Jl/7j5x7L7+PvI/vLxS9mTk5Jl3/nbRtnNzJat/0n2Wd/9Kvvfe/RrfLdysexn95yV/eG1B7JHRepz9Dj0huyTf18ou5nZ9ZPXZb928qrsr16Fyd5tdD/Z+7dsIjsAONWa4ydkr1yooOyDe4yXve3QDrK3qq6v42Zm6/b/I3v062jZXzx8IfvTO09kb/5lc9lnDdbnYMrvP8r+8ukr2V05tOqgy2MqNKwge9gtfR2t8mlV2WcNHCf70VN7ZN964rTsLx7pz/D+5fuyV2lRRXYzs1+/1fu1lj31d3lU346yr/xbf497NqgrOwA41dt372SPTkiQvXcXfQ26efOU7GMWz5LdzCwpIUn2/b/vl71UzVKyp3PX980d3rxX9m27l8v+3Szda7bQsyVPd3fZ+zVvL7uZWcVK+jd3xixBsse8iZH9wYP/ZB+5eLLsbSp9JPuzyEjZm9f9TPajp3bLbmbm5eEhe6+ek2TPlDOT7Jf/1X8LJ05skd2MOzwBAAAAAAAAOAgDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjeLzvgQM6DZe9/dDustev0VL2qKhw2Y9fOCq7mdne0+dlP3f4hOzp0rnLXqZ2Gdnz5cwm+5yRC2QPDAqS/cHd67K3HdhV9n7Th8huZrZl1xHZT/2zT/bL10/LXrVSQ9k/ad9R9gKlC8h+9XSU7H1GjZT9z5mrZTcz6zulr+xpaWmyTx89X/ZZy9e7XAMAfIgmdBsg+/AfpsvesFMT2VfNWKYXMEZnM7MFY6fIfvbSP7KPGPWj7K2GtNavP2Km7Is36ff4+48bZB86vqfsj1+/kv3+vSuym5kVqlBI9kunTsm+f4e+jp69fEz2K48fy750ov6MNu3+Q/YJZ/V+cN0PK2U3Mzt8dLPsR27ckH3Ud4tlP7Rtq+w9L9aVHQCcavHGXbIHhwTL7u6uRzBXr/8r+6yl62Q3M/u8dX3Z963Qv+mr168k+3+Xb8t+966ey0yc8rPsHXq3kH3/Hj3XWT1f/97+7/ZF2c3MDvz3n+xNypaVfeSYebJny99A9m97DJW99X/HZf+y2zjZD5/U3+N5S13PJLLkziJ7TFSk7M+fPpK997eDXK7BFe7wBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjuKWlpaX9n3ii+LdvZd958aLsl45cln31onku11C4cEXZvb39ZC9euZTsLx48l/1tvD4H9+5dkb3PxKGyVytfUvY/ft0m+/4tG2U3M9t3bIfs3unTy96h9UDZ79w5L7unp4/s3b8eKvv4AV1kn/Tjb7L7ZfSX3cxsSJfWsoe+eSN7Bh/9HhvX/kz2CxcPyA4ATlWoUAXZOw8aLPvGpctkv3j5H9mLFi4vu5nZ2SsnZY9zsV/a+NcR2R/feCx7wXIFZT+++ZjsRSsXk33n6jWy+/jo6+iYxZNlNzOLjU+Q/fjm47Kn89D/n1+hgf4e1ShTQvbYxETZC2XPLvu3036R/euvu8puZvb7n3/JfufcbdkDgwNl7zpA70WqFiokOwA4VckS1WXf6+L3dMsG7WQvUqyS7HU71pXdzCy9l/7N7uXrJXtCrL4On91zVvYc+fV1cNSAzrI/i4qS/YmL39ujeo6WffLS72Q3M5vcf5LsI34aK/uYLnouUrZ8HdkzZs0o+2d9mss+bdBU2eu0aiz7o+t6v2lmVqZOGdkrVigu+6XLeq+ye4X+W9q3b7nsZtzhCQAAAAAAAMBBGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHMMtLS0t7X0O7NZ7ouw9h3eQ/WV0tOzZM2SQffMfe2Q3M/tnz07ZD5/Q/XlUlOxnb92R/acR02X39Q2QfejcUbI/e/Rcdv8g/fwpySmym5llzxYs+6KxS2Rv3O0T2YND9PPHvImRvc5HZWW/8fSp7G/CI2Q/ue1f2c3M/DL6yd66e1PZjx06K7uHh4fs4wd0kR0AnOqmi3/joxLiZZ84YIbsn375mex+gb6ym5kdWHlQ9qmzh8h+JTRU9tMHz8seUiRE9oMu1heQSe8lsuXNJvub529k9/H3kd3M7Mb5y7I/enRd9qdP9X6tQ7fhsl+/eE72slWqyV6mThnZB7TSe6V69Vxf58tUrSi7t4vzPOqrrrKP/3ax7ItmfyM7ADhV7lxFZU8zPV7JmbOw7N8s0HOX8d0Gy25mNn31ItkLZdPX8hMXrsq+9zc9m/EL8Jf9adh92Yf/qOci80bMld0VD4/0Lo95+FCfg2LFqsielJQge0yM3i9FRryQ3d3Fe5j82w+yj+r0peyFClWQ3cwsKLOe7Uyb87XsM6ctl71xlwayt65USXYz7vAEAAAAAAAA4CAMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACO4fG+Bx45sEn2mm1ryt65Xi3Zh4/+UfZRY3vLbmbmnt5d9o5tvpI9e57cst/877zsY5Z8J3urSpVk//KrGbI37NpA9lkDJ8meI0cB2c3MilYqIbuXl4/sf877VfbTp3fJXrVKS9nLrZoj+9qZ62SftmC47NmyZ5bdzCzIz0/2l9HRsmfInEH2Qe31OQCAD9XKVfoa4ubmJntgxkyyR76MlH31rGWym5l9M3+c7NMmL5U9LipO9ieP78nesVhP2R8++E92jzBP2T29q8nef3gn2TdvOCC7mVm3cXrPt23BdtlDQ2/IXuzj4rKfPfG37HlK5JF9xqCvZb9/+b7s67Ytkt3MrGfHUbJXb6n35VNm6O9yhz7sRQDg/80nn/WQPfldsux1OtSRvWSuXLL/uW+D7GZmj169kv1VTIzsX7X9VPaTt27K3q/tF7J3GKx7Bh89c2jaTa9v+cwfZF+/f6PsZmbPIiNlnz9aX6uXrJou+8+/6PnaswfPZX/55Kns5fLqvUqH/v1lj43U+1Ezs9dPX8s+a8Zvss+YNlg/fsFq2Vu7mK+ZcYcnAAAAAAAAAAdh4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDI/3PTAkpIjsv09dIruXr5fs96/fkf328+eym5k9vv5Y9kxZssnuF+gre0pKsuzBAf6yt/50kOwFy+hzHOTnJ/uEpdNl716viexmZh1Hb5P98Q19jkcunCR7j77fyh6cOUT2a3cfyp6UlCD7gRPnZL/x7w3Zzcxqfl5L9gJZs8j+/bxvZX9w5YHs82cOkx0AnCpDlgyy16xZQfZJw3rKXrxoJdnT0tJkNzP7psMXsi/fuUb24/+clz0+Jk72nb9sk71Ctdqy12hTQ/ZTO0/Jfun2fdmXzJwgu5lZ6dJ6jX0mD5Q9U45MsofdDpN99V+rZHd3c5Pd22+G7OXKF5U9JvGt7GZmzb/4VPYtC9fJPue3qbI/j4x0uQYA+BCFFNa/V13NFBKi42WfO32F7GmpqbKbmX3at7nsK2b/KfvCLTtlH9tvsuwJCbGyn951WvZCpQvIfvDPvbL/tnO17OXy5pXdzKxhwx6yr9uyQPaPy9eRvVaDlrI/Dw2VPemtnntMGafnc1VbVpG9bZ3qspuZjR2/UPZRY3vLXr7kx7IHBen53YzR/WQ34w5PAAAAAAAAAA7CwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGG5paWlp73Pgl1/PlH3IN91kH9Znsuw//ab7pUePZDcz+3vdYdlj3sTIfubf/bJPWTlf9iGfdpR9w5E9sl+6fFv2N89ey354617Zh8z5RnYzs3AXr5ElR7Dsu3/T7zFfqfyyF6pQSPZ1M/+UPSAoUPZ2X7WRfe6w2bKbmbUd3Fn27s0ayF6n+qeyz1urv2c1ixaVHQCcKik5Wfblfx2QPS4yVvbw0HDZazarKruZ2dJJv8leoUFF2Yf2aSf77WfPZM8SGCB7z7Zfye7qGvf4Rqjsu9fr63StJi1kNzMLDNbX8ltnbsn+1bS+sv/x4wbZr106I3vZytVk9w3wlf2rr/U5Xrlml+xmZgXLFZT99G79Hm5d/E/28rU+lv2HSYNkBwCnyp27uOwjZuu5ycOrD2R/l6T3Ol8Mbi+7mdmkr+bK/uPSsbJPGb9Y9uptasgeG6H3W0mJSbKXLFdY9rXzNsl+5KC+zk9evkR2M7NyRQrI/tdfx2Q/seOI7NVb1pHdL1DvJXIVySV78/LlZE9Ieif7ztPnZDcz61K3luxNG/eUvdTH5WW/ePyU7P/8s152M+7wBAAAAAAAAOAgDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjuHxvgfW61BX9kz+frL/9Ntk2aeOXyL7kQObZDczu3rrvOzfzf5N9m4fD5H9xr83ZF93eLfsO9YflL1IxSKy37t8X/awsFuyv3r+RnYzs1XfL5X9y5nDZN+3c7XsmU7klH3w9+Nkj4x8IXu5uuVlf/YkXPZc+QrKbmbWpMZHss+cv0r2YT9NlH10rzGynzixRXYAcKpefb6VPThnsOwJsQmyT5jUT/bdJ8/JbmaWlpYie+Va5WSPiIuVfcuGA7If2f6X7JGRL2X/e+1h2W/dOiN7l6EDZa9SvazsZmaLJi6X/dSpHbKHf/FY9j0H18g+fd4fsg/o21b27p/r/WS/ThdlL17R9Tkqkl5v4d8lvpN91LyRsi+butLlGgDgQzTs++myV65UUvbLhy/JnpamXz93pkz6ADNrObCl7B1b9Jb9ozp1ZM+WVa/h1hk9l9jy+zLZy1dqIHuxysVk7ztqu+y3H4bJbmb2VZfhsn/ap4PsXl6+sj+791T2qq2qyT7v65myPxnUTfZ7l+7J/irslexmZkvG/iB7pVq1ZU9N0V/2Nl92crkGV7jDEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOIbH+x6YN3Nm2feevyR7k4rlZA9/+lz2/PlKy25mtuCPzbL7B/nL3rRpddm/H7NE9nyl88seGxEre+2K+j1OG/i17LXqtZH9xLYTspuZ7dq/SvZvRv4oe/0mHWUv+lER2c/uOSv7s2f3Zc9dLLfs/24/KXt8dLzsZmajBsySvdbntWR/evep7If/2eByDQDwIQp/qv/99PRKL3v062jZW9T/XHZf3wyym5nlzV9M9ts3Hsq+ePwvshcso6+j4eGhsqekJMvuG+Aj+9DZE2X3CfCV/ezpq7KbmYWHh8neqFkX2e/d1K9RtsTHsufPX0b21YvmyX7x+hnZu3f4Rvaje3fJbmZ2fP9u2es0ayH7sqkrZf+oyUcu1wAAH6IsubPI3q6u/vf3q++myO7j7y37DRd7ITMzNzc32UuUryT72X+Oyn7539Oyj180QfZZEwbIfue5ng09j4qS/ey5a7J3+KSu7GZm/9WsIfvpXfocTPxpmOzennrPOnXkItlHLdTnOE9wsOy/zdBznV+2LJfdzGxEtxGyfzVM79eG9Zkse5e+n7pcgyvc4QkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDAaeAAAAAAAAABzDLS0tLe19Dhw2cb7sc6d8JXtMYqLsV8PCZL/z5KnsZmaFQ3LKvvqXrbJHPI+QvcWAFrJfPnJZ9v0bt8j+29Zlsnf+pLPsmTLlkD17znyym5kd+Xud7C0//0L2+p3ry/7D4G9lv3jxgOylS9eW/dTZfbJHJSTI3rJhJ9nNzMYumSL7sR0nZf9h8mDZXf1Juqfj/ykAfJjepaTI/vWYH2W/fu6S7BMWjpX944IFZTczq1OzjexxcVGyr9m9RvZZ43+R3dvXS/afF42TfcWeQ7LfOntL9htnr8peskoZ2c3MYiNiZX/95JXsNT+vJXu35g1k33NZ7+fGdOwr+5Lt+jP0Tp9e9m2r9V7GzOzmeX2e4+IiZe8+rr/sh9celn3tqumyA4BT3Xyq5xJDe02UPTZWzxy+nDlM9isuZg5mZh81qiT7iHa9ZN9waLvsfx8+K/vmX1bKHhn5UvZWXXvq19++Q/ZS5avIHpApQHYzs9zFcst+/eR12a9dOi17s87tZN/062+yu5r9/PjH97KHvnkje8sKFWQ3M2vcqIfs/acPkv31C/23kDVHsOytK+nvuRl3eAIAAAAAAABwEAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMfweN8DPX08ZY+Mj5f9t617Zd/2yzrZP6pXU3Yzs1UnlsveblgX2U9sOSH7+C5fyn795lnZI19Eyp4tQwbZPT29ZG/S+VPZNyzR58fMLOltguw9hrST/adxS2V/8uS27EPHz5M9Z8Gcsk9fsEr247sOyb5h9++ym5n1bj9M9gMHV8petMhHsi/eqtfQsFQp2QHAqT6q2FD2P3b8LvvIvjdk3/LHHtkvlM4nu5lZXFyU7KXL1pD94JEzsmfMovcKOzf+LnvZ0no/9i4pUfaTF4/IfuyWvs6/CY+Q3czs1K7Tso+f85Xs7Rq2lT30Zqjs347qLXvI3k2yz/5mvuw7dy6WfbvbPtnNzCIinsuePVt+2ZdO/FH2Q0c3u1wDAHyICmfPLvvNm6dk7zVipOyVChaQffGoObKbmflm8JM9V66isvds2V327Nn1NWbI3LGy+/l4y75uzkbZG7f7TPbiVYvLfu34VdnNzBZ9O1n2dOn0vYMd+g2SPVOOYNkXbPxF9i9b95S9REiI7NGJeu5z4KrrcxQT80b2wxv/kf3G+cuyB2fJIXvr9ZVkN+MOTwAAAAAAAAAOwsATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4Bge73tgx86f/E8vtGPZRtkr168le5ceLVy+RvDgTrKPG/mT7JlDMsseEJBJ9sIFy8r+4+Y/ZHfF3T297FlyZZG967ABLl/jr9+3yX786AXZG3ZvJPu2bfNlD8wUIHvX1vr5q5XX36M6DdvI/ufWg7Kbme07oD/HjWdOy3756gnZh436UfaG80rJDgBO1XP0ENkPHNb//n7cuJrs+9Zvlf3U36mym5nVa95a9gnj+sg+dswC2V88fip7amqy7AOnjpP93sW7ss/6cZXsD648kP3Ro2uym5ldunRI9msXzsi+Zs862Tet3y97vVptZS9cvJzsGYL1frFO7Q6yV21aV3Yzs8PHNstetqT+rgcEBMl+NSxM9mqFC8sOAE61cMNO2UfMmSn79X+vy37l8WPZQ3IXkt3MrHsHPbuJeP5G9mYdG8p+cNtR2d/Gv5X91wmLZA8LvSl7zzH69/iwDv1lP3nO9W/+uKh42TNl19fRd0l6P3Z0oz6HP47W+zV/f/36a/7Wz1+zbEnZP2/cUXYzs7i4SNnDbofK3riLnvF92qSmyzW4wh2eAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMt7S0tLT3ObB7n0myx0XGyl68agnZxwzqIvunzb+Q3czswYP/ZG/SppPsobcey957Ug/Zy+TOLfuqDXtlz1sij+wvH7+UfcOi32Wf8usM2c3MTh25IPuSad/KPmruXNmXT9f9n1N7ZN9z8ZLs9y7dk/3+5fuyR72KlN3MzC/QX/bUlFTZHz24KXtgQCbZ9+3/TXYAcKp2HUbK3rBbA9lDb4bK3qF9E9mPntf7DDOzK0cuy759/a+yBwVll71+yzayTxrbV/bQN29knz/zD9lPHdF7mTcRz/Xzb1sru5lZSpq+jl4+ckX2V2GvZL9z9arshUuVkt0/o5/s1VtWk718vnyyt6zTWnYzs5whhWTPW6Sw7L4BvrK7pXOT/afpX8sOAE616ugx2ad9MUL2d+/eyl6mTF3ZL106JLuZ2fajf8m+a49+D2vnL5H94uUjsnfqNFr2gIz693Sb/i1lXz17vez37+r92vvMRXo3/Uz2iIgXsm85dVz2LAEBsp+6eF32Ie1ayb58/wHZ8+XIJvuiyStkNzN79fKp7PmLFZW9Sosqsu9brvecmzbNkd2MOzwBAAAAAAAAOAgDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjeLzvgXdv/Cf7J13byN7t86ayRycmyp45a07Zzcw6jeoh+6LR38te79OWsv935rrsV8/dlH3XyvWyZ8iQRfbXr5/KHhHxXPZrV+7IbmZ2cMNO2cuVqy/7s3vPZPf09JJ93b4jsq/+YansX04fLvuuletkH7PkO9nNzLYt3iH7qGn9Zc+bObPsB69edbkGAPgQZcubTfbv+g+VvUyZ2rJ/2esz2WMjYmQ3M/P08ZS9Xa9Bss/6VvctZ8/KntHPT/Ylq7fJHhDkL3vtJnqv9HHzj2WfN0zvxczMpi6bKntCTLzsMW/051T640qyn/r7kOwDZ46U/fj2E7IfjPtb9ly5ispuZlathf4uTxjUTfbEpCTZ0yzN5RoA4EN0dMNR2UuVqiF72brlZN/x+5+y581bUnYzsy1b9XUsMTZB9kpV9W/+ERPmy3775jnZB38/TvZBrbvK3m+cfny3kR1kX/7dStnNzMLDQ2UvU6aO7Bf+1fOzOWO/kX367ytk7953guw5s+qZw7mz12R/+Vy/fzOzaUsny96+XnPZv5um97y7l+52uQZXuMMTAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BgMPAEAAAAAAAA4hsf7HpgpU3bZk9+lyL52+0HZV8yaK/vMNYtlNzMb0a6P7G379JM9/PFL2b8b3Vf2LwdNl/3duyTZP+n5qex92zSR/ZdNu2V/G/9WdjOzlr3byx4bESN71jxZZS/0X3nZ9/+xT/YuI76QPUOGANlfvQqTPTlFf4/NzF4+fSp7q9rNZb9w9V/Zf524VPa2eyvLDgBO1XtAW9mf3dP/Pucullv28Bh9jWtav6rsZmYDV3wje6t++jo7ctIC2XMUzCl7w3GLZH/7Nl52V4bPGyP71iU7ZI+KCnf5Gr/PXS/781B9LR80c6DsIUGZZK/Ttpbsl49dkT0xTu+3Ro3rJfun9dvJbmaWIXNL2Zfv1fvu8kULyj62/3ey79+/QnYAcKq+X3eQ/dDfZ2QPyKR/ryYlJcg+aek02c3MsmfIIHubOvr3apEilWTvPWKc7Ls3rpE9NVn/5l6yXT/+5MFzsndr0Ez2S7f/k93MrOuYTrJfO39L9qaNqsnepoV+D+2bdpZ96ZZlsndvph+/59hO2TctWS27mdnv8zfKfuzicdlT09JkL1KxsMs1uMIdngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDE83vfAO3fOy75241zZX0ZHy/7s3lPZF4ycJ7uZWZYsuWX3DfCVPSBTgOzPo6JkL1ypiOyvn7+WfeWsRbInxCbIHnorVHYffx/Zzcy2r14h+8RlP8q+ZMwc2Ws1byL75hXLZPdc5yV73lL5ZK9YsbHsD64+lN3MLC4uUvaPq34i+68bd7t8DQDA/9OXHYbIXreV/vd36si+svfpP0X2r8b2kN3MbMKi8bK/ffdO9v2r9sheskYp2XPmySd7vc71ZHe1H5vUa4Ts5T6qJfuijUtlNzP7a9Nh2Ss2qij7vZuPZd96ZKfsbfq3lP3S4Yuye/vp/Wat8jVlr1Jdv76Z2b6Vei+RlKT3jIHTBspeqFQxl2sAgA/R74s2yf7w5n3Zvb39ZO889EvZ/zt3U3Yzsxvp9Zin21fDZG/Wqo7s8yctl93Ve3Rz1/fdtalSQ/bRsxbK3qhpd9n9vb1lNzMLcHHM3Qt3ZJ9/7rbsr56Gy16vZSvZM/n5y54unbvsDao0kv37P13v1xIS38reqUUf2XuO13uROi2qu1yDK9zhCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHMPjfQ9s36+f7A1qt5X947r1ZI8Mj5T92bN7spuZzVqzSPZtv/4le8nqJWVPTk2V/cimg7J3+KaL7EXyDJI9a2Cg7OF1YmT3SOd6vr16yRzZ538zQ/Yhc8fJvmvJLv38m1fIPrrbUNkLliso+3fzh8nernEH2c3MVv+1WvZ1a3bLXrh0Adk7tPje5RoA4EN0/MQW2c8/eCD76Xt6LzFkXE/ZJwycJbuZ2f0Hl2XfeXiz7JmyZpE9vafeuvll8JO9QEh22X39fWRvP6CP7K+fvpH90ctw2c3M5k/7Rvavxs2W/YdJg2X/984t2dvUair78Fn6On1iywnZR837QfamtT6W3cysbwe9n3F3d5e9ZYUKsv8ydqHLNQDAh8jXxXX2yZPb+vE+AbJXrabnLoWz6+u4mdn+C5dkv3Dgguz/5sgk+09LRsvuZm6yjximZw5d+46RPTEuUfaH927Ifvj6ddnNzHYu07Ojqq2qyf7LBP0eE+L17KZR98ayT5vws+wjF0yXfefinbI3LVtGdjOz6h9/InuePHq+tmb2b7K3/7q7XkDZsrobd3gCAAAAAAAAcBAGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHcEtLS0t7nwOjExJknzZnhewhRXLJ/vuMBbLnyl1MdjOzMrXKyl6kYmHZH98IlX3/uh2yP7h/WfbAwMyylypbQ/ZB3/aS/dC+f2W/evQ/2c3MHj26IfusP36QfXTPkbLHxkbKPuKn72Q/u/es7A071JP9VXiE7P+sOyK7mVmW3FlkL1WztOx1yus+Yfg82deumi47ADhVqosty2dthsn+/Nk92Yf9NE72W+fvyG5mls7dTfb0Xp6yt2ys9wJPI/R1LFuGDLLPn71K9tA7D2XPnjdEdh9/H9kzZNbrMzOr37ya7G/fJcs+d8Rc2S9fPiL74ElTZW/ZorbsI/tOlj06+rXsISEFZTcz6zGui+y5MmWSvWQuvS8vkLeE7KFht2QHAKeasexP2Y9uOSz71avHZL92+4LslcrWkt3MbNOhLbLnz6J/z5YrWUX2qjWayR6YRV/r1yybLfus1atl/7SWXt/IYXNkL+3i97qZ2b1Les+4e8tK2fPmLSn7kj/1XmXqyEWyJ8TFy56UlCj77dtnZG/SprPsZmZpKamye/l6ye4b6Cf731t2yv7vKT2fM+MOTwAAAAAAAAAOwsATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4Bge73vgX5cuyZ41T1bZV83+WfbZq+fJ/vOU32U3MwvKHiT70U3HZHf1Htzd08v+y671sh/be1p2bz9v2Qd+3k/2gIBMstds3kh2M7Os+bLJPnvET7JHRr6UvVGr9rJ3qFVd9qvHr8q+ctoa2deu/17207v0Z2RmVqV5Fdkn9xome+Etv8n+6P5Nl2sAgA/Rbhd7kULlC8k+a8ko2Yf1nSL7Z0PbyW5mdmyz3msUq1xMdi8PvTX7cfQS2RMTY2VPSkqUfdLiCbJ3a/yZ7A2adpC9UYd6spuZrZi9TvYzJ/fJ3n/SGNnPDtgj+6ndJ2S/fvK67N8vnSj7g/Bw2e/ffCS7mdnq2XrPOWBiD9nr1dGf0/DZer8EAB+qUX30v5/Z82eXPWlGguzbTp2RfdzPs2U3M/N0d5d967/6N+93KxfLPrJ9D9mrVv9U9nz5Ssm+/sdVsm+Yt1r227f0Ofx8UCvZzczSe+nZT8Fyk2XftOQP2b/7ZuH/9Pq//jxV9gF9v5O9dbc+smfKoWdLZmZNGlWT/Y+lW2U/d+iU7P2n67nK++AOTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjMPAEAAAAAAAA4BhuaWlpae9zYNWqrWSf+PPk/2khg9p0l733yG9cPod/UIDsu37dKvv836fJvn7zAdmjX0fLfvn4WdkLly4he9Y8WWXft16/vzotP5HdzCwqPEr2EtX0GjPnyiJ7jSJFZJ/74yrZ03m4yx79Sn8Gh/dskn3orO9kNzNrULWC7Cf+uyF71qxB+vF7z8g+c0x/2QHAqUZMXij7+SP/yj7w+yGyR0fF6u7iGmNmlvIuWfYze/Re4MQxfS0f9eMPspcpVVj2OiX0dbxw4UqyFymi+8gf9Dn+e/8p2c3MilUuKvu9S/dk98+o94Pr5i2XvXDJcrK/CHsi+7t3b2XvPeVL2dt+XFl2M7Nbz5/Lvv9vfZ4zZMkg+6qZv8p+9OgG2QHAqR6Gh8vesUVv2b+cMVz2+d/MkH3z3tWym5kdva5/j47ppNeYLp2H7DfuXJD9o/L1ZP/6h29l9/T2lL1NtSqyz5yvZwpD+7eX3czsi+6TZF+wfILsnzfrJXunYbpvWrhGP36EfnztiqVl/2n2StmfPXgmu5lZpmyZZF+2RJ+jFs31fmjUnKGy1y5eXHYz7vAEAAAAAAAA4CAMPAEAAAAAAAA4BgNPAAAAAAAAAI7BwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACO4ZaWlpb2Pgd27DxGdm8/b9kbdGsge4dqVWWvV7ej7GZm3cZ+IXvPRvVkX7H/b9kzBAXIvnLqKtnbj+wge6Ec2WS/cuu+7CnJqbJP6N1XdjOz2ev0eyhdIK/sG9bskb1c/fKyVy5YUPacGTPKfuvZM9mL5sghe88eE2U3MwspEiL76yevZXf1J1ejTXXZ+37SSHYAcKpf9x6U/cL+87KfOXpY9ppNmsh+59It2c3MUlJSZM9dOJ/sVVvq/dCDK3ovkBj3VvYDWzbL3nXEQNmz5dV7lUOr9Gd0/9ZN2c3MPv2inez1alSU3d9b70l/mPGb7D0GtZX98St9nX9yX+9Fjm06Jnvt9rVlNzP7cdR42a/ePCP7pJm/yn764D+yHzu2UXYAcKp7L1/KfuPJE9kLZM0qe97MwbKXLaH3CWZmq/bpf6O/bKvnJrsP671Cqovfs9ddnIOLZ6/LXrlKadkfPn0h++ldp2U/f+Ko7GZmq7br62Svtnq/9OLFA9lv37kg+90X+j1+2fkb2e/cPiv7R5Wbyd5yYEvZzczmDJ0k+76j22TP6Osre7WPm8p+6ZKe35lxhycAAAAAAAAAB2HgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHMPjfQ/Mli+r7AXLFZS9cI7ssleq2FD2UuWqyW5mViRfLtl/XLVZ9mz59RqrFCok+56QLLKvnLJc9nodGsseFxkre5kapWU/dOGk7GZmfdt+Kfv6XStk37V2jewzxunnHz9rmew3Tt2QvcWAFrIP7TZK9tELxspuZhYeGSX78omLZd+973fZe/WcJHvfTxrJDgBOFfMmRvaan9eSvUDZArLfuXBX9qdP78luZnb8352yz/rlT9nz5tT7rf+O/id7265NZX9yJ0z2dk3ryL5m2wHZr148I3u3kf1lNzPLlT+n7KtW7JD9r3V6L/Lzlt9kb1+/lewHTh2UPSRTJtnP7z8v+1ftWspuZpYph36NPr0my169td5XFyrf2+UaAOBD9M/lq7Jv+nG97PfuXZR997Fdsu84qruZ2cQhc2U/e15fy3v1+Vb2gCB/2Ss3+1j2tNQ02Yd3HiJ75+F6L7Fgzjey33rWWXYzs/AYvefMHBwi+4+rZsv+3cKVspeqUkL2V6/0fu6z7voc1W9bW3Yvj/Sym5nlzKFngG5ubrIv37FP9jW79Dl6H9zhCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHMPjfQ88sf+A7BEvImWvVL6E7DUbN5O92xetZDczO3PlpuxFSheQ/eq5W7IfT+8ue1LiO9lfvHwoe+P6VWT39NAf14kr12VfseOk7GZmSUkJskcnJsq+Yf962fsOnCp7unR6Bt9iQAvZH1x5IHv1ZvVlv3v7kexmZmVKFZa9SqNasj+LipK95cCWLtcAAB8iV//GH1yzT/bJC0bKHhySWfZuAz+T3cwsPilJ9iG9P5f90atXsr98/FL2YjlyyO7qOpozKEj2//65InuTDm1kD8qeSXYzs9lfTZPdx8df9hadusq+fvlO2fee3C/7kF6TZP9j3WzZP/niE9mX7zsku5lZ9Oto2S9f/Ef2whULyf7NgM4u1wAAH6LPa1aTvUKRgrL/umij7r9tlb1Ry5qym5nV79pA9prVW8k+bflM2YP8/GQ/ePiM7J1bN5T96+56v/U6Nlb2T1sMkD05We/VzMwmLRwje9sR7XSvo/dbjx7r2U2XbuNkz5o1j+wvH4fLvnul3usc2LlBdjOzBZtXyH73xQvZ2zbUc5Mpk3+WfcmPo2U34w5PAAAAAAAAAA7CwBMAAAAAAACAYzDwBAAAAAAAAOAYDDwBAAAAAAAAOAYDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGG5paWlp73Pgr3sPyp4hKED24AB/2a9fvy97XHS87GZm9y/r5/juuwGyt2vZX/bY2AjZe43/Svb9v++TPV/JfLJXbFxJ9mxBGWUf0u4L2c3MChfWr/H72hmyL9vwl+zu7nrG/t+xq7JfOH1E9qOndsvu6uu+6cgJ2c3M+jRrJHupktVl7zRIfw/P7jsj+/bt82UHAKf6evw82Z/cfSp7cI5Msh/dv0v22k1bym5m1u+rDrIvXbBe9uqt9DVk05xNsucqkkv2e//dlr3dcL3+uxfvyf5Fj1ayh0XovZSZ2bgvvpV97oppspfMpc/BiAn6Ovry0UvZn4U9kP3J07uyL9+5WvZAHx/Zzcymj1oke9jjO7Jv3r3C5WsoOTJm/J8eDwD/XxUSUlj2gZMmy75kiu57Tum5S7tG7WU3M8uZs5Ds8fExsqdP7yl7UtJb2as3aSD7qK+7yf7DfH2dLFypiOxevl6yr5n+h+xmZr6+gbJ3HtNR9mI5cujHN9fn4NMeXWWv17iK7FtX75X9zXO9H/ML9JXdzPXnsHzaHNn3Hd8p+9WwMNmblCkjuxl3eAIAAAAAAABwEAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMfweN8DD606JHvpWqVkX7XrX9mHzPxS9or588tuZvak0RvZz91/IHu6dO6yf1y3vuyFCuaWffHdC7IPnvaF7KePX5L9Quh52d3c3GQ3M0tNTZa9ctmasqdZmuxdhwyV/Ztv+8i+aYv+HkQnJMg+ZeIS2fOWyCu7mdnNp09lzxCYWfYLB/X34O3beJdrAIAPUfK7FNk7fdNe9s0Lt8n+abdusqdzd/3/xIM6D5f93btE2UMK5ZT93Lm9svsGtpG9QKnCsu9dvk/2lBT9GcyLWSt7vpKur7MbtutrdVKy3qtExevraKmaes96IvyE7M+e35e9So2msu/coPfUk0fpvZCZ2ZcTe8i+cMJy2T9v3lv2o8c2uVwDAHyIQsNuyz5w2PeyT1y2UPZWdT6VfcPBLbKbmWXw9ZV95ZpdsucvU0D2Zh9VkP3rr2bJ3rhOW9lrNdXX0R+GjpP95s3Tsi8YMVN2M7OfVnwn+/y5q2UP7K7fQ7/J38hesEAu2WMS9X4yY7Yg2T29vWRv30Wv38zsemiY7INnTpD92wmLZS9ZvaTsTcqUkd2MOzwBAAAAAAAAOAgDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjuKWlpaX9314EAAAAAAAAAPyfwB2eAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHIOBJwAAAAAAAADHYOAJAAAAAAAAwDEYeAIAAAAAAABwDAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHMPjfQ/8ad122UNvhcru5uYmu6e3p+zZ82eT3cwsPDRc9pTkVNlj3sTInrdEHtmTk1Nk9/H3kT2du54/R76IdPH83rJHvNSPNzNLl05/Tr6BfrLHRcXJHpwzWHZX3yNX5zAgU4Ds8dHxsiclJsluZpY5V2bZXzx4IXs2F99lV5/zd8N7yw4ATvX1hJ9kb9OliexbVu2RvVXnxrLv3HBIdjOz0wf/kX39jqWyX3j4UPbzx6/Ifv/yfdlLVCsh++XDl2Wv2rKK7DdO35Q9W17X+7ldK9fLniNHQdkvXdKf0407F2Tv0mGM7M37N5e9Y50aslcsV0f2PmOGy25mZi72awvGT5K9ftN2stdsW0v2HvXryA4ATtV30DTZ46P17+E34a9kb973U9lrVCotu5nZ0dOXZG9YvZLs4TF6LtLjk/ayN27dSfafvtfXueHj9H4vb6m8sofdCpM9U/ZMspu5ngtEupitFCyn9yq/fz9f9s/69pTd1V7pj616v9mlZS/ZP6pZR3Yzs8Ejusrevon+HvSbNEL2BeO+k/3WrTOym3GHJwAAAAAAAAAHYeAJAAAAAAAAwDEYeAIAAAAAAABwDAaeAAAAAAAAAByDgScAAAAAAAAAx2DgCQAAAAAAAMAxPN73wLcJb2UPypZR9ndJyfrxWYNkf3LniexmZmmpabIHh2SW3SO9u+wvH7+UPXex3LI/uPpQdi8fL9mTEpNkf5fkJ3v0q2jZzcwKlC0g+6NrD2UPyp5J9oTYBNn9M/rL7kp8dPz/9PjMufR3xMzsVdgr2d3c9OOjw6Nkf/dWf84A8KGa9e0g2actWiV7Ylyi7LcfhOrXnzhQdjOzOXmzyb52x0HZu7RqJLuHu96rNGtWU/Z/z1/Vrz+8nez+3t6yR4Xrvca5/WdlNzOb9ccPsp8/e032tl+3lT3exXX2zp1zsi8arc/h8Y9qyJ49u95r3TxzS3Yzs3071sjevo/+rvbp20b2xQvWyd6jfh3ZAcCpXjx+LvvcZeNlb9e0q+wBwQGyP36tf4uame36dbvsf3y/WPaQkCKyt+neV/bjew7Inpr6tewXT/wre8P2dWTfv2q37P7+GWU3M4uNiZC9SlN9rffL4Ct7TMwb2V3NftoN7in76AHfy75+90rZG1drIruZWeFK+nvSuO3nsndr1kD2XPlzulyDK9zhCQAAAAAAAMAxGHgCAAAAAAAAcAwGngAAAAAAAAAcg4EnAAAAAAAAAMdg4AkAAAAAAADAMRh4AgAAAAAAAHAMBp4AAAAAAAAAHMPjfQ+MjYiRPeaN7l6+XnohubLInhiXKLuZmVu6/21+G5glg+wpKamyx8ckyJ7FxXv08feR/eHVB7K7e7i76K7Pz92Ld2UvULaA7OlcfAbP7j2VPVv+7C4e/0x230Bf2YNzBsv+5HaY7GZmuYvnkT0+Ol72qPAo2QMyBbhcAwB8iNYcOSb76yevZW/atZHsJUJCZO/Z91vZzcxuX7ss+5hFE2X/feMe2dfO/1n2n9Ytlj1X/pyy7157UPZDOzfL3vTzjrKP+mGw7GZmm9fsld3VdXTumDGyu2/4Q/aCBcvr1498KfuvP0+QvWSJqvr5o8JlNzPz8Egv+7ql82Wv3aKa7MWrFHe5BgD4ECUkRMve9dO+sufLV0r26f2+kd3NzfVv+vZf9pP9xYPnsrfr21L2x8/1dTDiRYTsjRp0kT0tTc9dHt3XM4VcBfLKXrhiEdnNzOrV/Uj2B+H6Wn1yx7+yj108R/Yzu8/I/ipMv35kxAvZP2/cWfYWHbvLbmZ26e9LsmfKHiT7kCGzZM9XOp/srSpWlN2MOzwBAAAAAAAAOAgDTwAAAAAAAACOwcATAAAAAAAAgGMw8AQAAAAAAADgGAw8AQAAAAAAADgGA08AAAAAAAAAjsHAEwAAAAAAAIBjeLzvgfExCbKnJKfKnj1/DtmTEpNkz1Ukl+xmZolxb2X3DfBx8fhE2YOyB8kedjNU9ix5ssoeFx0n+7ukZNn9An1ld/dwPd+Oi4qX/e75O/+/du37u4qyjdv+md4rKYQkEGroXUCqFKVbwUK9raCCFVRERUWRG1FARUUEFAXEShEVRBCRrtKlE0qA9N7r+w+86zs8637WetaaHJ9fj8ne154ZmWufbtkj6kfKHp0YLfulfy/KHhlXT3anz3j2n7Oyh0aFym5mlnUly/EYJTgiWHZPL/4/BAD8/2nXpJHsL616RPbfNn4n+5P/nSV7TbXe65iZ3frAPbJfTc2QPShMP8urq/VeYOuPu2Tf8MWXsr+6fL7sf+/8U/YTB47J3u3VKbKbmc3Ypt/j2YXPyX7qyGHZP5n5nuzDJt4pe0hkiOxDBt8v+6iHHpb9yI5DspuZTZ03Q/bFL/xX9k0rN8veoX9HxzUAQF1UUlIoe0Wlnkn4+vvK/vfhHbLPWfSF7GbOs5M7bh8g+89bdsv+9+a/ZH9k1n9kb1F/suw39xwq+1cLVsie3L6j7BmX9F7MzCwuPFz25x58Sfav1n8s+8Zd+2T/e5e+DwID9dzi641LZX9/ydey//CZPsdmZsPuGSN7fma+7AHB/rKPGNHXcQ1OmKwAAAAAAAAAcA0GngAAAAAAAABcg4EnAAAAAAAAANdg4AkAAAAAAADANRh4AgAAAAAAAHANBp4AAAAAAAAAXIOBJwAAAAAAAADX8L7eA7289Gw0qV2S7OkX02X3D/KXPTgiWHYzM29f/XFKCktlr66qlj0vPVd2X39f2TMvZ8oeFh0me724SNnPHTkve3R8lOxmZkW5RbLHNW0g+5UzV2QvztevH9MoVvaSghLZs1KzZPcP8pO9qrJKdjOzmIYxsnt5e8l+7tA52f0C9RoBoK46nZYme68+d8heXlome0VZueyDHxgsu5lZTU2N7OcO6mfA1bNXZf9t9ybZU7OzZd+7+Q/ZNyz5UfbaWr1Xatyqqewjhk+W3cwsqUVL2SOD9J7Qx0c/R79Z/5HsJ67qa7B9637ZX//oZdkTIiJk/yk5UXYzs30/7pN90uxnZV+/+Hu9hu+/kH3yrUNkBwC3ys/X3+lbteou+5lTB2Wf9Mhs2SdOv092M7NJt46VffdBvRfITcuRfeb8qbJ/s3aL7O/vPCz746+/JHvnTnqfEBYYKHuD8HDZzcxenb1E9jsf09fhtlv0NfjhF/2c7fLtx7Lv2K3vo3nvfC57cX6x7I+9PkN2M7OtX26VfeiDw2S/fOqy7HHXcZ2c8AtPAAAAAAAAAK7BwBMAAAAAAACAazDwBAAAAAAAAOAaDDwBAAAAAAAAuAYDTwAAAAAAAACuwcATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALiG9/UeGBIZKntuWq7spYUlsnt6OsxeI3Q2M6soq5Ddx89Hdt8AX9lLCktlT2rTSPbzR1Nkz8/Ik722tlb2gqwC2TMupMtuZnbq1F+yx8bqz1hQkC17WFiU7HkZjWV3usbh0WGyZ13R62vbu63sZmaVDmsoq66RPTwmXPbivCLHNQBAXRTi7y/7Y6/8R/ZuTZvq3uVm2YfefY/sZmb3jh8m+8yxD8v+1Jw3ZZ888RXZ09Mvyt40WT/n7p16p+zvz0iT3T84QPan502V3cxsy/c7ZP925U+yz3z/BdnX/rZT9rTz12Q/uf+U7Ls2/iF7RYXeT7bv2VV2M7NLp/We8oahN8ie3LW17D1v7eO4BgCoiz78dqnsceHhsu/+96Tscx59TvaGm/T3cTMzb2891+jZqa/srVv3lP3xn7fK/s2Py2Q/MqS77D2aNZP94IULss9+ZpHsLXu0kt3M7N05T8l+LDVV9l1N2sh+IStL9qrqatmvnLki+/5tei/11rI5sv97+oLsZmavLHxa9rS8PNkPbz8ke9+uA2Q/e+6g7Gb8whMAAAAAAACAizDwBAAAAAAAAOAaDDwBAAAAAAAAuAYDTwAAAAAAAACuwcATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALiGR21tbe31HPjUzAWyN2qTJHv6xXTZQ+uF/k/dzKwwp1D22poa2aurdM+6kiW7p6eH7PmZ+bIXF5TIfjHlX9mjoxNlT2jWUHYzs5BIfZ7b39Re9qxUfY6czqHTNbx88rLsYVFhsvsF+snudI3MzGIaxcjudK9GxkXK7ump/z/Ek/feJjsAuFViQrLsDz43Q3anZ0R+ln4GdOvbUXYzszYJCbJPnzJPdi8vL9mHPDhY9sgI/Qza9fM+2ctLK2S/9z/DZb+QqZ/zL46dLLuZWVJSO9lTUo7IfvPIe2VfskjfJ5ezs3XPyZF92btrZP9k8Uuyz/5gpexmZkW5RbIf23NI9nEz/yO709eD8X37yA4AbtW7912y//nnd7JPevIt2X38fGQfMe4W2c3M3p/xoezdh/aUvWtv/Z3/z837ZY9pqL8vb/xUn6OiolzZV3y/RPbln/4ge+N2jWU3M6uuqpbd6Tt9WHCg7L1b6D3twk/Wyr7t+59lHz5R36d7Nu6W/a5nRsluZrZnnX6NtAt6BtiiS3PZf1yzWvbjx/+U3YxfeAIAAAAAAABwEQaeAAAAAAAAAFyDgScAAAAAAAAA12DgCQAAAAAAAMA1GHgCAAAAAAAAcA0GngAAAAAAAABcg4EnAAAAAAAAANfwvu4DfX1kz7ycKXtCiwTZPTw8ZL969qrsZmZBYYGyV1VWy556OlX2gOAA2cNjI2T3C/KXPS89T/ZGrYfK7uXjJXu3QV1kNzPz99HXeWiHDrL/cfKk7Bfqp8teUV4pu6envk/SL2bIHuhwj/j4689vZhZRX1/nkMgQ2asrq3R3XAEA1E1PzXlT9rKSMtlv7ttV9lYN4mWf89Eq2c3MFqxbIPt/Zj0i+y/LfpH9pYmPyv76ig9l375+k+x+fvo5mZD8v+3nXnh/nuxmZmMH9pP9tbeXyZ5y7ILsvXrdKXtISKTs3Qf3lH32nKmy33P3dNlXf/Vf2c3MRt3xlOytu7WXPSosVPfgYMc1AEBd1LVfb9lX/blL9tsfGCb7uuU/yb5g+iLZzcwmvvKg7GvnfSX7sw/fLfsLE56S/YZeA2RPSTkie9u2fWXff/KM7J9/oJ+jd9yn91JmZvmZ+bLXb1xf9q3rv5O9c7f+su/asUH2v47o+2zx5z/I/sHyWbIvW7VRdjOzOW/o/c7yH/SeNqpBPdl/3xjuuAYn/MITAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALgGA08AAAAAAAAArsHAEwAAAAAAAIBrMPAEAAAAAAAA4BoMPAEAAAAAAAC4hkdtbW3t9Rw4a8Fy2SPqR8p+/vB52ROSE2T39PSQ3czs1IHTsvv4esteVVUte2T9CNlLi8pk9/TS82UfXx/Zy0vLZW/QrIHsDVs1lN3MrCi3UHZvhzVePXtF9sTkRNnPH02R/dq5a7IHhQXJHhIZInttTY3sZmbhsfo+SE9Jk93T20v2qAb1ZH92wijZAcCtthw9Kvs/+47L3rZLsuxfLfpO9r/3/Sa7mdnH3+v90pwn35a9WYdWundqJvuZvxz2Qv6+sjfv0lx2bx+9lwqtFyr7mw9Pl93MLCw8RnanrWtVVYXs3l76M8xfOV/2VR//IPut/xkqe2V1lewpZ1JlNzPLuJQh+x13DZS9QYTeywzvP1r2fw7+KjsAuNWgQRNkn/neC7J3a9pE9s826b1GeEy47GZmWalZsv+0fL3suXnpsqel6e/smZmXZe/ff4zspaVFsj/z7rOyr1u6SXan79tmZr2H95D96IGTskc6zMcCgv1lTz2t9wI9+3aWPS48XPY5L38s+/CHh8luZjZjzCTZGybqPe3GzZ85vocS4q/PoRm/8AQAAAAAAADgIgw8AQAAAAAAALgGA08AAAAAAAAArsHAEwAAAAAAAIBrMPAEAAAAAAAA4BoMPAEAAAAAAAC4BgNPAAAAAAAAAK7hfb0HFuUXyx4QEih7SGSI7DlpObLnZ+TJbmbm5e0le1B4sOwVZRWyZ1/VaywtLJHdy0ef7rTLqbLfMnao7J26tpL98KHTspuZhUWHyb53417ZS4tKZb9y+orsnQZ1kr26slr21FP6HJYU6GsUWi9UdjOzs3+fkd3pPqt1+AzZ1/R9BgB11boVP8k+8L4Bsq9+9xvZ6zeuL3vL/B6ym5kF+fnJPvqZMbJnXEyX/cDPB2TvPry77O/NnCW73zq9n+t64yDZy0vKZY9PaCG7mVnD5k1lT0hOkP2zd+fLnpio90s/b/hD9uN//yP7nu1bZA8Li5G9oqJMdjOzQaNGyD517DOyf7hmoeyPvjndcQ0AUBcVFurvaotfWSb79p6tZT+x76TsVRVVspuZjZisnxEjH7lD9jULVsjudA5++We/7BfTMmT//I1PZb9wVn/nv3L+ouznjh+X3cysXnyU7HFN4mQ//ZeevSS2TJT90onLsm//Vu81bh4zXPZPl7wi+9INm2U3M4uNaSR7p743yt42uYvs4eF6v3T48HbZzfiFJwAAAAAAAAAXYeAJAAAAAAAAwDUYeAIAAAAAAABwDQaeAAAAAAAAAFyDgScAAAAAAAAA12DgCQAAAAAAAMA1GHgCAAAAAAAAcA3v6z2wtqZW9mvnr8keUT9C9nr16skeHh0uu5lZWUmZ7BmXMmQvzC6UvWGrhrL7+PvIfvnEJdnvmTZW9qCQQNnPnk+VPS89V3Yzs5DIENnLivU5zs3Ikj0sKkz2gqwC2a+c1p8xIDRA9uK8Ytkj4yJlNzOLbVxfdl9/X9nLikplr66ucVwDANRFN48ZKPvRvf/KvnvnBtlPLPtH9t43jpTdzGzbjr9kr6qskj0qPkr2C+dOyD48Ybjs73y9TPavF34ne6eBnWT39NT/L73HDW1kNzPbtu2A7FtW/yj71NdflX3Phj2y3zy8l+wPTrxV9pu69pf942+XyN6xYSPZzcx+O35c9puH95Y9JEDvl9q2aOK4BgCoi15bOkf2Vx+ZKfvFNfo53mfoENn3bNkqu5nZmX/OyD5+/AjZG74fL/vuzftlf2DEfbLHx7eQfci422V30mtkP9knjdfPcTOzb7ftkn3bqm2y975TP4dj4vT86/nlb8v+7Jvvyj5qhN6L9OszSvZOvXrKbmbWY9AA2bsP7yZ7l1u6yJ6bluO4Bif8whMAAAAAAACAazDwBAAAAAAAAOAaDDwBAAAAAAAAuAYDTwAAAAAAAACuwcATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALiGR21tbe31HPjGh1/K7hfoJ3tRbqHsPn6+17MMycNhfFtWXC57bnqu7DU1NbL7Behz4GTK1PtkX/fLH7KXl1XIXlVR5biGyvJK2T299Em+ePyi7F2HdJU9N01fA6fbtaSgWHan+6wgu0B2M7P6jevLXl6q77PSghLZ/YMDZH/tqftlBwC32njwH9l7t0iWvUu7nrJXV+vn5LYD22U3M+vXubfsMxcvkn1kn+6yn7p2TfYTJ1NkP3fwrOxnDp2SfcPGD2WPjoqX3cvTW3Yzs1HjH3c8RolKiJL9l6++lz02tpHsnQfovcyWtetkf2rhi7JXVVXLbmb2w0L9GW75zy2yf/veV7K37NxO9vfmPiM7ALjViq16L+Dh4SH771/9LnuTDk1kHzT4RtnNzFo1aCD7bUP198mV3y2W/fcjx2V/67HnZP/ohxWy+3jpvcKplMuyXzlzRfbctBzZzcxeeuEh2Yf0Hy371at6v/XSh+/JnpeRJ/vSt+bKHhgQIntZuZ6bNG3aSXYzs1sn3SV7UhOnPaGeLRWUlsp+T48espvxC08AAAAAAAAALsLAEwAAAAAAAIBrMPAEAAAAAAAA4BoMPAEAAAAAAAC4BgNPAAAAAAAAAK7BwBMAAAAAAACAazDwBAAAAAAAAOAa3td7YECwv+zFBSWyh8WEy15ZXil7Xkae7GZmtbW1svv5+8peXVkle2BYkOzB4cGyh0aFyr7rxEnZi/KLZU9PSZO9QbN42c3MBgy9Ufb9e4/KHtsoRvaLxy/KnpueK3tgSKDs1dXVsgcEB8heWVYhu5lZUW6R7D7+PrJ7eHrI7rRGAKirBrZuI3tKZqbsgYH6OTzh2amyRwbpfYCZWbduI2Rf+dZHsq//6DvZZyx4VvaHbx0s+5jV22X39PCSveeNt8o+b/UXsudcy5HdzGz1oo9lj4iIk/2F+5+XvXXP1rJvWbFF9qkPjZbdL9BP9lVzVsnesGUj2c3MCgr0eVz/0Q+yj31houw5aXo/BgB11fY1+jn64uzJsntP0COYc4fOyX7m8hXZzcyqHL4Tl5fr2c3KtT/rvnCh7Glp52Xf/ftB2Ru3byz7+8+/Kfum7XovNbj3SNnNzO6aMEx2Dw/928HPNuvn8N97jzmuQUlO7ib71atnZW+f3F/2kY85n6M5k/WeNDgkQvYtf+hz9MW6X/UCeuhsxi88AQAAAAAAALgIA08AAAAAAAAArsHAEwAAAAAAAIBrMPAEAAAAAAAA4BoMPAEAAAAAAAC4BgNPAAAAAAAAAK7BwBMAAAAAAACAa3hf74EFOYW6Z+XLHhwRLHtQaKDs2VeyZTczq66ulr22ukZ2b18f2Ytyi2QPjw6XvVGLRNnLKyplz8/U57h51xay14uvJ7uZ2fET52WPjIuU/fSBU47voXh46hl8bnqu7PHNG8ieevqK7P5B/rKbmVU5XCdvHy/ZE5L1fWC1tY5rAIC66KfDh2UvKS2T/cl5L8t+7tA52R+f/KbsZmY33XuT7JVlPWUfdFM32e8acLvs/v5BssfFNZG9c7/usifmN5I9NkbvE95/3vkcZmdfkz0urpnsJ0+kyO60n+p3Tz/ZF33ytezNO+v1xZ+Ml/2HVUtkNzNbv3Oz7DUOe4m9fx+X/dcvf5H96TF3yA4AbjXt1Ydlf3fuZ7KXFJTI3rF/B9njop2/0z8+epLsw++7V3Zff1/ZAwNDZS8u1s/Zxu0byz6yS2fZl9XTz9GlK9fL3rGrfs5fj4AAPd96YcIzsl9L03OXL7dukL1t15ayt0lIkH3Oq3qv4XSfmpn9uHOT7DOnviP7lEf0njC2UazjGpzwC08AAAAAAAAArsHAEwAAAAAAAIBrMPAEAAAAAAAA4BoMPAEAAAAAAAC4BgNPAAAAAAAAAK7BwBMAAAAAAACAazDwBAAAAAAAAOAa3v+3Xqimplb2vPRc2cNiwmWPaRjtuIa8zHzZffx8/qc1HPn9iOxte7eRfd8vB2T3D/KXPbFloux5GXmy14uvJ7uZWVzDWNlzs/U5Dq0XKnt1VY3sQeFBsl8+cUn2nDSH+yw6TPb4Zg1kNzMLCguWPTM1U/aCLH0OnV4fAOqquY+9Ivui1e/JHhGknzGFuUWyN2qTJLuZWf8bOsj+/KNzZQ8K18+AKbP1OXj3+Rdl37rrR9knjH5K9h9/Wip740atZJ//7UrZzczWv79e9j6j+sh+ZIfer/268SvZZ3zwtuwbv1gl+/Dq+2T/8ZsVsnfsOFB2M7ODp87J/skrC2T38fGTvffQmx3XAAB10fOPzJZ9yMThst837CbZX39DP2fjWyTIbmbWoEEz3ZvFy+7hoV9/2L33yN570FDZMy5lyP70Kv0cDg6OkP2P9Vtl79Snu+xmZtMnTpP94EH9Ho0bt5f9xhtvkz06VM9V0i9dlr2qulr2k4cPyn7Po7fLbma2599TshcV5cmemak/Q1BoL8c1OOEXngAAAAAAAABcg4EnAAAAAAAAANdg4AkAAAAAAADANRh4AgAAAAAAAHANBp4AAAAAAAAAXIOBJwAAAAAAAADXYOAJAAAAAAAAwDUYeAIAAAAAAABwDY/a2tra6znwv8vXyl5VUSl7YU6h7IktG8peWa5f38ysvLRc9rLiMtlTjqTIXlVZJXvm1auyN+/YWvYWXVvInn01W/bqqmrZvby9ZDczS0hOkL1+/Xr6PTz0DH3nL/tkz7mWo7vDOYhr2kD26MRo2bOuZMlu5nwfNGyl72Wn+zAvI0/2BbOflB0A3KpRozayN05qJ7unl7fsy9cukN3Px0d2M7PIoCDZe/YYIfsfu9fLPnjgWNm9vfQaPTz1XqB5O32OF82fJvsD98+S3S/QT3Yzs7imcbI77YeqHPaMs958XPbRwybKPn76JNl//eJX2a9dOyf7s++/LLuZ2Tdvfyv7gLEDZP9y3hLZc/PSZT95Uu/nAMCtHps2T/bSglLZM65ck72kRM9N0tP1zMLM7I7/3C/7j6tWy96hax/Z585/Wvbu7XrIPm7KM7I379Jc9nlTZ8g+YNidsrfs0Up2M7OiXH0dls+bL/srny6UPT4mSnYPD5lt/M23y/74rNdkv3Dsgux7tm/WCzCzhV99KPtHs5bL7uWj9+U9b+sp+5P33ia7Gb/wBAAAAAAAAOAiDDwBAAAAAAAAuAYDTwAAAAAAAACuwcATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALgGA08AAAAAAAAAruFRW1tbez0HvrV0jewZF9Nlj02qL7uXt5fsAcH+spuZndh3UvaI2AjZy0vKZU9LSZO9ICdfv39MpOyJLRNlX/PRYtm9vLxln/jsU7KbmSW1SZI9KjRE9m+XbpQ983Km7IWF2bK36tJB9ryMPNkDQgJkDw4Plv16VJRVyB4UGih7YFiQ7LOmTvw/XhMAuEHbtn1kr6zUz/HNf26S/WJWluxfLv5edjOzjgM6yt66ZWPZHxh+r+x+fvo5NuGZJ2X39tF7hb0b98r++ntPy35jK/2cvn3U47KbmR0/pNfQtU9/2adMHy/7+g2/y57ULkn2BU++LnuPgTfL/uZr+hwE+vrKbmaWUVAg+8Abh8jerFln2e98YrTsDw0ZJDsAuFWb1r1k797nFtnTU6/JHhFVT/YHnh8ju5nZ3Kfmy96up34GDL7rJtmfGP2Q7BUVZbL36ner7JnX9Dka9oD++/jGcbKP7NRJdjOzDu30nnPIKL1fKykokf2HNR/LHhUVL/vAkXfJvnvrFtl9fPReIzhYz87MzC5f1vO3eyZNkn3orf1kjw7Rs6dmsbGym/ELTwAAAAAAAAAuwsATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALgGA08AAAAAAAAArsHAEwAAAAAAAIBrMPAEAAAAAAAA4Bre13tgXnqu7MERIbL7BfrJnnk5U/aUI/r9zcw8vPT8tqK0QvZTf52UPTo+WvaamirZU1POy56bkSN7cHC47FFRCbIntUmS3cysS5PGsr/z1grZzxw9LntOzlXZmzbtJLuHh4fsQWFBsleUlsseFh0mu5lZWJQ+pqJM32dO96FvgK/jGgCgLuo7ZKTsc2dPlT23uFj2D15dLnu/0X1lNzO775Z+so+6dbLsb6/9VPZPX/lE9j0b/5S9401dZP/q63my7zp9WvYF33wte4P6UbKbmXl4jJJ92eyVsm/+ba/s4TH6Ob5m7peyv/nZfNlTUq7IvvPUKdkXTntXdjOz0pIC2e+8/0HZy4vLZC/MKXRcAwDURQNG3il79xHdZc/PzJf9xs5tZJ81VT+nzcze/3yO7Jt3HpC9T3Ky7LW1tbKPeUzvxy4cTZE9qWUz2ZfOfkf2kePGyv7VfL1XMTOb8fF/ZZ8zabrsN/YdJvua7T/JHhceLvuSxfoz+PkFyj56ygTZx48cJLuZ2cR79Tm4+O8l2deXb5X97N9nZP/2W30fmPELTwAAAAAAAAAuwsATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALgGA08AAAAAAAAArsHAEwAAAAAAAIBrMPAEAAAAAAAA4Bre13ugf3CAfiEf/VJ+gX6y19TU6AV4euhuZvkZebIX5hTqv8/PkL2oKFd2P79A2UNDo2RPSTkse2RkA9lHPnSX7E3rx8huZrZw3uey79i8TvbcnDS9hqYdZY+oHyG7030UEOwv+9Vz12T39feV3cws83Km7CGRIbJXllfInn0t23ENAFAXrVv9iey11XovcWT/ftkvXjwme7+7+8luZtYuuZPsCQnJss+Z/JLsa39aKfuBc+dl/+ObP2R/duYi2ee/8YTsT977mOwTn5squ5nZtPGjZF/+6zbZF02bLXv/EbfKXlCQJfv7Mz6U/fUPnpc9KkTvE6Jj42U3Mzty6JzszTs3l/3swbOyBzjs+wGgrnpm2njZ33xV71Xa9Goj+7YdB2Svra2V3czsTJr+Tj5++EDZX3xdP+fi4prKvmvTdtkbNm0m+/4/9HO+18Chssc01HOPhx+4XXYzs+H99DFf//qt7ElRevZz+NIl2Q+mXJC9KK9I9t9+Xyv7mh1/yj761smym5ktWjlX9vEjJsju7TtA9nEzxjiuwQm/8AQAAAAAAADgGgw8AQAAAAAAALgGA08AAAAAAAAArsHAEwAAAAAAAIBrMPAEAAAAAAAA4BoMPAEAAAAAAAC4BgNPAAAAAAAAAK7hfb0H+vr7yF6YUyi7l7eerXr76KUkJifKbmZWGB0ue/qFNNnjGzaVPTsjXfbi4jzZg4NDZff3C5K9c78esjdoXF/2P/cclt3MbNdvv8h++vQB2Rs2bK3fwEPfB6WFpbKn5qfKnpCcIHtS2yTZr5y5IruZWXzzeNmDwgJlz76aLXtoPX2fAEBd9eCzM2R3+vcz7YJ+jq9at0T2QT1ukd3MbPCIcbIHhQfLHh4TLvtnKzfK7vQc/HvXH7J37tlH9vzSEtmHjL5b9nYdWshuZtap4yDZMy9nyt53yAjZn3hyjOw7+neU/dfPt8i+aoW+Rqf/OSG7l6fz9vzm20bL/tMnP8keHhMm+7hxwx3XAAB10U1d+8s+b+0y2WeOe0z2AcNGyd68U0vZzcwO7Tku+2O3jZV9ycY1sh/cuVf2tt07y96gWQPZd2z9TvaohCjZPb30zGFon5Gym5m9smyB7Ms++lb2ea9NkT0lTe9JtyzfLHvrXm1kLyork/3nT3+WvWOvG2Q3M/tixQbZ3/jsHdm/nLdWdn8fPYO8HvzCEwAAAAAAAIBrMPAEAAAAAAAA4BoMPAEAAAAAAAC4BgNPAAAAAAAAAK7BwBMAAAAAAACAazDwBAAAAAAAAOAaDDwBAAAAAAAAuIb39R6YcSlT9hZdW8h++eQl2UOjwvT7X0yX3cwsLCZc9tRzF2T39fOXPS9Pr6F5q46yZ6WlyT54zJ2yxzdrIHvv5GTZX/p8i+xmZiXF+bInJraSPSIiVnYfH1/ZA0MDZa8XFyl79rUc2Rs0jZM9sWWi7GZmaeevyV6QHSC7l4+X7DXVNY5rAIC6qGX3lrJnXMqQ/fDhbbIXlD0je8NGbWQ3M2txg34WF+UVyb7yvXdlT7lwTPbnXl8se1z9JrJfOHFG9rnzP5fd6Tn+8awVspuZNUvuJPuSN+bI3rhxe9m/39xI9iYtdT96aLfsw1vfJ/vISbfr1995VHYzs/AYvW9+Y9ajsq8/8JfuP/0he5tHxsgOAG41c7F+Tu/4eofsG3ZslP2TD7+W/dwh/Zw2M+s0oKPst977gOzHDp6Wfd7yN2V/9O7HZA87EC1734F6LnL6L72+zxe+LfudEybLbmY2bfQE2d/9/gvZl/zws+zvvfiq7I+99pLsk0YNk/2Jp/U5iGkYI7uvv57bmJmFReu9SHZOgeyh9UJl//KdtbIP+Vzv98z4hScAAAAAAAAAF2HgCQAAAAAAAMA1GHgCAAAAAAAAcA0GngAAAAAAAABcg4EnAAAAAAAAANdg4AkAAAAAAADANRh4AgAAAAAAAHAN7+s9sGHrhrKnHE2RvUmHJrKXl5TLHhYTLruZ2YWjF2SvqNTvER0XJ3tohF5DVHw92YPCgmRPapOk/z44QPa1P/8u+9+7t8tuZpaWrq9j69a9ZPf01DP0oKBw2f0C/WTPupote1OH+yw/M192f4dzbGZWWVEle/OWibJf+vei7OGx4Y5rAIC66OBvB2U/e/CM7MNHT5B95fL1sg+6Z6jsZma/rf1Z9p7Db5L94w2rZf/vp1/JPmfmZNkH9r9P9uT2HWTvcksX2Tsk6f3ibwnRspuZrVvyteyj7n9E9on33yZ7g4gI2bu27y37oBF3y+7h4SH76vnLZW+S3Fp2M7Onp+l7+VhqquydmiTJPrKrvs4AUFfV1NTKnnrmsuyBvr6ydx7UWfaOAzrJbmbWv31b2X9Zvln2o/sOyP7+K7NkP3xC/70TX28v2Xt20/uxyqoK2UdNHOa4hp2/6P1crb4NLNthbrFi0xrZt23ZK/vCFd/KHtVAz6buf+B22dPz9dzEzKx3crLsn//2u+wJyQmyV5ZXOq7BCb/wBAAAAAAAAOAaDDwBAAAAAAAAuAYDTwAAAAAAAACuwcATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALgGA08AAAAAAAAAruF9vQcW5xXLHtc0TvaK0grZSwr06/v4Oi+1SYcmskfEhstekF0ge4f+HWXPTc+VvejkZdkPbTsoe0T9SNmP/XFU/32EvkZmZomJrWSvqtTXMSw8RnYPDw/ZL5+6JHvzzi1kL3e4zzw89fuXF5fJbmbWtGNT2UsLSmR3uo7+gf6OawCAumj1koWyN2yon2FJbZJkv2FwV9m/eONz2c3MTp7YK3vzdq1lH9Ba95n3T9d/P+AG2dvdoD/jxq+Xy953VB/ZX3v2Pdn37/1JdjOzwsIc2Vt1ai/7mOHjZZ8wfYrs2/f9KvsTD70h+59bN8m+9pc1sr/4+DzZzcx2HT+h32O+fo/Jb0ySfeFb+l7/dPFLsgOAWyUkxso+duY42ee/rf99LXP4Purr5yO7mdnbT+h/ow8e/l32Wwbqz7Bh58+y39hlgOze3r6yO80kgoLCZe/YcaDsH13Hfu7phS/LnnI0RfZf1nwn+/CRfWVv20PvB996VK9v8hvTZG8cHS17bomeaZiZzf1ktez7Nuk9caPWjR3f43/FLzwBAAAAAAAAuAYDTwAAAAAAAACuwcATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALgGA08AAAAAAAAArsHAEwAAAAAAAIBreNTW1tZez4HzP/9G9tKiMtn9g/xl9/Lxkj0/M192M7PqqmrZI+MiZS/JL5E9on6E7Kf2n5K9acemsmemZsqefiFd9pqaGtn37tgsu5mZp6e+DpWV+jq3a9dHdh9/X9kbNGsge3h0uOwVZeWyh8Xov7947KLsZs7X0cffR/bSolLZA4IDZH/s9mGyA4BbVTs85xZ++b3shXlFsq96/wPZv9n6nexmZks/+Fr27sO6yV5eqp9j6xbrzxgZEy17rcM5TO7WUvZ9P++WveUNbWRf+s6rspuZDRg4XvZdf+rr0CC+meyTXpsue/aVbNn79O8i+7ibb5f9wPH9so8a/oDsZmbf/LhM9tEjHpQ9LEzfJ5cvn5T9yJHfZQcAt2rWrLPsdz88WfacazmyvzH7cdkfmThLdjOzYY/o74vH/zwme+qZK7I7fefOztZ/Hxmpv/NPefMR2edMnSN7eVmx7Kmpem5jZpaQkCx7fkGW7ElJ7WSvn6jPwflT+jk8ee4TsidG1pM9OS5O9j1nz8huZvbXjsOy33nnQNmLy/V91Dw2Vvbo0FDZzfiFJwAAAAAAAAAXYeAJAAAAAAAAwDUYeAIAAAAAAABwDQaeAAAAAAAAAFyDgScAAAAAAAAA12DgCQAAAAAAAMA1GHgCAAAAAAAAcA0GngAAAAAAAABcw/t6D8xNz5O9srxS9sKcQtkDwwJlj02Kld3MLDgsSPa8zHzZ23RrKXthYbHs9z10q+xnr16T3ekc+Af6y15SWKJfPzBUdjOz8nL9Gi2a3yB7ZIN6sqdd0OcgKzVL9thG+j7Iz9LXuKI8U/YWXZvLbmaWfjFDdh8/H9mjE6MdXj/dcQ0AUBdNnfa27C17tJL9+M5jst8+/n7ZX3p8ruxmZndMHSV788R42bs3bSp7YW6R7ItefFn2R1/WPSI2Qvbq6mrZ/913VPag4HDZzcw6Dewk+1uLp8seFqj3U0u/3CB7dZX+jP6++jlfa7Wy+3p5yT58or6HzMyee/od2auqKmS/cuW07JGRcY5rAIC66MY+w2VPOZIie2W5/vf5+ecWyB7TMEZ2M7NfPv1Z9pBIPRcICtVzlelzJst+6spV2d98ZJrsh//uK/voJ8bJ7uHpIbvT3MjMrLZWP8tXvPax7FPmPCr7wDZtZD95Tc9NVn3+o+z7KvR8zstb70X8g/TsyczslhG9ZV/6wdeyt+/bTvb0fD3buaNrV9nN+IUnAAAAAAAAABdh4AkAAAAAAADANRh4AgAAAAAAAHANBp4AAAAAAAAAXIOBJwAAAAAAAADXYOAJAAAAAAAAwDUYeAIAAAAAAABwDe/rPTC+RbzsWalZsofWC5X92rmr/9Prm5mlnb8me0zDGNlPHzkne9++XfTfX0qV3dvXR/ZmjfQ57tyymez/Xtbv7xfoJ7uZ2dl/zsru5e0lu9M5rt+4vuzlpeWyB4QEyO7tq29p/yB/2a+e0/eQmVlxfrHsjdo0kj0vI0/2wJBAxzUAQF3UoJl+Tnp6esi+d+8m2aMb3S97fYfntJnZru//lH3Z0Q9kT0s7L/voBybJfs9Dj8ue2DJR9u8XfC/7wuWvye7vo/c6u05NlN3M7KXxj8r+yPjbZA/09ZV9/YpVsg8bc7fs44feJ3vXrkNl33dO7zdrqqtlNzObPGO87BOGbpR98G36M1SUVzquAQDqosi4SNmdvq8e2vGX7CtXzZH9nlFPy25m1qpbW9nzMvNkv2FIV9nffXmp7Dfde5Pst43TewGnc9yuWZLsf+49LPt7L+i9jJlZUlI72Zd8tUD2jIIC2Z+eof++400dZE85liL7/TPGyv78+Cdlf/jlZ2U3M3tguN5LvL5ysezDOnWU/fDFi45rcMIvPAEAAAAAAAC4BgNPAAAAAAAAAK7BwBMAAAAAAACAazDwBAAAAAAAAOAaDDwBAAAAAAAAuAYDTwAAAAAAAACuwcATAAAAAAAAgGt4X++BuWk5sodEhsheUlgiu5ePXkp1VbXsZmaRcfVkL8wtkj0vPVf27dv2y57YMlH2nGvZsldXVslelF8se1WF/vua6hrZzcy6Du4qe3W1vg65afochseEy17jcJ1ra/RncOpO56hevL6HzMxiGsbIXuRwnxUX6OsY2yjWcQ0AUBed+eeM7I/PmCD7qImTZI9Nqi/7rvV/yG5m9tW378revHFb2Sc+MV32XoO7yf7YbWNln/bkHtlLHtL7ta17/pH9rSeeln3Kq6/Lbmbm5eUj+9SHZsv+wEvjZV/942eyf/39VtnHTHlc9j2bdsr+zYfrZB804WbZzczOXUuTfe2v38t+6PR52ft3bu+4BgCoi66euSJ7lcP32ZVfvyP7lCfmyn7ypJ5JXI9VDmv47fhx2Z+Z/bDsC175VPbLF0/LnuMwU0icpL+P/7Rinex/H94hu5nZmbR02bftPyR7ypEUvYadek85Ytwtsg++f7Dsu7f+Jfujs5+Xfcmst2U3M5vyxizZe7ZoLvuaX/V1yM/Ml71fq1aym/ELTwAAAAAAAAAuwsATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALgGA08AAAAAAAAArsHAEwAAAAAAAIBrMPAEAAAAAAAA4Bre13tgeUm57AVZBbKXFpXJHhYdJntoZIjsZmZlxfo98rPyZY9OjJY9MCxI9n93/yu7p6eeL9dU1+peUyO7X6Cf7MX5xbKbmYVEBMvuHxwge021XmP6xXTZfXx9ZI9pqK9RaL1Q2QND9TW8evaq7GZmsY1iZC/I1v8thEXrNRblFjmuAQDqok4DO8leWlEh+2szJ8nesU0P2WNjG8tuZvbbcb0XSEpqJ/s3ny6R/fLJy7IHBOjnuI+X3voN6dZZ9mOpqbIPvX2i7JmpmbKbmXXvNUT2XTs2yB72SbjsUQlRsldXVsu+9tPFst/z0OOyp57S5/CtyTNkNzN78u2XZV/x0Xey97q9l+x33XKv7IcObZMdANxqwZIXZb+SmyP7W/NWyL5o0fOy33zTX7KbmQ0aO1j2skq9X6oXrPcSXy5dL/sNw26QPfwvPfuJqB8p+4q5q2Vf9Nkc2Z32QmZmB8+cl33/pv2yj3tylOztb2ov+0+rt8r+9LQJsr/0xW+y79u8U/YV6/V9amaWHFdf9pFDH5I9Pf2C7GFhevbzwkN6r2LGLzwBAAAAAAAAuAgDTwAAAAAAAACuwcATAAAAAAAAgGsw8AQAAAAAAADgGgw8AQAAAAAAALgGA08AAAAAAAAArsHAEwAAAAAAAIBreNTW1tb+v14EAAAAAAAAAPzfwC88AQAAAAAAALgGA08AAAAAAAAArsHAEwAAAAAAAIBrMPAEAAAAAAAA4BoMPAEAAAAAAAC4BgNPAAAAAAAAAK7BwBMAAAAAAACAazDwBAAAAAAAAOAaDDwBAAAAAAAAuMb/Bwr3FPe5o4L0AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets visualize weights after pruning\n",
        "# Weight pruning at 80 percent sparsity\n",
        "N_WEIGHTS = 9\n",
        "\n",
        "weights = weight_pruned_model.input_layer.weight.data\n",
        "\n",
        "plot_weights(weights, N_WEIGHTS)"
      ],
      "metadata": {
        "id": "bNpmr_LVNY_1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "ff9691c3-383a-4fee-eaf4-d4f64564677c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABTwAAAMWCAYAAADcdEn9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFiUlEQVR4nO3aeZCXd34f+B+ihW5p5BHi1gFC0mhOe8bJxo4rceI43krFtTmqEk8yloQOBBI0DXTTd9N3N93QdCOBQAIdY89MKntkvZtar7NZZ5Nae2LP2J5LJ0JCHN2AZjQ6R/dv/9iaqlQifZ6Gp5/f0/3l9fr3/fy+3zfQEj+99cyrVqvVCgAAAABAAi4ouwAAAAAAwEwxeAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJqJvugxu27SyyB5w3Ft+4KMynXjwV5nuGt8xkHYA545mTJ8N8944na9Rk9ursWhvmPd37a9SET9Kw7fYwHxt+okZNzt3Du5vLrgBQik1tY2VXmPN29W0K883tu2vSg7ltd39D5jPe8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJRV3YBON9MvXiq7AoAc9LuHU+WXWHWe++DDwo9f2vLnWE+OvhYofenYGz4ibIrAHAeG+nZGOaNnRNhPj6wJczrW3eG+eb23WEOM8UbngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMurKLpCSxTcuCvOpF0/VqMnHW7JyceYzk0ematAEACjCUP/BQs8fHXys0POpjcbWNWE+MnCoRk0AqLXGzolcn69v3TlDTSjLSM/GMM/7M1KpVCo7ujeEeVPXntx3ZPGGJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQjLqyC6TkL/7dX4b50puW1qjJx5s8MlXq/ZVKpbJk1ZIwn3xhskZNADjfbGr63TC/6ML4a9Fw/6GZrENBtrbcGeajg48Ven9L+91hPtj3aKH3T8fIgJ9lANI00rMxzBs7J2rUZPaqxe9BU9eewu/I4g1PAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZdWUXmEtOPH8izJetXlbo/ZMvnAzzJauWhvlLT72QecfFl14W5otvWBzmJw/HHVOQ9eec9XMCQDl273iy7Aq5tXfeG+Z9PQdq1KQ433p8V5j/8zs2h/no4GMzWeesDfY9Wur9AHA+a+ycKLsCs4Q3PAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBk1JVdYC5ZtnpZrs9ff9t1YV6txp9fsnJxmE8emQrzG25bFV9QqVSOP3c8zKeOTIb50puWZt5Rplt++eYwf/bPn8s848TzJ8I86/fg5OGTmXcAwMfp6zlQdoVcHtvXnfnMJZdcUYMmxentWR/mHZ17a9QEAGpvZ299mG/pGK9RE8533vAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJRN1MHLb1paZifPHxypq6as44+9XKh5x/54fNhvvJzqzPPWH7z8lx3ZDn8/WfD/KYv3JLr/Cxvvf52mH/x17+Yecb3/vh7Yb7gogvPqtN/bcnKxbk+D8C5adh2e5iPDT9Roybl+fojg2H+tXtacp1/57quXJ+fCzo694b5xsavhfnEyNdzd3hg678I8wdHfz/3HQDwcbZ0jJddgTlgZ299mM/Ez5E3PAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZNTN1EEnD5+cqaMKc/y542H+7rtvh/mqz9+c6/4XfvBcrvOz+q/83Oowf/57T4d5pVKprP7iZ8L80suvCPOjT78U5jd94ZYwX3rT0jDP+3N22VWXhfkrx1/JdX6lUqm89KOjuT4/eWQqdwcAzt7Y8BNlVyjc7z06FOZfu6elRk2Ks7XlzjAfHXysRk0+3gXz5uU+4576fxbmD47+fu47AACKsqVjvPA7vOEJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACSjruwCtbT85uWl3n/JJZfn+vyrr06F+fJK/Ov76U9PZd5x5Ifxj8Rbb70W5ldccXXmHZGTh0/m+vyi6xeF+bN/9myu8yuVSuULf/sLYX7m5dNhPnkk/nMEYHba2nJn5jOjg4/VoMm5+5d3N5ddoXB1F8zu/5+/e8eTuc94ZPxfzUCTT3Zfw+8Uej4AQNFm9zdCAAAAAICzYPAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSUVerixbfuCjMp148VXiHqZemwvy9d94N8+tuvT7X/W+++dMwv+GzfyPX+T/49p+H+Ruv/yTzjKyOn/rUtWF+w22rMu8o0qmjxf8cff8/fL/wOwCYfUYHHyu7AtMw1H+w7Apz3sNj34zz3c01agIAnI929zfkPsMbngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMupqddHUi6cKv+OFHzwX5qs+f3Oh96+4dUWYf/k3vxzmp18+nev+H/zwP4b5T34ymXnGRQsuCfPLr7g6zD/84P0wv+TSK8P8s7/8i2H+7F/+MMwvvPCiMF/5udVhDsDs1di6Jsx/9t67Yf7g6O/PZJ1zkvVrGBk4VKMmAAAwO21qGwvz3f0NmWd4wxMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASEZd2QVm0qrP3xzmz/3VU2F+85duy3V/3YL4t/Pm65aF+TtvvRPm1992XZhfeulVYX7hhReF+f//zMVhvmLFrWF+/PizYb5gQXz+Bx+8F+bVajUj/yjMj/zw+TBfsOCSMK9UKpXlNy/PfAaAmTcycCjMNzZ+rUZNzl3Wr6FoX39kMMy/dk9LjZp8sp6edWE+9drrYb535+/PZJ0Zd++mfx7mB3Z/K/cdWf8sTIx8PfcdAMDctKN7Q5g3de2pUZNiecMTAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEhGXdkFZtILP3guzG/+0m1hvvrLq8P8+e8+H+aXXnFpmH/68svD/L7f/q0w/9PDh8O8bsdgmL/wVy+E+XRMvnAyzC+++LIwn5o6EuaXX351mK/6/M1h/vIzR8O8Wq2G+fKbl4c5ALPXxMjXy64w633tnpYwf3h3c5gfPnUqzEcHHwvzXcMNYV6pVCqbt41lPjOXHdj9rTDfsPVfZp6xZ/T3wtw/CwBQnInBrWG+sWW0Rk3OTVPXnrIr1IQ3PAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZNSVXeDnpo5MhvnilUsyz1j1+ZvDfMWtK8L8+e8+H+bLb1ke5lctvCrMl3zqU2E+9dprYb74qivDfOGKhWH+4fsfhHmlUqms/tzKMP93/+qPw3zJqqVhfskPLw/zlZ9bHeZZrrv1+lyfB4Dz2eRPfxrmr739dq7zN28by3xm3eavhvm+Xd8I80cf7Ajzux/ozexQpj2jv5f5THPbXWE+1H9wpuoAQFLGB7ZkPlPfujPMN7aMzlQdCuQNTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGXVlF/i5xSuXFH7HsWeOhfltv3JbmF96xSVhPnlkMswPvXQqzG//7b8X5j9+440w//SVV4T5t3/07TCvVCqVl350NMxXf3l15hl5HH36pTC//jM3hPlT3/mrML/tK186qz4fZyrjz7kWP8sA1N7Wljszn/ngww/DfPeOJ2eqTiG6tz9cdoXKvl3fyPX5ux/ozfX53p71Yd7RuTfX+Vk6u9ZmPtPTvb/QDgCcv/YMNYb5huaRGjUpRn3rzrIrFG58YEuYnw+/B5WKNzwBAAAAgIQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZNSVXWC6pl6aynxm8Q2Lc93x1J88levzv/xbXwnzZddeE99/4kSYf/ryy8P85KlXwvzzv/a5MK9UKpUf/KcfhvlV11wZ5j+ZejXMn/mL74f5/PkXhnmW277ypVyfn44PPviw8DsAmH0++DD73/+7dzxZgyaf7BuHRsL8q2saa9Rk7uro3Bvmu4Ybwvytd9/NdX5P9/4wB4AibWiOv0sw+9W37iy7wqzgDU8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBl1M3XQkpWLw3zyyFSu8xffEJ9fqVQqJ54/EebLVi8L86kjk3GHlUvC/M//8DtxHqbZ/uG//M0wv37pojA/88brmXf8rX/ya2H+8vPHw3z+/PlhfsUVnw7zn/70dJjPBstvXl52BQA+xn0NvxPmD499M9f5u3c8mevz03Hvpn8e5gd2fyvMv7qmcSbr/Df2j7eE+dr6wULvr1Qqlea2u8J8qP9gofdv3jZW6PkA8ElGejZmPtPYOVGDJmnb2Vsf5ls6xmvUhDy84QkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJKNupg6aPDI1U0eds2Wrl4X5Cz94LswvuCDef3/4Z38R5gsuvCjMb/7Fz4Z5lv/t9/4ozK+/7bowP/rUy7nur1Qqlc/8jc+E+dN/+nSYZ/0ZzTs876w7AUClUqk8PPbNsivkdmD3t8quEFpbP1h2hcpb775bdoXQtrY1YT7cf6jwDhsbvxbmEyNfD/OsXwMAH2+kZ2OYN3ZO5Do/7+eZni0d42E+Mbg1zDe2jM5kHc6RNzwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGTMq1ar1ek8uGHbzqK7lO65v/xRmL/9szfC/PTpo2F++eVXh/mv/MZvhnkKsn6PK/PmhXFd3YVhvmDBJWF+wQXxxr/0pqVhXqlUKpMvnAzzJauyz8hjz/CWQs8HmK3u2zRU6v3b2tZkPjPcf6jQDt98LP4+9jt3xn9HPLG/P8xvX9t21p04/zy8u7nsCgCl2NQ2VnYFErCztz7Mt3SM16jJ3LW7vyHzGW94AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJqKvVRUtWLg7zySNThXeYeim+48qrfiHMb/7Fz4b5ycPXhfmbb7wa5n/1J98O8y/9yn8X5kd++HyYL1hwSZhXKpXK8puXZz6TR9bvYZanv/u9MK+rWxDmWb++l556IbPDDbetynwGgPQM9x8qu0Lld+7ckuvzt69tm6Em5fnGoZEw/+qaxho1AQA4e1s6xsuucF7whicAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkIy6Wl00eWQq1+eP/PD5zGdWfm51mC++YXGuDlmW3rQ064lc5588fDLMs379s8HiGxeF+Xf/6C/C/IMP3g/zN974SUaD68P0httWZXwegNlqW9uaML9gXvz/eQf7Hp3JOqV49MGOML/7gd4aNSnOV9c0hvk3Do3k+nyWgb4Hwry1/cFc5wMAkJ83PAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZNRN98HFNy4K86kXT4X5kpWLw3zyyFSYr/zc6jA/H3z57385zP+f/+nfh/mbb76aeccv/c1fPatOZyvr5+S9934W5tdcszTMl6yK89kg658lAM7NcP+hsivk1tm1Nsx7uveH+d0P9M5knTnpq2saw3xb25owz/o5am1/8Kw7nY2NjV/LfGZi5Oth3tZxT5j39z5yVp0AYLYY6dkY5o2dEzVqwmznDU8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBl1031w6sVTuS6aPDKV6/NUKpMvTIb5zV+6rfAOU0fiDotXLsl1/o2fvSnX5+eCrH+Wlq1eVqMmAMw2Pd37y64Q+sahkTD/6prGGjU5d8P9h8quEJoY+XrmMy3td4d5f+8jM1UHgBoaH9gc5vWtu2rUZPZq7JwouwJzhDc8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBk1JVd4HyyZNWSMJ98YbJGTc7dL/3ml8P85OGTNWqSrhPPnyi7AgB8rK+uaSy7Quk2Nf1umL/93nthfmD3t3J3GOx7NPcZAMw+9a27yq4AyfCGJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJCMurILnE++84ffCfNlq5eF+fPfezrMV3/xM2fd6WydPHwyzBddvyjML5g/L8wnj0yddaezsWTl4lLvB4DzWXPbXZnPDPUfrEGTc7d7x5NlV6is3/Ivwnzvzt+vURMASM/u/oYw39Q2VqMms9eeocYw39A8UqMmn8wbngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMurKLnA+WbZ6Wa7Pz5+f74/r5OGTmc98+OEHYX7hRQtydSjb5JGpsisAUJLmtrvCfN68eWE+2PfoTNYpxWD/hjBvadtT6P1D/QcLPb8WurbfF+avvf12mO/e8WTuDnt3/n6uz9/X8Dth/vDYN3OdDwBz2aa2sbIrzHobmkfKrpDJG54AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDLqZuqgpTctDfOTh0/O1FXnrZWfW53r81l/RsyMJSsXh/nkkakaNQHgvzTUf7DsCpk2Nn4tzCdGvp7r/Ja2Pbk+T6XSvf3hsivk9vDYN8uuAABQKG94AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJqJupg04ePjlTR5GwJSsXh/nkkakaNSlOCr8GgLmos2ttmPd0769Rk3M3MfL1sitQsnWbv5r5zL5d36hBEwD4b40PbA7z+tZdNWoCMW94AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJmFetVqtllwAAAAAAmAne8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJRN90HNzbvKrJHpaP17jDvHXi00Puno3nbnWE+NPxYjZqka7B7fZi3dO2d1efXwsTQ5rIrAJSivnWs7Aq5DXStC/PW7n1h3tV6T5h3Dzxy1p1m0mjvxjDf2jGR+47u9rVhftlFFxXe4Xw3PtBQdgWAUuT9LrKt8Y4wHx55PNf5s0HW39NdffvDfLAr47/Zu2f/f7PPddP5e77s7+XT6egNTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBnzqtVqdToPbmzeleuiiaHNhZ6fgsHu9WHe0rW3Rk3O3Vz/c87qX6lk/xp2DzaE+aaWsbPq9F+bTkeAFNW35vv3ZwpGejaEeWPnnjAf6FoX5q3d+86601wz1r8pzD/48MMwz/o9ngu2Nd4R5sMjj4f5+ED8XQcgVa++9VaYb+8/UKMm5257271xPst/DVl/jze07a5JD8o1ne8i3vAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJIxr1qtVqfz4MbmXUV3ya2x8fYwHxl5okZNoDgTQ5vLrgBQitOvvxbm/UOHatQEzm/jAw1lVwAoRX3rWK7PNzfdGeZDOx7Ldf5s0LJtTZgPDvu+Ntt1td6T+Uz3wCO57hjsWh/mP3377TBfevXVmXd4wxMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASMa8arVanc6DG5t35bqoo/XuMO8deDTX+XC+mBjaXHYFgFLUt46Ven9HS/xdplKpVHoHfZ+Z67pa7wnz7oFHatSkOLv66sN8c/t4mI8PNMxkHYA549333w/z13/2szDvGzo4k3XmpO72tWHe1be/Rk3mrpGeDWHe2Lkn1/nb2+7NfOaiurowb+nem6tDlul8F/GGJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQjLpaXdQ78Giuzzc23p75zMjIE7nuIL++rnVh3t69r0ZNZq/dgw1hvqllrEZNADgbvYP5vstUKpVKb8d9Yd7R+3DuO4htb7s3zvsP1KjJuRvafn+YN29/KMw3t4/PZB2A80ZT14OFnj/SsyHMGzv3FHp/LXT17S/1/u72tZnPZHXs68zYPXqK3T2q1Wquz3e03B3mc+G70HR4wxMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIxrxqtVqdiYPefvfdMG/uemgmrqFgTU13hPnFF14Y5j39j8xgm7M3MbQ5zDc278p9x0jvhjBv7NgT5h2td4d578CjYZ71awRIVX3rWKHn7+h+IMybuh7MPKO9+a4w7xs6eFadmHu629eGeVff/ho1Kc74QEPZFQBKUfR3kfPBYNf6MG/p3lujJsxl0/ku4g1PAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZ86rVanU6D25s3lV0l8Jtb783zvsO1KgJnLuJoc1lVwAoRX3rWNkVmAMeHd8e5nfXx3nRxgcawrwWP+fb2zK+E/fH34mzfg0AqfJdBGaH6XwX8YYnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJCMulpdNNR9f5g3dz1UeIftfQcKvyOys78+zLe0jdeoSXkmhjaH+cbmXTVqMntl/ZwAAJ/sww8/KLtCqL51LMzbmtdkntE/dChXh+395X4nBmD2emhHU5jf37SjRk1I2fa2e+N8Br6reMMTAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEjGvGq1Wp3OgxubdxXdhYI9sW8w85nb17XUoAl5TAxtLrsCQCnqW8dyfX6kZ0OYN3buyXU+nC/GBxrKrgBQirzfRSjfxGD2f09vbIn3r67We8K8e+CRs+o022T9+iqV8n+N0/ku4g1PAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZdWUXoHZuX9dSdoXzQv/29WHetn1vjZoApGWkZ0OYN3buyZXPBt3ta8O8q29/jZoAAKRnY8uu3Gd0DzwyA02KM9C1Lsx/9t77YT6dX99o78Yw39oxkXlG0bzhCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAko26mDmpvvSvM+wYOztRVn6h/+/owb9u+t/AO4OcMoBiNnXvKrlC4rr79ZVdIXl/nujBv79lXoyYAcHa2t92b/Uz/gRo0YTZr7S7+u8zWjonC78jLG54AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDLqZuqgvoGDYd7eeleuz09H2/a9uc8433V3rA3zrt79NWpy/hrt21h2BQBIVnvPvkLP72i5O8x7Bx8t9H4A0rW9/0DZFXIbH2gI8/rWsULv39Z4R+YzwyOPF9qB2vCGJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQjLqZOmhiaHOYv/HOOzN1FQXq6t1f6v17hreE+YZtO8N8qPv+MH/tZz8L88GhQ2E+E3YPNoR5tVotvAPAXDTauzHMt3ZMFHr/YNf6zGdauvcW2iGvkZ4NYd7YuadGTcrT3b42zLv68n0X6h18NNfnASBl9a1jpd4/PPJ4qffPBbv66jOf2dw+XoMm+XjDEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIxrxqtVqdzoMbm3eFeU/nfWHe2fPw9FvNUSO9G8K8sWNPmLc0rwnzwaFDZ93pbD2xdyDMb1/fWniHuW5sYFOYN7TuznX+xNDmXJ8HmKvqW8fKrnDea8v4rtJfg+8qRRvoWhfmrd37cp0/PtAQ5rPh57y3I/5ef+Ull9SoCcDs8uFHH4X55vbxGjWBTzbYtT7MW7r31qhJcbK+T1Uq3vAEAAAAABJi8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJRV3aBmdTYeHuYj4w8Uez9HXsKPb8Wbl/fWuj5e0e3hfn6rcNh3tpyV5gPDB48605na2d/fZg3tO4O86Hu+8O8ueuhs60EwBzR1rwmzPuHDtWoybmpRb+HdjSF+f1NOwq9v7V7X6Hn17eO5T5joGtdmOf9NXT0Phzm4wMNuc4HmKs2t4+XXYGcutvXZj7T1be/Bk2K09K9t+wKs4I3PAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkzKtWq9XpPLixeVehRYa67w/z5q6HCr2f2aGz7Z4w7+l/pEZNZq+Joc1lVwAoRX3rWNkVZr2u1vjv0e4Bf4+S3/hAQ9kVAErhu0ilMtq7Mcy3dkzUqEl5HtndFeb3bOou9P6HdjSF+f1NOwq9fzaYzncRb3gCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMmoq9VFjY23h/nr77xToybMZj39j5RdoXA9nfeFeWfPwzVqAsB/qbnpzjAf2vFY4R1GezeG+daOiTD/1GWXzWQd5qix/k1h/s5774V5tVKdwTYApCTru0gK9o22hPm6rYM1avLx7m/aUer9M2Fb4x1hPjzyeO47vOEJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACSjrlYXjYw8UaurCjPSuyHMGzv2FHr/YPf6MG/p2lvo/cyMarVadgUAPsbQjsfKrlDZ2jGR6/MNbbtnpghzWtE/B+MDDYWeDwBlWrd1sOwKyRseebzwO7zhCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAko26mDurfvj7M27bvnamrStPYsafU+1u65v7vIZVKV+/+sisAnJfG+jeFeUPb7pr0AADg4x0Y6wzzext6cp3f3b4285muPv/NngJveAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyZhXrVar03lwY/OuorsAlUqlsfH2MF/x6U/XqAnA7PLhRx+F+eb28Ro1oUj7d7WH+drNfWG+Z2hrmG9oHj3rTqkZ2n5/mDdvfyjMxwcaZrIOwJxR3zpW6v0t29ZkPjM4fKjQDm3NcYd5lXlh3jd0cCbrUICBrnWZz7R276tBk082ne8i3vAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJRN90H+7rWhXl7977cZYBKZWTkiTCfGNpcoyYAs8u8efPKrkANrN3cl+vzG5pHZ6hJuurmzy+7AgAfo7fjvjDv6H049x19nRnbTk+87fQPHcp1/+MPDYT5Hfe3hnnLtjVhPjicrx+VSmsi+543PAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZMyrVqvV6Ty4sXlX0V2gMtzzQJhv63xwVp9fCxNDm8uuAFCK+taxsisAlUplfKCh7AoApfgoYz5paNtdmyKc19qb7wrzvqGDNWpSnul8F/GGJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQjLqyC3B+aWy8Pcy3dT5Y6P1Fnw8As9nE4OYw39iyq0ZN+CQ7uh/IfKapy/cZgDI0tO0uu0LpBrrWhXlr975C7+/rjO9v74nvb2++K/uOoYNn1anWZnu/SqVS2dVXH+ab28cL7+ANTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGXVlF/i5sYFNYd7Qujv3HbsHG8J8U8tY7juIjYw8kevzwz0PhPm2zgcL/TwAzGUbW3aVXYEMTV35v4sMdK0L89bufbnvAGDmzYZ/f29rvKP0DpH2nnz39w0dnKEm525XX32Yb24fr1GT4px67bUwH+xaH+Yt3Xtzd/CGJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJCMedVqtTqdBzc27yq6y6zX1HRHmO/Y8XhNesxluwcbwnxTy1iNmpRnqPv+MG/ueijMJ4Y2z2QdgDmjvnX2/x0x2LU+zFu699aoyblp3Hp7mI+MPlGjJuev8YH4u1KlUv4/C9PpCJCisv/9OxNGezeG+daOiRo1KUbqv765ornpzjAf2vFYrvOn813EG54AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDLmVavV6nQe3Ni8q+guwDRMDG0uuwJAKepbx8qukKmj5e4wn39B/P+at/cfyHV/V+s9Yd498EiYj/VvCvOGtt1n2ej80958V5j3DR2sUZNz19e5LsyvuPjiGjUBmF3eeOedMG/v2VejJuna0f1AmP/4zTfDfHjk8RlsUwzftyqV5qY7w3xox2NhPj7QkHmHNzwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGTMq1ar1ek8uLF5V9FdCtfeeleY9w0crFGTuWt7+71x3negRk0+XnfH2jDP+nGfTv/dgw1hvqllLPOMPCaGNhd6PsBsVd9a7L9fScNo78Yw39oxUaMms1dz0525Pr/kU5+amSIAc4zvIvm1N2fsMkN2GbKND8S7TKXiDU8AAAAAICEGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBnzqtVqdToPbmzeVXSXUE/nfZnPdPY8XIMmUK6Joc1lVwAoRX3rWNkVqIGOlrvD/L0PPgjzBXV1Yd47+OhZd6qlwa71mc+0dO8ttMNY/6Ywv2DevELvB5itPsqYTxradtemSInam+8K876hgzVq8vHGBxrC3PfJNGT9OVcq3vAEAAAAABJi8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJIxr1qtVssuAQAAAAAwE7zhCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJKNuug+Of+t/DfOpl6Zyl4ksXbUk85mTL0wW2iGvJSvjX8MF8+P9+cTzJ2ayTikWXb8ozE8dPVWjJnPXYPPasisAlGL91h1lV8itve2eMO/rf6RGTc5fI/0bw7yxbSLX+aP99WG+tW081/mzwd7RprIrAJSivnWs7Apz3mhv/Pfw1o58fw9zfhgfaMh8xhueAAAAAEAyDJ4AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMuqm++DUS1NF9qgsvmFxmJ98YbLQ+2fCspuWhvmJwydr1OTcLbp+UZifOnoq1/l5Pw/A+Wt8eEuY12/bWej9D440Zj7zQONImPf1PxLmnR1rw7ynd39mB2Jvv/teoedvbRsv9HwAmMs++uijsiswC/R23BfmHb0P577DG54AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDLmVavV6nQebBnaX2iR5TcvD/Pjzx0v9P7ZYPENi8N86qWpGjX5ZAtXLAzzCy6IN/RTR0/lOv/MsTNhnmXJyiVhPnlkMvOMZauXhfmJ50+cVaezNdi8ttDzAWar9Vt3lF0BMu3ZsTXMNzSN1qhJcfaONpVdAaAU9a1jZVcAKpXK+EBD5jPe8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAklFXq4sWXb8ozN97573COyy7aWmYnzh8svAOkamXpkq9fzrOHDszp8+fPDKZ+4wTz5+YgSYAcP7Zs2Nr5jMbmkZr0KQ4c70/AOUZ6FoX5q3d+2rUpDibN38tzHft+nqNmnyyrVt+N8xHdz4Z5i3b1oT54PChs+50Nsq+f7bwhicAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkIx51Wq1Op0HW4b2h/nCFQvD/MyxM9NvBbPYkpVLwnx+3fwwP/7c8Vz3DzavzfV5gLlq/dYdZVcoXWdH/HdAT2/8fY1KpW/7+jBv37630PtbWu4K88HBg4XePxP2jjaVXQGgFPWtY2VXyK1l25owHxw+VKMm5ehouTvzmd7BRwvtsK3xjjAfHnm80Pvngqzfo6VXX515hjc8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGTMq1ar1ek82DK0v+gu5HTsmWNh/s6bP8s849lnvxPmixZdH+avv/7jML/qqmvCfNkNN4b5e++8F+bXfzbul/V7tOLWFWE+Gww2ry27AkAp1m/dkevzOwc2hfmW1t25zofzxd7RprIrAJSivnWs7Aqz3rbGO8J8eOTxMO9uj/97t6uv+G1qpGdDmDd27im8A7HxgYbMZ7zhCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAko26mDrr2umvD/PTLp3Odv2z1ssxnTjx/ItcdRTv14lSYv/X622F+9MWnwnzhwhVh/rlf/WKYVyqVyuqv3Bzmn/3Vz4b5K8dfCfNTR09ldog8+2fPhvnUkfj3eMWt8e/RSz94KbPDDZ+/IfOZSNY/Kxdc4P9DABRhS+vuwu/Y1rwmzIeHDhXegdje0aYwP/bjH4f54ODBXPfv2bE1zDc0jeY6H4DyjA80hHl961iNmpRneOTxXJ/v6ts/M0VyaOzcU+j529vujfP+A4Xef76wrAAAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyaibqYNOv3w6zJetXhbm8+bNC/Pjzx0/606zzaIbF+f6/Movrsz1+b/2G1/OfObiCy8M8//+i18M8//4zDNh/tLiq8P8vXffD/MsR390NNfnL7w4/vXPhKx/VgAox54dW8N8Q9No5hnDQ4dmqs55a7S/Psy3to3nOn/91h25Pp+lve2eMM/6Odo9tDnzjk3Nu86qEwC1Ud86VnaF3Aa61oV5a/e+GjVJ1/b+A7k+39Uaf9foHngkzLdu+d0wH935ZGaH7W33xnnOX+NM8IYnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJCMedVqtTqdB1uG9of5ousXhfmpo6em3+ocvXL8lTC/Zvk1hXeYzb7wt76Q+cybr74R5nULLgzz577zXJh/5q/fGuZP/+dnwnw2WLhiYZifOXam0PsHm9cWej7AbLV+646yK0ASOtrvDfPevgNhvne0aSbrAMwZ9a1juT4/0LUuzFu79+U6fzY4MNYZ5vc29NSoCbNZX2f8z0KWKy6+OPMZb3gCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMmom6mDTh09NVNHnbNrll9T6v3Hnn45zOdfGP92Tx07Hua/9Ot/Lcz/3m//zTD/3l89F+aVSqVy1cKrwvy578RnnHj+ROYdkZVfuDHMj3z/xTCfOjIV5otXLj7rTv+1M8fO5D4DAKAsvX0Hyq4AcF5q7d5XdoXcntw3HOb3NvTUqAnnaqRnQ5g3du4pvEN7T75/FsYHGjKf8YYnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJCMurIL/NzCFQvD/MyxM4V3OP7s8TBffsvyMF/xmevC/PTRU2H+z7b+izC/7IpLw/zwkbj/6ZdPh3mlUqlc8QtXhPnJwyfD/NXTr4T5stXLwvzI918M81MvToX54pWLwzyr/9KbloY5AOnaObApzLe07q5JD2a3xm13hPnI8OO5zh8bbMh8pqFlLNcdW5ruCPOdOx7PdT4A6frdddvKrpBLW/OazGf6hw7VoMknO7SnN8zXbOjIdX5j555cn58rvOEJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACRjXrVarU7nwZah/UV3Oe/dveYfhfmjh/6XGjU5d5MvTIb5V37rK2F+4vkTM1knSYPNa8uuAFCK9Vt3hHl/9/1h3tb1UK77tzTdkfnMzh2P57qjtfXuMB8YeDTX+Xk9ONIY5g80jtSoCWXaO9pUdgWAUtS3jpVdIdP2tnvjvP9AjZrMXR0t8fex3sFyv49RqYwPNGQ+4w1PAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZdbW6aOGKhWF+5tiZGjWZvf7fp58pu0LlN3/718L8j/7gP4X5klVLwvzE8yfCPOvnIOvnCIB0jfbXh/nWtvFC7/+Fyy4r9PxKpVIZGHg01+fHBhvCvKFlLNf5DzSO5Pp8Cnq61oV5Z/e+GjUBgP/W9v4Dpd5/cKI7zO/a2FXo/Y1bbw/zkdEnMs/oHcz3fYxsbc1rwrx/6FDuO7zhCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAko67sAj+37KalYX7i8MkaNSnOl//OL4b5u++9n+v8a6+7Nsw/vezTmWf86OkjYb7yiyvD/Nt/8KeZd0TmXRBv8MefPR7my29ZHubHnjkW5hdfdnGYVyqVysIVCzOfiSxbvSx+oFrNdT5Aqra2jZd6f1vXQ6XePx0NLWNlV0heZ/e+sisAwKz11luv5fp8b8d9Yd7R+3CYj4w+ket+aqN/6FDhd3jDEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIRl2tLjpz7EytrirMi99/Mcx/9R/9Sph/9//+y1z3X3vdtWF++uXTYf7pZZ/OvGPJdYvC/NUfvxbm114ffz6vY0+/nOvzK25dkbvD4hsWh/nUS1NhfuL5E7nOB4Bz1dmxNsx7eveH+e6hzWG+qXnXWXcCgPNFW/OaMO8fOlSjJsXZ2JLvu0BH78Mz1KQ4D+9sDfP7tgzUqAkRb3gCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyagru8DPLb95eZi//+77mWecOnoqV4fJFybDvG5B/Nv1vz/yb8L8l/7OXw/zJSuXhPnkkbhflqf/9OnMZ1Z+cWWYL1786TBf+lvXhPl/+sP/nNkh8u7b7+b6/EyYemlqTp8PwMdr2nZnmF904YWZZ/zCZZeFeUPLWJiP9G8M88a2icwOkZ7e/bk+/+FHH+X6/GywrXlNmA8PHapREwDON/0J/B2zrfGOMB8eebwmPcp035aBsiswDd7wBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSYfAEAAAAAJJh8AQAAAAAkmHwBAAAAACSUVd2gZ87/tzxMF9209LCOyxZtSTMjz8bd7z4ksvDfPKFybPu9F/614/uD/P58+M/zn98512Zd1x25WVhflHdhWH+x//2T8L8mf/8TJi/8caPw/yXf+NXwzzrz2j5LcvDHIB0NW27M8x3DD9WoyafrLFtouwKoS2tu8uukNvw0KFS7+/vvj/ML12wIPOM06+/HuaDgwfPqhMATNfwyONlV8hlpGdD5jONnXtq0ISiecMTAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEhGXa0uuva6a8P89Munw/zE4ZMzWedj/ehPfhjmC5ctDPOPPvogzI+/eCTMXz39kzC//PJPhfk11ywP88//2ufDvFKpVL688sYwf/yJPwjzb/+f/zHMf/KT+M9x1apfDPMsy2+Jfw9OvTgV5otuXJx5x6LrF8V3HD2VeQYAM2+od0OYv/rWW4V36Nu+Pszbt+8tvEMeu4c2h/mm5l01apKutq6Hyq4AAJ9oaPv9Yd68fW7/PdbYuafsCrNeR8vdmc/0Dj5agyb5eMMTAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEhGXa0uuujSiwq/49jTL4f5BXXzw/y1106H+ZtvvhrmF110aZhfeeU1Yf7ii98L81/4haVh/g/v/idhvmrxtWFeqVQqjz/xB2H+Pz92MMxf/clU3GHVl8L86sVXh3le77/3Qe4zTh09NQNNAJhpzR17yq5Qad++t+wKuWxq3hXmY4MNYd7QMpa7w76d28J83Zbh3HeUqWnbnWG+Y/ixGjUB4Hy0ve3eMG/e/lCNmqSrt+O+MO/ofbjQ+4e23x/mWX/GvYOPzmSd0njDEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIRl2tLjr2zLHC71jxmevC/PTRU2G+7LpVYf7j0/Hn33rrp2F++eVXhvnFF10W5n/9N38tzJfeuDjM/82//vdhXqlUKv/2G98M8+ee+/Mwv+662+IL5sUb+8/e+FmYv/SDl8L8hs/fEObLb1ke5jNh6aolYX7yhcnCOwBw9pq23RnmO4Yfq1GT2eu1n71d+B3rtgwXen7jtjvCfGT48ULv93MEQJm29x8ou0LyLqqr2dT2sd58551S758tvOEJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACTD4AkAAAAAJMPgCQAAAAAkw+AJAAAAACSjbqYOWn7z8jA//tzxmbrqnB1/4aUwX3DRxWH+05+eCvPVn/lSmL8yNRXmf/t/+AdhvvILN4b537zlljD/wyf+KMwrlUrl7bdeC/MVKz4T5ldfvSjML7xwQZhfeuWlYb5wxcIwP3PsTK7Pz4STL0wWfgcAM2/H8GNlV8j04EhjmD/QOFLo/dt79hd6fi2MDD9edoXCDfVuCPPmjj01agIA55+mrgcLPX+ga12Yt3bvK/T+ucIbngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMgyeAAAAAEAyDJ4AAAAAQDIMngAAAABAMupm6qDjzx0P8xW3rgjzd99+N8xPv3w6s8PkC5Nh/t778R0LlywJ8yuv/lSYZ/0as/JbfvmWML/s8kvC/F/9H/8hzL/7J38c5pVKpTJ16sUwv+22Xw3zCy6IN/TLLvtUmC9csTDMs34Orr3u2jCvhaw/52PPHKtREwBS80DjSJjvHtoc5puad81knTlpoOeBMG/tfLBGTYrT3LEn1+ezfo8AmL22bvndMB/d+WSh929vuzfX5xfUzQ/z1u59uc5PwWz4PdjVVx/mm9vHa9Tkk3nDEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIxrxqtVqdzoMtQ/tzXbTo+kVhfuroqVznz4TTGR2++OtfCvPJI5NhfubYmTBfuGJhmGc5/N3nw/yVV+J+lUqlMjV1JMwvufjyML/qU9fGn78k/nxd3YIw/+yvfC7Ma+Ha6+Jf4+mXT+c6f/nNy8P8/n/8D3KdDzBXrd+6o+wKhRsbbAjzhpaxGjU5f7W03BXmg4MHa9Rk9to72lR2BYBS1LfO/b+HR3o2hHlj554aNaEsO7ofCPOmrgdr1OTcjQ/E35krFW94AgAAAAAJMXgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJqKvVRaeOngrzhSsWhvmZY2dyd8i6I8v8uvm5Pv/5X/t8mE+9NBXmrxx/JcyvXHhVmD/11LfDvFKpVOrqFoT52z97PcxX3fTFML/w4vj86z97fZjPBhdcEP9/ghW3rgjzY88cC/N5F8w7604A54M9O7aG+Yam0Ro1KU5Dy1jZFc57g4MHCz2/s2NtmPf07s88o6P93jDv7TtwVp0AmB06Wu4O897BR3Pf0di5J/cZzG1vvPNO2RUytTfflfsMb3gCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyTB4AgAAAADJMHgCAAAAAMkweAIAAAAAyagru8DPnTl2JsxX3Loi84zLr7oszH965rUw/2u/8eUwf+ONt8L8t//p3w3zwycnw/zSqy4N87wuvfTKzGfeffftML959S+H+U1fXh3mT/3pj8J88oX492jJqiVhnteym5ZmPnPi8Mlcd1z3mevC/OWnX851PkCqNjSNll2hcA+ONIb5A40jNWoyd3W03xvmvX0HatTk4y2YPz/3GWX/GgAoRu/go2VXoAZ2dD8Q5k1dDxZ6/1z4OesbOhjm4wMNmWd4wxMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASEbddB9cceuKMD/2zLEwX3T9ojA/dfRUrvOnY9lNS8P8z/6v74b5b//Tvxvmz718PMzrFlwY5jddvyzMf+nWm8L8qWPx/dNx/Nn4jPl183Odf9vf+Gyuzy9bHf8enXj+RK7zTxw+mevzlUqlsnDFwjB/+emXc98BQJoeaBwpu0Iu25rXhPnw0KHcd3R2rA3zSxcsyH1Hkdq37y27AgCz1GDX+jBv6fZ3SAqauh4su0Lputvj73Ndfftz3+ENTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGQZPAAAAACAZBk8AAAAAIBkGTwAAAAAgGXXTffDYM8fC/Nrrrg3zU0dPTfeqc7bo+kVhfuLwyVzn/8H/+O/D/LZfuS3MfzL54zD/8P0PwvzN194K8w/eiz8/HV/5+18J8w8//DDMzxw7E+ZZPyenXz4d5tWPPgrzWsj6Ocv7s551PgCz10j/xjBvbJuoUZOPt71zbZz37M91/vDQoVyfn46e3nwdybal6Y6yKwAkqa15TZi3dO8tvENvx31h3tH7cOEdyjTQtS7zmdbufTVocn7r6iv++5w3PAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZBg8AQAAAIBkGDwBAAAAgGQYPAEAAACAZNTN1EGnXz49U0eds1NHTxV6/opbV4T5U3/yVK7zP/qwGucffRTmF116Ua77K5VKZeqlqTBftnpZrvPL/jlZdP2iMJ/Oz1DRP2dFnw9wvhrq3RDmzR17ct/R2DaR+4wiXTg//urX0nJXmA8OHpzJOsxSO3c8HuZ7R5tqUwRglmncenuYj4w+Eeb9Q4fCvK9zXZi39+wL8+l45/33cp8xl2V9F2J6Brrin9XW7vw/q3l5wxMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASIbBEwAAAABIhsETAAAAAEiGwRMAAAAASMa8arVaLbsEAAAAAMBM8IYnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQDIMnAAAAAJAMgycAAAAAkAyDJwAAAACQjP8P1u8hUEVXFKUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets visualize weights after pruning\n",
        "\n",
        "# Unit pruning at 80 percent sparsity\n",
        "N_WEIGHTS = 9\n",
        "\n",
        "weights = unit_pruned_model.input_layer.weight.data\n",
        "\n",
        "plot_weights(weights, N_WEIGHTS)"
      ],
      "metadata": {
        "id": "JTTzx1kQN-Zc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "f5a93b73-c246-48dc-96fb-b0c84a3b1670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVBWjZMMDglS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "7ba2a9de-25c9-4ad2-b724-b895fdc56dc4"
      },
      "source": [
        "plt.figure()\n",
        "plt.title(\"Accuracy vs Sparsity\")\n",
        "plt.plot(df_unit[\"sparsity\"], df_unit[\"accuracy\"], label=\"Unit-pruning\")\n",
        "plt.plot(df_weight[\"sparsity\"], df_weight[\"accuracy\"], label=\"Weight-pruning\")\n",
        "plt.xlabel(\"Sparsity (as fraction)\")\n",
        "plt.ylabel(\"% Accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7d5168a81120>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterative Pruning"
      ],
      "metadata": {
        "id": "v0o38Rn74CfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike One-time  pruning + fine-tuning which achieves the desired prune rate by pruning and fine-tuning once, multi-time iterative pruning + fine-tuning achieves the desired prune rate by pruning and fine-tuning multiple-times.\n",
        "\n",
        "For example, to achieve the desired prune rate of\n",
        "98\n",
        "%\n",
        ", we could run pruning and fine-tuning for many iterations, achieving prune rate of\n",
        "30\n",
        "%\n",
        ",\n",
        "50\n",
        "%\n",
        ",\n",
        "66\n",
        "%\n",
        ",\n",
        "76\n",
        "%\n",
        ",\n",
        "\n",
        ",\n",
        "98\n",
        "%\n",
        " in each iteration.\n",
        "\n",
        " So basically we **train for some epochs, prune slightly and train again**. We repeat this process until desired sparsity is reached. In this case we use masks (either 0 or 1 ) per layer to get remember the indices of pruned elements."
      ],
      "metadata": {
        "id": "EeLX9LKIJ9wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLinear(nn.Linear):\n",
        "\t\"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n",
        "\n",
        "\tdef __init__(self, in_features, out_features, bias=True):\n",
        "\t\tsuper().__init__(in_features, out_features, bias)\n",
        "\t\tself.register_buffer('mask', torch.ones(out_features, in_features))\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\treturn F.linear(input, self.mask * self.weight, self.bias)\n",
        "\n",
        "class MaskNet(nn.Module):\n",
        "  \"\"\"A non-sparse neural network with four hidden fully-connected layers\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MaskNet,self).__init__()\n",
        "    self.input_layer = MaskedLinear(784, 1000, bias=False)\n",
        "    self.hidden1_layer = MaskedLinear(1000, 1000, bias=False)\n",
        "    self.hidden2_layer = MaskedLinear(1000, 500, bias=False)\n",
        "    self.hidden3_layer = MaskedLinear(500, 200, bias=False)\n",
        "    self.hidden4_layer = MaskedLinear(200, 10, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden1_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden2_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden3_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden4_layer(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "NDNO5rNOO_0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_model = MaskNet()\n",
        "train(masked_model, train_loader)"
      ],
      "metadata": {
        "id": "ssVHc3r3Rd6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84ca7be-7fcb-44c8-b6ff-51f470b724c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0\n",
            "Running loss: 2.304874897003174\n",
            "Running loss: 76.76715981960297\n",
            "Running loss: 109.778748229146\n",
            "Running loss: 135.28254686295986\n",
            "Running loss: 157.45223983377218\n",
            "Running loss: 177.13233572244644\n",
            "Starting epoch 1\n",
            "Running loss: 0.1056298092007637\n",
            "Running loss: 16.24857532605529\n",
            "Running loss: 30.733562655746937\n",
            "Running loss: 45.56268412619829\n",
            "Running loss: 60.21372275054455\n",
            "Running loss: 75.56490474194288\n",
            "Starting epoch 2\n",
            "Running loss: 0.15725381672382355\n",
            "Running loss: 11.790306011214852\n",
            "Running loss: 21.916142197325826\n",
            "Running loss: 33.41096215136349\n",
            "Running loss: 43.41215421445668\n",
            "Running loss: 54.65624484233558\n",
            "Model trained in  2.2282642364501952 minutes on  180000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = test(masked_model, test_loader)\n",
        "print(\"The accuracy of our masked vanilla NN is\", acc, \"%\")"
      ],
      "metadata": {
        "id": "CFyraaMvSHIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c31372-ac75-4e56-8bc7-058f97f6c113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of our masked vanilla NN is 96.6 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_prune_rate(max_prune_rate=0.5,start_epoch=0,num_prune_epochs=8,total_epoch=30):\n",
        "    \"\"\"Function to calculate pruning ratio for different pruning epochs\"\"\"\n",
        "    final_prune_epoch = int(0.8*total_epoch) # change ratio to change final prune epoch\n",
        "    #num_prune_epochs = 8\n",
        "    prune_rates = [max_prune_rate*(1 - (1 - (i / num_prune_epochs))**3)\n",
        "                    for i in range(num_prune_epochs)]\n",
        "    prune_rates[-1] = max_prune_rate\n",
        "    prune_epochs = np.linspace(start_epoch, final_prune_epoch, num_prune_epochs).astype('i').tolist()\n",
        "\n",
        "    return prune_rates,prune_epochs\n",
        "\n",
        "def prune_iterative(model, prune_percentage):\n",
        "  \"\"\"Function that takes un-sparsified neural net and does weight-pruning\n",
        "  by k sparsity\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "  linear_layers = [m for m in sparse_m.modules() if isinstance(m,MaskedLinear) ]\n",
        "  with torch.no_grad():\n",
        "    for idx,layer in enumerate(linear_layers):\n",
        "      if idx == 4: # skip last layer of 5-layer neural net\n",
        "        break\n",
        "      # change tensor to numpy format, then set appropriate number of smallest weights to zero\n",
        "      layer_mask = torch.ones(len(layer.weight.reshape(-1)))\n",
        "      layer_copy = torch.flatten(layer.weight)\n",
        "      layer_copy = layer_copy.detach().numpy()\n",
        "      indices = abs(layer_copy).argsort() # get indices of smallest weights by absolute value\n",
        "      indices = indices[:int(len(indices)*prune_percentage)] # get k fraction of smallest indices\n",
        "      layer_mask[indices] = 0\n",
        "\n",
        "      # change masks of the layer\n",
        "      layer.mask = layer_mask.reshape(layer.weight.shape)\n",
        "\n",
        "\n",
        "  return sparse_m\n",
        "\n",
        "def train_iterative_prune(model, train_loader, epochs=10,learning_rate=0.001):\n",
        "  \"\"\"Function to train and prune  neural network \"\"\"\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  time0 = time()\n",
        "  total_samples = 0\n",
        "  t=0\n",
        "  prune_rates,prune_epochs = gen_prune_rate(max_prune_rate=0.98,start_epoch=0,num_prune_epochs=4,total_epoch=epochs)\n",
        "  print('Pruning rate: ',prune_rates,'Pruning epochs: ',prune_epochs)\n",
        "  for e in range(epochs):\n",
        "    print(\"Starting epoch\", e)\n",
        "    total_loss = 0\n",
        "    if e in prune_epochs:\n",
        "      model = prune_iterative(model,prune_rates[t])\n",
        "\n",
        "      acc = test(model, test_loader)\n",
        "      print(\"Pruning epoch : \",t,\"The accuracy after pruning is\", acc, \"%\")\n",
        "      t+=1\n",
        "\n",
        "    for idx, (images,labels) in enumerate(train_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      optimizer.zero_grad() # forward pass\n",
        "      output = model(images)\n",
        "      loss = lossFunction(output,labels) # calculate loss\n",
        "      loss.backward() # backpropagate\n",
        "      optimizer.step() # update weights\n",
        "\n",
        "      total_samples += labels.size(0)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if idx % 100 == 0:\n",
        "        print(\"Running loss:\", total_loss)\n",
        "\n",
        "  final_time = (time()-time0)/60\n",
        "  if e == epochs-1 :\n",
        "    acc = test(model, test_loader)\n",
        "    print(\"Final accuracy  is\", acc, \"%\")\n",
        "  print(\"Model trained in \", final_time, \"minutes on \", total_samples, \"samples\")"
      ],
      "metadata": {
        "id": "HpIgDPacJ85O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterative_prune(masked_model, train_loader)"
      ],
      "metadata": {
        "id": "5F9YGsjHgUsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91383787-277a-433b-bfb3-69cf69f3413a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruning rate:  [0.0, 0.5665625, 0.8574999999999999, 0.98] Pruning epochs:  [0, 2, 5, 8]\n",
            "Starting epoch 0\n",
            "Pruning epoch :  0 The accuracy after pruning is 96.6 %\n",
            "Running loss: 0.16523437201976776\n",
            "Running loss: 9.3027507327497\n",
            "Running loss: 18.76873247232288\n",
            "Running loss: 28.65185328759253\n",
            "Running loss: 37.88563337083906\n",
            "Running loss: 48.35851882677525\n",
            "Starting epoch 1\n",
            "Running loss: 0.08906206488609314\n",
            "Running loss: 10.208917921874672\n",
            "Running loss: 20.237705050501972\n",
            "Running loss: 30.570326490793377\n",
            "Running loss: 40.23407665500417\n",
            "Running loss: 49.941341259051114\n",
            "Starting epoch 2\n",
            "Pruning epoch :  1 The accuracy after pruning is 96.32 %\n",
            "Running loss: 0.0834960862994194\n",
            "Running loss: 10.913656605407596\n",
            "Running loss: 22.058276074007154\n",
            "Running loss: 32.71847140416503\n",
            "Running loss: 43.38931733742356\n",
            "Running loss: 55.170114032924175\n",
            "Starting epoch 3\n",
            "Running loss: 0.07965534180402756\n",
            "Running loss: 11.188260098919272\n",
            "Running loss: 22.246824849396944\n",
            "Running loss: 33.01389125175774\n",
            "Running loss: 43.88015326671302\n",
            "Running loss: 54.408241333439946\n",
            "Starting epoch 4\n",
            "Running loss: 0.13052557408809662\n",
            "Running loss: 10.512196231633425\n",
            "Running loss: 21.337160743772984\n",
            "Running loss: 32.835115602239966\n",
            "Running loss: 43.41224339790642\n",
            "Running loss: 54.71823941357434\n",
            "Starting epoch 5\n",
            "Pruning epoch :  2 The accuracy after pruning is 91.71000000000001 %\n",
            "Running loss: 0.31485819816589355\n",
            "Running loss: 34.71181686222553\n",
            "Running loss: 69.0109581053257\n",
            "Running loss: 104.79689775407314\n",
            "Running loss: 139.99045410752296\n",
            "Running loss: 174.18558225035667\n",
            "Starting epoch 6\n",
            "Running loss: 0.3044002950191498\n",
            "Running loss: 34.375983104109764\n",
            "Running loss: 69.02695566415787\n",
            "Running loss: 104.2226292937994\n",
            "Running loss: 139.00199621915817\n",
            "Running loss: 174.2158580571413\n",
            "Starting epoch 7\n",
            "Running loss: 0.44321882724761963\n",
            "Running loss: 34.881333097815514\n",
            "Running loss: 69.50831165909767\n",
            "Running loss: 104.42663560807705\n",
            "Running loss: 139.772991001606\n",
            "Running loss: 174.92079286277294\n",
            "Starting epoch 8\n",
            "Pruning epoch :  3 The accuracy after pruning is 55.779999999999994 %\n",
            "Running loss: 2.205476999282837\n",
            "Running loss: 222.45804834365845\n",
            "Running loss: 442.84693360328674\n",
            "Running loss: 663.2260255813599\n",
            "Running loss: 883.4098317623138\n",
            "Running loss: 1103.6484537124634\n",
            "Starting epoch 9\n",
            "Running loss: 2.200683355331421\n",
            "Running loss: 222.5257408618927\n",
            "Running loss: 442.7098479270935\n",
            "Running loss: 663.0531287193298\n",
            "Running loss: 883.2844133377075\n",
            "Running loss: 1103.5822353363037\n",
            "Final accuracy  is 55.779999999999994 %\n",
            "Model trained in  4.881158530712128 minutes on  600000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNGO5Ih9Hv52"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Clearly, my hypothesis that accuracy will rise and then negatively correlate in a roughly linear way with pruning was incorrect. The figure instead shows a dramatic nonlinear relationship between accuracy and pruning. Accuracy remains roughly constant until dropping off at about 75% sparsity for weight-pruning and until 70% sparsity for unit-pruning. My hypothesis that unit-pruning impacts accuracy more dramatically than weight-pruning held up.\n",
        "\n",
        "These results are fascinating: Less than 25% of the neural net represents important information about its function. The data also suggest that accuracy may slightly increase with a light amount of pruning (~30%), although I would run on more iterations with a larger dataset to be sure. It would make sense that keeping the net's smaller weights reduces its generalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Consider the below statements about pruning and answer question Q1:\n",
        "\n",
        "A. In network pruning, we prune the small-weight connections: all connections with weights below a threshold are removed\n",
        "from the network.\n",
        "\n",
        "B. In network pruning, we prune the small-weight connections: all connections with weights above a threshold are removed\n",
        "from the network.\n",
        "\n",
        "C. Pruning is one of the methods for inference to efficiently produce models smaller in size, more memory-efficient, more power-efficient and faster at inference with minimal loss in accuracy, other such techniques being weight sharing and quantization.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7fitFsxdfU4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHDYadgG-RHl",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Which of the above statement(s) is/are true?\n",
        "Answer1 = \"Both A and C\" #@param [\"\",\"Both A and B\", \"Both B and C\", \"Both A and C\", \"Only A\", \"Only B\", \"Only C\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Consider the below statements about quantization and answer question Q2:\n",
        "\n",
        "A. Neural network quantization is a process of reducing the precision of the weights in the neural network, thus reducing the memory, computation, and energy bandwidths.\n",
        "\n",
        "B. When deploying neural networks models on mobile or edge devices, quantization and model compression in general, is desirable and often the only reasonable way to deploy a mobile model because the memory and computational budget of these devices is very limited.\n",
        "\n"
      ],
      "metadata": {
        "id": "-Dtz-Nlq1nz6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu3sufDD7pu1",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Select the true statement(s) from the above:\n",
        "Answer2 = \"Both A and B\" #@param [\"\",\"Only A\",\"Only B\",\"Both A and B\",\"Neither A nor B\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"Nil\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1043bc5f-d245-481f-d8ef-608b0c158b3c"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2922\n",
            "Date of submission:  16 Mar 2024\n",
            "Time of submission:  15:22:11\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}