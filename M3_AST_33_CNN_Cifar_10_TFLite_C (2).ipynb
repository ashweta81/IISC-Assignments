{"cells":[{"cell_type":"markdown","metadata":{"id":"p4Nwm4FK3wgU"},"source":["# Advanced Programme in Deep Learning (Foundations and Applications)\n","## A Program by IISc and TalentSprint\n","### Assignment : IoT and Edge Devices - CNN to classify Cifar 10 dataset using TFLite"]},{"cell_type":"markdown","metadata":{"id":"qu26Vq9jDTpj"},"source":["### Learning Objectives:\n","\n","At the end of the experiment, you will be able to:\n","\n","*  train and evaluate a CNN model on CIFAR10 dataset using Tensorflow\n","*  convert a Tensorflow model to TensorflowLite for Microcontrollers\n","*  test the TFLite Model using TFLite Interpreter\n","*  understand the types of optimization in TFLite - quantization, pruning and clustering\n","*  prune and fine-tune the model to 50% sparsity\n","*  apply sparsity preserving clustering on the pruned model and observe the number of clusters and check that the sparsity is preserved\n","*  apply both QAT and PCQAT on the sparse clustered model\n","*  evaluate the model, which has been pruned, clustered and quantized and then check the accuracy from TensorFlow persists in the TFLite backend.\n"]},{"cell_type":"markdown","metadata":{"id":"2NiLPMhPgYFa"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"e9CxCComgihL"},"source":["#### Description"]},{"cell_type":"markdown","metadata":{"id":"S0k4a9Qbe6p1"},"source":["In this experiment, we will use the CIFAR-10 dataset from keras API. It consists of 60,000 colour images(32x32) in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.\n","\n","\n","\n","Here are the classes in the dataset, as well as 10 random images from each:\n","\n","\n","<img src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Images/CIFAR10.png\" alt=\"Drawing\" height=\"350\" width=\"440\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"iiVBUpuHXEtw"},"source":["### CNN to classify Cifar-10 dataset (Images)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Gt0KwMrt0I0n"},"source":["So far, we saw how to build a Dense Neural Network (DNN) that classified images of digits (MNIST) or even fashion images (Fashion-MNIST). Here we will instead, recognize the 10 classes of CIFAR ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' and 'truck'). There are some key differences between these two image datasets that we need to take into account.\n","\n","First, while MNIST were 28x28 monochrome images (1 color channel), CIFAR is 32x32 color images (3 color channels).\n","\n","Second, MNIST images are simple, containing just the object centered in the image, with no background. Conversely, CIFAR ones are not centered and can have the object with a background, such as airplanes that might have a cloudy sky behind them! Those differences are the main reason to use a CNN instead of a DNN."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["## Setup Steps:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWMVQWk58aXm"},"outputs":[],"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwqosl928dBA"},"outputs":[],"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWzUR5IMqb7b","cellView":"form"},"outputs":[],"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","ipython = get_ipython()\n","\n","notebook= \"M3_AST_33_CNN_Cifar_10_TFLite_C\" #name of the notebook\n","\n","def setup():\n","\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer1():\n","  try:\n","    if not Answer1:\n","      raise NameError\n","    else:\n","      return Answer1\n","  except NameError:\n","    print (\"Please answer Question 1\")\n","    return None\n","\n","def getAnswer2():\n","  try:\n","    if not Answer2:\n","      raise NameError\n","    else:\n","      return Answer2\n","  except NameError:\n","    print (\"Please answer Question 2\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_WnUJoAL1pc2"},"source":["### Importing required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6OQ_tVTaU3oo"},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","#import tf_keras as keras\n","import tensorflow.keras as keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.metrics import classification_report, confusion_matrix"]},{"cell_type":"markdown","metadata":{"id":"sPogyxx5zt5J"},"source":["### Import and Inspect the Dataset"]},{"cell_type":"markdown","metadata":{"id":"MNoaUYniNr5M"},"source":["Cifar-10 repository: https://www.cs.toronto.edu/~kriz/cifar.html\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiwcGwmLJVIc"},"outputs":[],"source":["# From tensorflow import and download the cifar10 dataset\n","cifar10 = tf.keras.datasets.cifar10\n","(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3x39c36sJ60N"},"outputs":[],"source":["# Check for the shape of train and test images\n","print(train_images.shape, train_labels.shape)\n","print(test_images.shape, test_labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"yzWNjl4JjiKU"},"source":["- The image data shape is: `(#images, img_heigth, img_width, #channels)`, where channels are in RGB format (red, green, blue).\n","- The labels shape is `(#images, label)`, where label goes from 0 to 9.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vopv6HetVj8y"},"outputs":[],"source":["train_images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"chqX49HIlW5c"},"outputs":[],"source":["# Plotting the sample image from the dataset\n","plt.imshow(train_images[1]);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APDXWEtgKVfT"},"outputs":[],"source":["# The CIFAR labels happen to be arrays, which is why you need the extra index\n","train_labels[1][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7oj0W6uIQxDm"},"outputs":[],"source":["# CIFAR10 labels/classes\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3O7Gy8-HU_5"},"outputs":[],"source":["class_names[9] # The List's index is the label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMfskv-rpXv-"},"outputs":[],"source":["idx = train_labels[1][0]\n","class_names[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTRiPf7QKJWh"},"outputs":[],"source":["print(\"\\t\", class_names[train_labels[1][0]])\n","plt.imshow(train_images[1])\n","plt.axis('off');"]},{"cell_type":"markdown","metadata":{"id":"KJjO_Qi2YV1s"},"source":["### Visualize the images from trainset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tubjj9IeLLWp"},"outputs":[],"source":["def plot_train_img(img, size=2):\n","    label = train_labels[img][0]\n","    plt.figure(figsize=(size,size))\n","    print(\"Label {} - {}\".format(label, class_names[label]))\n","    plt.imshow(train_images[img])\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sw_E_Jp4M1fK"},"outputs":[],"source":["plot_train_img(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIww0M0YqFe3"},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(train_images[i])\n","    plt.xlabel(class_names[train_labels[i][0]])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JyjP8DsPp-rj"},"source":["Note that images are in color, not centered and with different backgrounds"]},{"cell_type":"markdown","metadata":{"id":"SuPx6Fpg2-ER"},"source":["### Preprocessing the Images (Train and Test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDC5qAy9lxHI"},"outputs":[],"source":["test_images.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zr1V-RwhJ0dC"},"outputs":[],"source":["# Normalize pixel values to be between 0 and 1\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cx8cqGbeWHB2"},"outputs":[],"source":["test_images.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8FbgbiaYpNV"},"outputs":[],"source":["plt.hist(train_labels[:5000]);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5t5JF1P-Zm3f"},"outputs":[],"source":["# Validation set\n","val_images = train_images[:5000]\n","val_labels = train_labels[:5000]\n","print(val_images.shape, val_labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxWvJq-BaB8s"},"outputs":[],"source":["train_images = train_images[5000:]\n","train_labels = train_labels[5000:]\n","print(train_images.shape, train_labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peWqyBAyaw5X"},"outputs":[],"source":["plt.hist(train_labels, alpha=0.5)\n","plt.hist(val_labels, alpha=0.5)\n","plt.hist(test_labels, alpha=0.5);"]},{"cell_type":"markdown","metadata":{"id":"xsvJeVm6_3VB"},"source":["### Create CNN Model Architecture and Compile"]},{"cell_type":"markdown","metadata":{"id":"TsXxF2G2GWLy"},"source":["In `tf.nn.conv2d()`: [Convolution layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n","\n","* **images** is the input mini-batch (a 4D tensor).\n","\n","* **filters** is the set of filters to apply (also a 4D tensor).\n","\n","* **strides** is an integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Default (1,1).\n","\n","* **padding** must be either \"SAME\" or \"VALID\": Default = 'VALID'\n","\n","  * If set to \"SAME\", the convolutional layer uses zero padding if necessary. The output size is set to the number of input neurons divided by the stride, rounded up. For example, if the input size is 13 and the stride is 5 as shown in the figure below, then the output size is 3 (i.e., 13 / 5 = 2.6, rounded up to 3). Then zeros are added as evenly as possible around the inputs, as needed.\n","\n","  * If set to \"VALID\", the convolutional layer does not use zero padding and may\n","ignore some rows and columns at the bottom and right of the input image,\n","depending on the stride. This means that every neuron’s receptive field lies strictly within valid positions inside the input, hence the name valid.\n","<br><br>\n","<center>\n","<img src=\"https://wizardforcel.gitbooks.io/scikit-and-tensorflow-workbooks-bjpcjp/content/pics/padding-options.png\" width=450px/>\n","</center>\n","$\\hspace{9.5cm} \\text {Different padding options}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FTRI--UM5fK"},"outputs":[],"source":["model = Sequential()\n","\n","model.add(Conv2D(\n","    filters=32,\n","    kernel_size=(3,3),\n","    activation='relu',\n","    input_shape=(32, 32, 3))\n",")\n","model.add(MaxPool2D(2, 2))\n","\n","model.add(Conv2D(64, (3,3), activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Flatten())\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dxq2JU7dVIEX"},"outputs":[],"source":["# Define the optimizer and loss\n","LOSS = 'sparse_categorical_crossentropy'\n","OPTIMIZER = 'adam'\n","\n","# Compile the model\n","model.compile(optimizer=OPTIMIZER,\n","              loss=LOSS,\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"utH8U6ud44cY"},"source":["### Train the model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjAJ1xXz4pPE"},"outputs":[],"source":["NUM_EPOCHS = 20\n","\n","early_stop = EarlyStopping(monitor='val_loss',patience=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1QKpQHFoVJI"},"outputs":[],"source":["# Fit the model\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=NUM_EPOCHS,\n","                    validation_data=(val_images, val_labels),\n","                    callbacks=[early_stop]\n",")"]},{"cell_type":"markdown","metadata":{"id":"CpPMPhbQjTXw"},"source":["### Visualize accuracy vs no. of epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2VQKFylodzi"},"outputs":[],"source":["# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","#plt.xlim([0,NUM_EPOCHS])\n","plt.ylim([0.4,1.0])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"I9KzYRI05g_M"},"source":["### Evaluate the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3eLN-JVvduA"},"outputs":[],"source":["model.evaluate(test_images, test_labels)"]},{"cell_type":"markdown","metadata":{"id":"GTW4IkOrwAAl"},"source":["**Accuracy**\n","- Train: 85% - 90%;\n","- Validation: 68%-70%\n","- Test: 66%-68%"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MD_r_qxxwF1M"},"outputs":[],"source":["# Model predictions\n","predictions = np.argmax(model.predict(test_images), axis=-1)\n","predictions.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hDUe5uq6PoX"},"outputs":[],"source":["# Print the classification report\n","print(classification_report(test_labels, predictions, target_names=class_names))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCwieAWB6yAh"},"outputs":[],"source":["# Print the confusion matrix\n","confusion_matrix(test_labels,predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3QJQ3ER7Btb"},"outputs":[],"source":["# Plot the heatmap\n","plt.figure(figsize=(15,8))\n","sns.heatmap(confusion_matrix(test_labels,predictions), cmap='Blues', annot=True, fmt='g');"]},{"cell_type":"markdown","metadata":{"id":"nhL-fPKh7ryx"},"source":["### Testing the Model (Predicting)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8q8RbCU-i5g"},"outputs":[],"source":["plt.imshow(test_images[15]);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2GDFWkXG_H6Y"},"outputs":[],"source":["test_labels[15][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LshDRY7-8Iq"},"outputs":[],"source":["class_names[8]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eE6NF_wV-YfZ"},"outputs":[],"source":["test_images[15].shape"]},{"cell_type":"markdown","metadata":{"id":"7UGml1PgAYxS"},"source":["The input Tensor shape should be: (num_images, width, height, color_channels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-bF-5tY_-RT"},"outputs":[],"source":["my_image = test_images[15]\n","my_image = my_image.reshape(1,32,32,3)\n","my_image.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a43UdQJqXYn9"},"outputs":[],"source":["# Using trained CNN model to predict the image\n","model.predict(my_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGUt18nZ_WN8"},"outputs":[],"source":["# Taking max probability from the predictions\n","img_pred = np.argmax(model.predict(my_image))\n","class_names[img_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OofdGSufEIHT"},"outputs":[],"source":["img_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WF1DSVq1DGAV"},"outputs":[],"source":["pred_prob = model.predict(my_image)[0][img_pred]\n","pred_prob"]},{"cell_type":"markdown","metadata":{"id":"SDTrIeHPlsNf"},"source":["### Visualizing model predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kojDXuqRBodN"},"outputs":[],"source":["def img_pred(img, size=4):\n","    label = test_labels[img][0]\n","    my_image = test_images[img]\n","    plt.figure(figsize=(size,size))\n","    plt.imshow(my_image)\n","    my_image = my_image.reshape(1,32,32,3)\n","    img_pred = np.argmax(model.predict(my_image))\n","    pred_label = class_names[img_pred]\n","    pred_prob = model.predict(my_image)[0][img_pred]\n","    print(\" Label {} <=> Pred: {} with prob {:.2}\".format(\n","        class_names[label],\n","        pred_label,\n","        pred_prob))\n","    plt.grid(False)\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E11rQVtPCg5x"},"outputs":[],"source":["img_pred(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kIEAJ6KE86i"},"outputs":[],"source":["for i in range (10):\n","  img_pred(i)"]},{"cell_type":"markdown","metadata":{"id":"BhnnOt7bgXGg"},"source":["### Saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ty0qLh9ngk2-"},"outputs":[],"source":["!pwd # Linux command, shows where we are in CoLab's folders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vN8z8vpgaup"},"outputs":[],"source":["model.save('cifar_10_model')"]},{"cell_type":"markdown","metadata":{"id":"wdEONve0j0ht"},"source":["Use [Netron](https://netron.app) to visualize the model, hyperparameters, tensor shapes, etc. Netron is a viewer for neural network, deep learning and machine learning models (See [GitHub](https://github.com/lutzroeder/netron) for instructions about instalation in your desktop)."]},{"cell_type":"markdown","metadata":{"id":"G0Fksxpps31l"},"source":["## Converting to TensorFlow Lite\n","\n","TensorFlow Lite is TensorFlow’s lightweight solution to run TensorFlow models on mobile, embedded and IoT devices with low latency quickly. With TensorFlow Lite we can perform classification and regression task without incurring server cost.\n","\n","TensorFlow Lite model supported on both Andriod and iOS via C++ and Java API. Generally, the size of the TensorFlow model is quite big that we can’t run it as it is on mobile and other embedded devices with limited compute and memory resources. So to make it possible to run TensorFlow model on low processing devices,  TensorFlow introduces a lightweight solution i.e TensorFlow Lite.\n","\n","Before we start with TFLite, we need a trained model that is trained on a set of data using a high powered machine. This trained model can be converted to TensorFlow Lite format. We cannot create or train a model using TensorFlow  Lite. So, we must start with a regular TensorFlow model and then convert it to TFLite.\n","\n","The next step is to convert the saved TensorFlow model to TensorFlow Lite model using TFLite converter. TensorFlow Lite model is an optimized FlatBuffer format identified by the` .tflite` file extension. Flatbuffers is a special serialization format that is optimized for performance."]},{"cell_type":"markdown","metadata":{"id":"9t8jGdGOhMoH"},"source":["### Your Task starts here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8T6QFv7fdAs"},"outputs":[],"source":["# Initiate the Conversion of the saved model here\n","converter = tf.lite.TFLiteConverter.from_keras_model(model) # path to the SavedModel directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GdiCDhvt3Ry"},"outputs":[],"source":["# Convert the model to the TensorFlow Lite format without quantization\n","tflite_model = converter.convert()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TzxfGL_0FnrE"},"outputs":[],"source":["# Save the .tflite model\n","with open('model.tflite', 'wb') as f:\n","  f.write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CN53gyEtt-cK"},"outputs":[],"source":["# Another way of Saving .tflite model\n","open(\"/content/cifar10.tflite\",\"wb\").write(tflite_model)"]},{"cell_type":"markdown","metadata":{"id":"CuZkj9EMqa4n"},"source":["Here, the return value 673260 is the size (in bytes) of the generated TensorFlow Lite model with no quantization. We can even significantly reduce the TFLite model size by applying quantization."]},{"cell_type":"markdown","metadata":{"id":"6alsly58mggB"},"source":["### Dynamic range quantization\n","\n","The simplest form of post-training quantization statically quantizes only the weights from floating point to integer, which has 8-bits of precision:\n","\n","#### Convert using default quantization\n","\n","Now let's enable the default `optimizations` flag to quantize all fixed parameters (such as weights):\n","Here, we are using saved model API. Will be useful for your custom trained model for conversion."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33zb5BSYwqe-"},"outputs":[],"source":["# Initiate the Conversion of the saved model here\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","\n","# Set the optimization flag\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","\n","# Convert the model to tflite here\n","tflite_quant_model = converter.convert()"]},{"cell_type":"markdown","metadata":{"id":"tzuSqBZeZ-xp"},"source":["At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.\n","\n","To further improve latency, \"dynamic-range\" operators dynamically quantize activations based on their range to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inference. However, the outputs are still stored using floating point so that the speedup with dynamic-range ops is less than a full fixed-point computation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZJs4AkZnJDI"},"outputs":[],"source":["# Save .tflite model\n","open(\"/content/cifar10_quant.tflite\",\"wb\").write(tflite_quant_model)"]},{"cell_type":"markdown","metadata":{"id":"CPzHZELar3Os"},"source":["Use [Netron](https://netron.app) to visualize the quantized model. Pay attention that now the weights are int8."]},{"cell_type":"markdown","metadata":{"id":"7eg01uNRwxkq"},"source":["### Testing TFLite Model\n","\n","The converted TFLite model can be executed on mobile, embedded and IoT devices. The `TensorFlow Lite Interpreter` used to run an inference with TFLite model. `TensorFlow Lite Interpreter` is a library that takes a TFLite model file, executes the operations on input data and provide output.\n","\n","Interpreter support multiple platforms and provide API to execute TFLite model from Java, C++, Python, Swift and Objective-C.\n","\n","When you receive results from the model inference, you must interpret the tensors in a meaningful way that's useful in your application.\n","\n","For example, a model might return only a list of probabilities. It's up to you to map the probabilities to relevant categories and present it to your end-user."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FKrv21y1G4w"},"outputs":[],"source":["# Initialize the TFLite interpreter\n","# Now let’s load TFLite models into Interpreter (tf.lite.Interpreter) representation, so we can run the inference process on it.\n","# Load the TFLite model in TFLite Interpreter\n","interpreter = tf.lite.Interpreter(\"/content/cifar10_quant.tflite\")"]},{"cell_type":"markdown","metadata":{"id":"_vBd8NLIFEv8"},"source":["tflite_model can be saved to a file and loaded later, or directly into the Interpreter. Since TensorFlow Lite pre-plans tensor allocations to optimize inference, the user needs to call allocate_tensors() before any inference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHZm4ztQ1yku"},"outputs":[],"source":["# Load the TFLite model and allocate tensors\n","interpreter.allocate_tensors() # Needed before execution!\n","\n","# Get input and output tensors.\n","input_details = interpreter.get_input_details()   # Model has single input\n","output_details = interpreter.get_output_details() # Model has single output"]},{"cell_type":"markdown","metadata":{"id":"X9XRg0EctcWy"},"source":["The `input_details` represents the pieces of information related to input data formats such as the shape of input data, the data type of input and other various information related to quantization. And output_details contains information related to the model output format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwBI7x3D2HSp"},"outputs":[],"source":["input_details"]},{"cell_type":"markdown","metadata":{"id":"ZUMPJzgHJ04W"},"source":["Gets model input tensor details.\n","\n","Returns:\n","\n","A list in which each item is a dictionary with details about an input tensor. Each dictionary contains the following fields that describe the tensor:\n","* name: The tensor name.\n","\n","* index: The tensor index in the interpreter.\n","\n","* shape: The shape of the tensor.\n","\n","* shape_signature: Same as shape for models with known/fixed shapes. If any dimension sizes are unkown, they are indicated with -1.\n","\n","* dtype: The numpy data type (such as np.int32 or np.uint8).\n","\n","* quantization: Deprecated, use quantization_parameters. This field only works for per-tensor quantization, whereas quantization_parameters works in all cases.\n","\n","* quantization_parameters: A dictionary of parameters used to quantize the tensor: ~ scales: List of scales (one if per-tensor quantization). ~ zero_points: List of zero_points (one if per-tensor quantization). ~ quantized_dimension: Specifies the dimension of per-axis quantization, in the case of multiple scales/zero_points.\n","\n","* sparsity_parameters: A dictionary of parameters used to encode a sparse tensor. This is empty if the tensor is dense."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zo9Uci3O2YJE"},"outputs":[],"source":["output_details"]},{"cell_type":"markdown","metadata":{"id":"t2zMQ9SMKwuj"},"source":["Gets model output tensor details.\n","\n","Returns: A list in which each item is a dictionary with details about an output tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQZ25ePb286y"},"outputs":[],"source":["def set_input_tensor(interpreter, image):\n","    tensor_index = interpreter.get_input_details()[0]['index']\n","    input_tensor = interpreter.tensor(tensor_index)()[0]\n","    input_tensor[:, :] = image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qwGTmnH7BSO"},"outputs":[],"source":["image = test_images[0]\n","plt.imshow(image);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FmMayAqu619U"},"outputs":[],"source":["set_input_tensor(interpreter, image)\n","\n","# Run inference.\n","interpreter.invoke()\n","output_details = interpreter.get_output_details()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tl1ah6LnHCzp"},"outputs":[],"source":["# The function `get_tensor()` returns a copy of the tensor data.\n","# Use `tensor()` in order to get a pointer to the tensor.\n","interpreter.get_tensor(output_details['index'])"]},{"cell_type":"markdown","metadata":{"id":"le5VV5X7bKln"},"source":["get_tensor(\n","    tensor_index\n",") : Gets the value of the output tensor (get a copy).\n","\n","If you wish to avoid the copy, use tensor(). This function cannot be used to read intermediate results.\n","\n","Args:\n","\n","tensor_index: Tensor index of tensor to get. This value can be gotten from the 'index' field in get_output_details.\n","\n","Returns\n","a numpy array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NI5jYogHGob"},"outputs":[],"source":["np.squeeze(interpreter.get_tensor(output_details['index']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwFI3HIk7i8Q"},"outputs":[],"source":["output = np.squeeze(interpreter.get_tensor(output_details['index']))\n","output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJ5GRhoCcjAT"},"outputs":[],"source":["output.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8RlpEe08UVl"},"outputs":[],"source":["img_pred = np.argmax(output)\n","class_names[img_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5E1ni5ALF009"},"outputs":[],"source":["img_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pe897NEJFwcE"},"outputs":[],"source":["output[img_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBIum-db-dkQ"},"outputs":[],"source":["def classify_image(image):\n","    set_input_tensor(interpreter, image)\n","    interpreter.invoke()\n","    output_details = interpreter.get_output_details()[0]\n","    output = np.squeeze(interpreter.get_tensor(output_details['index']))\n","    img_pred = np.argmax(output)\n","    pred_label = class_names[img_pred]\n","    pred_prob = output[img_pred]\n","    plt.imshow(image)\n","    print(\" Pred: {} with prob {:.2}\".format(\n","        pred_label,\n","        pred_prob))\n","    plt.grid(False)\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itLar4xF_m_G"},"outputs":[],"source":["classify_image(test_images[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kflk_u-aBhqe"},"outputs":[],"source":["for i in range (10):\n","  classify_image(test_images[i])"]},{"cell_type":"markdown","metadata":{"id":"j2WIl3G_-WB6"},"source":["## Initiate TFlite optimization\n","\n","**Note:** Refer the following [link ](https://www.tensorflow.org/model_optimization/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9A6jbqz-5Zs"},"outputs":[],"source":["!pip install -q tensorflow-model-optimization\n","!pip install -q tensorflow\n",", quantized_layer_name_prefix='quant_\n","#pip install -q tensorflow-model-optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYUpEwfD-59w"},"outputs":[],"source":["import numpy as np\n","import tempfile\n","import zipfile\n","import os"]},{"cell_type":"markdown","metadata":{"id":"tdJ9lLiT-d2H"},"source":[" Evaluate the baseline model and save it for later usage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T63xHw0C-pWA"},"outputs":[],"source":["_, baseline_model_accuracy = model.evaluate(\n","    test_images, test_labels, verbose=0)\n","\n","print('Baseline test accuracy:', baseline_model_accuracy)\n","\n","_, keras_file = tempfile.mkstemp('.h5')\n","print('Saving model to: ', keras_file)\n","tf.keras.models.save_model(model, keras_file, include_optimizer=False)"]},{"cell_type":"markdown","metadata":{"id":"y5kTyaLWAJM4"},"source":["### Prune and fine-tune the model to 50% sparsity\n","\n","\n","Apply the prune_low_magnitude() API to prune the whole pre-trained model to achieve the model that is to be clustered in the next step. For how best to use the API to achieve the best compression rate while maintaining your target accuracy, refer to the [pruning comprehensive guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide)."]},{"cell_type":"markdown","metadata":{"id":"ICRh5fxGAWYl"},"source":["#### Define the model and apply the sparsity API\n","\n","\n","To make the whole model train with pruning, apply `tfmot.sparsity.keras.prune_low_magnitude` to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1cClAsRQARgd"},"outputs":[],"source":["import tensorflow_model_optimization as tfmot\n","\n","# define a list containing the names or types of the\n","# layers that shall be pruned, and iterate the layer-wise pruning over all layers in this list\n","prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude # Provide the type of sparsity API\n","\n","# Pruning schedule with constant sparsity(%) throughout training\n","pruning_params = {\n","      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=10) # Provide the sparsity value\n","  }\n","\n","callbacks = [\n","  tfmot.sparsity.keras.UpdatePruningStep() # Keras callback which updates pruning wrappers with the optimizer step and is required during training.\n","]\n","\n","pruned_model = prune_low_magnitude(model, **pruning_params)\n","\n","# Use smaller learning rate for fine-tuning\n","opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n","\n","# Compile the pruned model\n","pruned_model.compile(\n","  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  optimizer=opt,\n","  metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6bNEN17g7rj"},"outputs":[],"source":["# Print the pruned model\n","pruned_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"_D_aJvfOC8nT"},"source":["### Fine-tune the model, check sparsity, and evaluate the accuracy against baseline\n","\n","Fine-tune the model with pruning for 3 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9I3JUVXZDEJl"},"outputs":[],"source":["# Fine-tune model\n","pruned_model.fit(\n","  train_images,\n","  train_labels,\n","  epochs=3,\n","  validation_split=0.1,\n","  callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"4Klr0NApDNzD"},"source":["### Define helper functions to calculate and print the sparsity and clusters of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zbavog4pDP8R"},"outputs":[],"source":["def print_model_weights_sparsity(model):\n","    for layer in model.layers:\n","        if isinstance(layer, tf.keras.layers.Wrapper):\n","            weights = layer.trainable_weights\n","        else:\n","            weights = layer.weights\n","        for weight in weights:\n","            if \"kernel\" not in weight.name or \"centroid\" in weight.name:\n","                continue\n","            weight_size = weight.numpy().size\n","            zero_num = np.count_nonzero(weight == 0)\n","            print(\n","                f\"{weight.name}: {zero_num/weight_size:.2%} sparsity \",\n","                f\"({zero_num}/{weight_size})\",\n","            )\n","\n","# Define helper functions to calculate and print the number of clustering in each kernel of the model.\n","def print_model_weight_clusters(model):\n","    for layer in model.layers:\n","        if isinstance(layer, tf.keras.layers.Wrapper):\n","            weights = layer.trainable_weights\n","        else:\n","            weights = layer.weights\n","        for weight in weights:\n","            # ignore auxiliary quantization weights\n","            if \"quantize_layer\" in weight.name:\n","                continue\n","            if \"kernel\" in weight.name:\n","                unique_count = len(np.unique(weight))\n","                print(\n","                    f\"{layer.name}/{weight.name}: {unique_count} clusters \"\n","                )"]},{"cell_type":"markdown","metadata":{"id":"M4K_DGP9DaW-"},"source":["Let's strip the pruning wrapper first, then check that the model kernels were correctly pruned."]},{"cell_type":"markdown","metadata":{"id":"1FgNP4rbOLH8"},"source":[" `tfmot.sparsity.keras.strip_pruning` Strip pruning wrappers from the model and is necessary to see the compression\n","benefits of pruning.\n","\n","*   `strip_pruning:` Once a model has been pruned to required sparsity, this method can be used to restore the original model with the sparse weights.\n","\n","\n","First, create a compressible model for TensorFlow."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-kqiVA_iDbHR"},"outputs":[],"source":["stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model) # Pass the pruned model\n","\n","print_model_weights_sparsity(stripped_pruned_model)"]},{"cell_type":"markdown","metadata":{"id":"i8TOINPODt4T"},"source":["### Apply sparsity preserving clustering and check its effect on model sparsity in both cases\n","\n","Next, we apply both clustering and sparsity preserving clustering on the pruned model and observe the number of clusters and check that the sparsity is preserved on the pruned model.  Generate a TFLite model and check that the accuracy has been preserved in the pruned clustered model.\n","\n","Clustering, or weight sharing, reduces the number of unique weight values in a model, leading to benefits for deployment. It first groups the weights of each layer into N clusters, then shares the cluster's centroid value for all the weights belonging to the cluster. This technique brings improvements via model compression.\n","\n","\n","Note that we stripped pruning wrappers from the pruned model with tfmot.sparsity.keras.strip_pruning before applying the clustering API."]},{"cell_type":"markdown","metadata":{"id":"c259LsuYk7mZ"},"source":["### Fine-tune the pre-trained model with clustering\n","\n","Apply the cluster_weights() API to a whole pre-trained model to demonstrate its effectiveness in reducing the model size after applying zip while keeping decent accuracy.\n","\n","#### Define the model and apply the clustering API\n","\n","Before you pass the model to the clustering API, make sure it is trained and shows some acceptable accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdaE9TMWDwRB"},"outputs":[],"source":["import tensorflow_model_optimization as tfmot\n","\n","# Sparsity preserving clustering\n","from tensorflow_model_optimization.python.core.clustering.keras.experimental import (\n","    cluster,\n",")\n","\n","# Clustering\n","cluster_weights = tfmot.clustering.keras.cluster_weights # Provide the type of sparsity API\n","CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n","\n","cluster_weights = cluster.cluster_weights\n","\n","clustering_params = {\n","  'number_of_clusters': 8,\n","  'cluster_centroids_init': CentroidInitialization.KMEANS_PLUS_PLUS,\n","  'preserve_sparsity': True\n","}\n","\n","# Cluster a whole model\n","sparsity_clustered_model = cluster_weights(stripped_pruned_model, **clustering_params)\n","\n","# Compile the model\n","sparsity_clustered_model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","# Train the sparsity preserving clustering model\n","print('Train sparsity preserving clustering model:')\n","sparsity_clustered_model.fit(train_images, train_labels,epochs=3, validation_split=0.1)"]},{"cell_type":"markdown","metadata":{"id":"JO91KeSCD_wM"},"source":["Strip the clustering wrapper first, then check that the model is correctly pruned and clustered."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmw5Yy4iEBO7"},"outputs":[],"source":["stripped_clustered_model = tfmot.clustering.keras.strip_clustering(sparsity_clustered_model)\n","\n","print(\"Model sparsity:\\n\")\n","print_model_weights_sparsity(stripped_clustered_model)\n","\n","print(\"\\nModel clusters:\\n\")\n","print_model_weight_clusters(stripped_clustered_model)"]},{"cell_type":"markdown","metadata":{"id":"XrWd9l16ELsr"},"source":["### Apply QAT (Quantization Aware Training) and PCQAT (Preserving Quantization Aware Training) and check effect on model clusters and sparsity\n"]},{"cell_type":"markdown","metadata":{"id":"tzclCwSBENhF"},"source":["Next, apply both QAT and PCQAT on the sparse clustered model and observe that PCQAT preserves weight sparsity and clusters in your model. Note that the stripped model is passed to the QAT and PCQAT API.\n","\n","Since this model is going to be deployed on a microcontroller, we want it to be as tiny as possible! One technique for reducing the size of a model is called quantization. It reduces the precision of the model's weights, and possibly the activations (output of each layer) as well, which saves memory, often without much impact on accuracy. Quantized models also run faster, since the calculations required are simpler."]},{"cell_type":"markdown","metadata":{"id":"JsZROpNYMWQ0"},"source":["You will apply quantization aware training to the whole model and see this in the model summary. All layers are now prefixed by \"quant\".\n","\n","Note that the resulting model is quantization aware but not quantized (e.g. the weights are float32 instead of int8).\n"]},{"cell_type":"code","source":["# QAT\n","qat_model = tfmot.quantization.keras.quantize_annotate_model(stripped_clustered_model) # Apply Quantization on stripped clustered model\n","#qat_model = tfmot.quantization.keras.quantize_apply(stripped_clustered_model1) # Apply Quantization on stripped clustered model\n","\n","# Compile the QAT model\n","qat_model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","# Train the QAT model\n","print('Train qat model:')\n","qat_model.fit(train_images, train_labels, batch_size=128, epochs=3, validation_split=0.1)\n","\n","# PCQAT\n","# Annotate a model (stripped_clustered_model) to be quantized\n","# This function does not actually quantize the model. It merely specifies that the\n","# model needs to be quantized. 'quantize_apply' can then be used to quantize the model.\n","pcqat_model = tfmot.quantization.keras.quantize_annotate_model(\n","              stripped_clustered_model)\n","#pcqat_model = tfmot.quantization.keras.quantize_model(\n","#              quant_aware_annotate_model,\n","#              # Default 8 bit Cluster Preserve Quantization Scheme\n","#              # preserve_sparsity - the flag to enable prune-cluster-preserving QAT\n","#              tfmot.experimental.combine.Default8BitClusterPreserveQuantizeScheme(preserve_sparsity=True))\n","\n","pcqat_model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","print('Train pcqat model:')\n","pcqat_model.fit(train_images, train_labels, batch_size=128, epochs=3, validation_split=0.1)"],"metadata":{"id":"bHRv2aJ64R4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHl4X4pbEUe9"},"outputs":[],"source":["print(\"QAT Model clusters:\")\n","print_model_weight_clusters(qat_model)\n","print(\"\\nQAT Model sparsity:\")\n","print_model_weights_sparsity(qat_model)\n","print(\"\\nPCQAT Model clusters:\")\n","print_model_weight_clusters(pcqat_model)\n","print(\"\\nPCQAT Model sparsity:\")\n","print_model_weights_sparsity(pcqat_model)"]},{"cell_type":"markdown","metadata":{"id":"uo1MzQ8oEZ4r"},"source":["### See compression benefits of PCQAT model"]},{"cell_type":"markdown","metadata":{"id":"mfs6G7DqEdRi"},"source":["\n","Define a helper function to actually compress the models via gzip and measure the zipped size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WYWz49tEgpm"},"outputs":[],"source":["def get_gzipped_model_size(file):\n","  # It returns the size of the gzipped model in kilobytes.\n","\n","  _, zipped_file = tempfile.mkstemp('.zip')\n","  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n","    f.write(file)\n","\n","  return os.path.getsize(zipped_file)/1000"]},{"cell_type":"markdown","metadata":{"id":"6QekIvW6Eu2d"},"source":["Observe that applying sparsity, clustering and PCQAT to a model yields significant compression benefits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SpIgRGhExqT"},"outputs":[],"source":["# QAT model\n","converter = tf.lite.TFLiteConverter.from_keras_model(qat_model) # Initiate the Conversion of the QAT model here\n","converter.optimizations = [tf.lite.Optimize.DEFAULT] # Provide the default optimization option\n","qat_tflite_model = converter.convert() # Conversion to tflite\n","qat_model_file = 'qat_model.tflite'\n","\n","# Save the model\n","# Provide the save model filename and code\n","with open(qat_model_file, 'wb') as f:\n","    f.write(qat_tflite_model)\n","\n","# PCQAT model\n","converter = tf.lite.TFLiteConverter.from_keras_model(pcqat_model) # Initiate the Conversion of the PCQAT model here\n","converter.optimizations = [tf.lite.Optimize.DEFAULT] # Provide the default optimization option\n","pcqat_tflite_model = converter.convert() # Conversion to tflite\n","pcqat_model_file = 'pcqat_model.tflite'\n","\n","# Save the model\n","# Provide the save model filename and code\n","with open(pcqat_model_file, 'wb') as f:\n","    f.write(pcqat_tflite_model)\n","\n","print(\"QAT model size: \", get_gzipped_model_size(qat_model_file), ' KB')\n","print(\"PCQAT model size: \", get_gzipped_model_size(pcqat_model_file), ' KB')"]},{"cell_type":"markdown","metadata":{"id":"qvx9isKDE3Y0"},"source":["### See the persistence of accuracy from TF to TFLite"]},{"cell_type":"markdown","metadata":{"id":"eYSI-pi7E_nd"},"source":["Define a helper function to evaluate the TFLite model on the test dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ma2rbSK3E4d7"},"outputs":[],"source":["def eval_model(interpreter):\n","  input_index = interpreter.get_input_details()[0][\"index\"]\n","  output_index = interpreter.get_output_details()[0][\"index\"]\n","\n","  # Run predictions on every image in the \"test\" dataset.\n","  prediction_digits = []\n","  for i, test_image in enumerate(test_images):\n","    if i % 1000 == 0:\n","      print(f\"Evaluated on {i} results so far.\")\n","    # Pre-processing: add batch dimension and convert to float32 to match with\n","    # the model's input data format.\n","    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n","    interpreter.set_tensor(input_index, test_image)\n","\n","    # Run inference.\n","    interpreter.invoke()\n","\n","    # Post-processing: remove batch dimension and find the digit with highest\n","    # probability.\n","    output = interpreter.tensor(output_index)\n","    digit = np.argmax(output()[0])\n","    prediction_digits.append(digit)\n","\n","  print('\\n')\n","  # Compare prediction results with ground truth labels to calculate accuracy.\n","  prediction_digits = np.array(prediction_digits)\n","  accuracy = (prediction_digits == test_labels).mean()\n","  return accuracy"]},{"cell_type":"markdown","metadata":{"id":"ufCkJqBQFHX3"},"source":["Evaluate the model, which has been pruned, clustered and quantized, and then see that the accuracy from TensorFlow persists in the TFLite backend."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLBgzm7WFIx1"},"outputs":[],"source":["interpreter = tf.lite.Interpreter(pcqat_model_file)\n","interpreter.allocate_tensors()\n","\n","pcqat_test_accuracy = eval_model(interpreter)\n","\n","print('Pruned, clustered and quantized TFLite test_accuracy:', pcqat_test_accuracy)\n","print('Baseline TF test accuracy:', baseline_model_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"b2NCPuTd07fr"},"source":["# TensorFlow Lite Micro"]},{"cell_type":"markdown","metadata":{"id":"9PYjt32yBEVU"},"source":["### Generate a TensorFlow Lite for Microcontrollers Model\n","To convert the TensorFlow Lite quantized model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers on MCUs, we simply need to use the ```xxd``` tool to convert the ```.tflite``` file into a ```.cc``` file."]},{"cell_type":"markdown","metadata":{"id":"Hub-VrZT61lR"},"source":["**Convert to a C array**"]},{"cell_type":"markdown","metadata":{"id":"NukaFEnE7pXf"},"source":["First install xxd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-L6O8zj7as9"},"outputs":[],"source":["!apt-get update && apt-get -qq install xxd"]},{"cell_type":"markdown","metadata":{"id":"krTZ7tFDBiK1"},"source":["Now, convert and save the .cc converted model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFFrKX3_-Vvg"},"outputs":[],"source":["MODEL_TFLITE = 'cifar10_quant.tflite'\n","MODEL_TFLITE_MICRO = 'cifar10_quant_model.cc'\n","!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n","REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n","!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"]},{"cell_type":"markdown","metadata":{"id":"38A1ZGaV_xDt"},"source":["If you'd like to download your model for safekeeping:\n","1. On the left of the UI click on the folder icon\n","2. Click on the three dots to the right of the ```cifar10_quant_model.cc``` file and select download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iof2YquSAH7S"},"outputs":[],"source":["!cat {MODEL_TFLITE_MICRO}"]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qj9C17QoNw45"},"source":["#### Consider the statements given below and answer Q1.\n","\n","A. Quantization works by reducing the precision of the numbers used to represent a model's parameters, which by default are 32-bit floating point numbers. This results in a smaller model size and faster computation.\n","\n","B. Pruning works by removing parameters within a model that have only a minor impact on its predictions.\n","\n","C. Clustering works by grouping the weights of each layer in a model into a predefined number of clusters, then sharing the centroid values for the weights belonging to each individual cluster. This reduces the number of unique weight values in a model, thus reducing its complexity.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-INhmnH1RDq0"},"outputs":[],"source":["#@title Q.1. Which of the above statements is/are True about TensorflowLite Model Optimization?\n","Answer1 = \"\" #@param [\"\",\"Only A\", \"Only B\", \"Only C\", \"Only A and B\",\"Only B and C\", \"A, B, C\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Qu3sufDD7pu1"},"outputs":[],"source":["#@title Q.2. Magnitude-based weight pruning gradually zeroes out model weights during the training process to achieve model sparsity. Sparse models are easier to compress, and we can skip the zeroes during inference for latency improvements.\n","Answer2 = \"\" #@param [\"\",\"TRUE\",\"FALSE\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMzKSbLIgFzQ"},"outputs":[],"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjcH1VWSFI2l"},"outputs":[],"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VBk_4VTAxCM"},"outputs":[],"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH91cL1JWH7m"},"outputs":[],"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8xLqj7VWIKW"},"outputs":[],"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"FzAZHt1zw-Y-"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}